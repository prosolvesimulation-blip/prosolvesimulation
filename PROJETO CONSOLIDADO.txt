================================================================================
PROJETO CONSOLIDADO - PROSOLVE SIMULATION
Scripts do projeto: 148 arquivos (.py, .tsx, .j2)
================================================================================

--------------------------------------------------------------------------------
ESTRUTURA DOS SCRIPTS
--------------------------------------------------------------------------------
  (raiz)/
    - explore_normals.py
    - med_extractor.py
    - test_med_groups_v2.py
    - test_normals.py
    - verify_med_extrusion_v2.py
  .agent/.shared/ui-ux-pro-max/scripts/
    - core.py
    - design_system.py
    - search.py
  .agent/scripts/
    - checklist.py
    - verify_all.py
  .agent/skills/api-patterns/scripts/
    - api_validator.py
  .agent/skills/database-design/scripts/
    - schema_validator.py
  .agent/skills/frontend-design/scripts/
    - accessibility_checker.py
    - ux_audit.py
  .agent/skills/geo-fundamentals/scripts/
    - geo_checker.py
  .agent/skills/i18n-localization/scripts/
    - i18n_checker.py
  .agent/skills/lint-and-validate/scripts/
    - lint_runner.py
    - type_coverage.py
  .agent/skills/mobile-design/scripts/
    - mobile_audit.py
  .agent/skills/performance-profiling/scripts/
    - lighthouse_audit.py
  .agent/skills/seo-fundamentals/scripts/
    - seo_checker.py
  .agent/skills/testing-patterns/scripts/
    - test_runner.py
  .agent/skills/vulnerability-scanner/scripts/
    - security_scan.py
  .agent/skills/webapp-testing/scripts/
    - playwright_runner.py
  backend/
    - app.py
  backend/api/
    - routes.py
  backend/services/
    - mesh_reader.py
    - section_calculator - Copia.py
    - section_calculator.py
    - section_extractor.py
    - section_mesh_service.py
    - vtk_converter.py
    - vtk_shell_extruder.py
  backend/services/jinja/
    - generate_comm.py
    - inspect_mesh.py
    - server.py
    - teste.py
  backend/services/jinja/builders/
    - affe_cara_elem_shell.py
    - affe_char_meca_ddl.py
    - affe_materiau.py
    - affe_modele.py
    - asse_maillage.py
    - defi_materiau.py
    - force_coque.py
    - force_nodale.py
    - geometry.py
    - lire_maillage.py
    - load_cases.py
    - meca_statique.py
    - pesanteur.py
    - post_elem_mass.py
    - post_releve_t_reactions.py
  backend/services/jinja/templates/
    - affe_cara_elem.j2
    - affe_cara_elem_shell.j2
    - affe_char_meca_ddl.j2
    - affe_materiau.j2
    - affe_modele.j2
    - asse_maillage.j2
    - defi_materiau.j2
    - export.j2
    - extract_results.j2
    - force_coque_calc.j2
    - force_coque_load.j2
    - force_nodale.j2
    - geometric_check.j2
    - inspect_mesh.j2
    - lire_maillage.j2
    - load_cases.j2
    - meca_statique.j2
    - pesanteur.j2
    - post_elem_mass.j2
    - preamble.j2
  backend/services/med/
    - call_med_mesher.py
    - debug_api.py
    - debug_env.py
    - debug_env_deep.py
    - debug_medloader.py
    - med_extractor.py
    - med_extruder.py
    - med_mesher.py
    - mesh_viewer_vtk.py
    - section_extruder.py
    - vtk_extruder.py
    - vtk_extruder_test02.py
  deletar/
    - main.py
  frontend/src/
    - App.tsx
    - main.tsx
  frontend/src/components/
    - ConsoleOverlay.tsx
    - Dashboard.tsx
    - Settings.tsx
    - StructuralWorkspace.tsx
  frontend/src/components/config/
    - GeometryConfig.tsx
    - LoadCaseConfig.tsx
    - LoadConfig.tsx
    - MaterialConfig.tsx
    - MeshConfig.tsx
    - ModelConfig.tsx
    - RestrictionConfig.tsx
    - ThreeDModel.tsx
    - VtkMeshViewer.tsx
  frontend/src/stories/
    - Button.tsx
    - Header.tsx
    - ModelConfig.stories.tsx
    - Page.tsx
  prosolve/
    - _sectionproperties.py
    - run_aster.py
  prosolve/jinja/
    - generate_comm.py
    - inspect_mesh.py
    - teste.py
  prosolve/jinja/builders/
    - affe_cara_elem_shell.py
    - affe_char_meca_ddl.py
    - affe_materiau.py
    - affe_modele.py
    - asse_maillage.py
    - defi_materiau.py
    - force_coque.py
    - geometry.py
    - lire_maillage.py
    - load_cases.py
    - meca_statique.py
    - pesanteur.py
    - post_elem_mass.py
    - post_releve_t_reactions.py
  prosolve/jinja/templates/
    - affe_cara_elem.j2
    - affe_cara_elem_shell.j2
    - affe_char_meca_ddl.j2
    - affe_materiau.j2
    - affe_modele.j2
    - asse_maillage.j2
    - defi_materiau.j2
    - export.j2
    - force_coque_calc.j2
    - force_coque_load.j2
    - inspect_mesh.j2
    - lire_maillage.j2
    - load_cases.j2
    - meca_statique.j2
    - pesanteur.j2
    - post_elem_mass.j2
  temp/
    - research_shell_methods.py
    - simulate_frontend_handshake.py
    - test_flask_pipeline.py
    - test_native_merge.py
    - test_pipeline_handshake.py
    - test_simplified_shell.py
    - validate_global_state_data.py
    - verify_modular_vtk.py
    - verify_vtk_ext.py


================================================================================
CONTEÚDO DOS SCRIPTS
================================================================================


################################################################################
# PASTA: (raiz)
################################################################################

--- ARQUIVO: explore_normals.py ---
import sys
import os
import json

try:
    import MEDLoader as ml
    import medcoupling as mc
except ImportError:
    print("MEDCOUPLING not found")
    sys.exit(1)

def explore(file_path):
    print(f"Checking {file_path}")
    mesh_names = ml.GetMeshNames(file_path)
    mesh_name = mesh_names[0]
    
    # Read just the shell group
    group_mesh = ml.ReadUMeshFromGroups(file_path, mesh_name, 0, ["12mm"])
    
    print(f"Mesh Dim: {group_mesh.getMeshDimension()}")
    print(f"Space Dim: {group_mesh.getSpaceDimension()}")
    
    # Look for normal/orthogonal computation methods
    methods = dir(group_mesh)
    relevant = [m for m in methods if 'Normal' in m or 'Orthogonal' in m or 'Measure' in m]
    print(f"Relevant Methods: {relevant}")
    
    # Try to compute normals
    try:
        # buildOrthogonalField is a common one
        if hasattr(group_mesh, 'buildOrthogonalField'):
            print("Trying buildOrthogonalField()...")
            normals = group_mesh.buildOrthogonalField()
            print(f"Result type: {type(normals)}")
            print(f"Number of tuples: {normals.getNumberOfTuples()}")
            print(f"Number of components: {normals.getNumberOfComponents()}")
            
            # Print sample
            arr = normals.toNumPyArray()
            print(f"Sample normal (0): {arr[0]}")
    except Exception as e:
        print(f"Error computing normals: {e}")

explore(r"c:\Users\jorge\OneDrive\ProSolveSimulation\test01\shell.med")

--- ARQUIVO: med_extractor.py ---
#!/usr/bin/env python3
"""MedExtractor: extract a MED mesh and emit a standardized JSON payload.

This module is designed to be agnostic to the frontend framework. It always
produces a JSON object on stdout (minified) and can optionally write the same
payload to a JSON file when a second argument is provided.
"""

import json
import os
import sys

try:
    # MEDCoupling Python bindings
    from medcoupling import MEDLoader  # type: ignore
except Exception:
    MEDLoader = None  # pragma: no cover - will trigger fallback errors if used without MED


def _safe_call(obj, method, default=None):
    """Call a method defensively if available."""
    if obj is None:
        return default
    if hasattr(obj, method):
        try:
            return getattr(obj, method)()
        except Exception:
            return default
    return default


def _category_from_dim(dim: int) -> str:
    if dim == 3:
        return "3D"
    if dim == 2:
        return "2D"
    if dim == 1:
        return "1D"
    # 0 or Node-like
    return "Node"


def _load_mesh(path: str):
    if MEDLoader is None:
        raise RuntimeError("medcoupling is not available in this environment.")
    loader = MEDLoader()
    # Try common entry points used by different MEDLoader bindings
    for method in ("LoadMesh", "LoadFile", "Load", "loadMesh", "loadFile", "load"):
        if hasattr(loader, method):
            func = getattr(loader, method)
            return func(path)
    raise RuntimeError("MEDLoader does not expose a compatible load method.")


def _extract_one_group(mesh, group_key: str) -> dict:
    # Try to determine mesh dimension
    dim = _safe_call(mesh, "getMeshDimension", 0)
    if not isinstance(dim, int):
        dim = int(dim) if dim is not None else 0
    category = _category_from_dim(dim)

    # Count (number of cells/elements)
    count = _safe_call(mesh, "getNumberOfElements", 0)
    if not isinstance(count, int):
        count = int(count) if count is not None else 0

    # Geometry: points, connectivity
    points = []
    connectivity = []
    normals = None

    try:
        coords = mesh.getCoords()
        if coords is not None and hasattr(coords, "toNumPyArray"):
            points = coords.toNumPyArray().flatten().tolist()
        elif coords is not None and hasattr(coords, "toList"):
            points = list(coords.toList())
    except Exception:
        points = []

    try:
        conn = mesh.getNodalConnectivity()
        if conn is not None and hasattr(conn, "toNumPyArray"):
            connectivity = conn.toNumPyArray().flatten().tolist()
        elif conn is not None and hasattr(conn, "toList"):
            connectivity = list(conn.toList())
    except Exception:
        connectivity = []

    # Normals for 2D meshes
    if category == "2D":
        try:
            orth = mesh.buildOrthogonalField()
            if orth is not None:
                arr = orth.getArray() if hasattr(orth, "getArray") else None
                if arr is not None and hasattr(arr, "toNumPyArray"):
                    normals = arr.toNumPyArray().flatten().tolist()
        except Exception:
            normals = None

    # VTK cell type (best-effort; default to Triangle=5 as per doc)
    type_vtk = 5
    for method in ("getVTKCellType", "getTypeVTK", "getCellType"):
        if hasattr(mesh, method):
            try:
                val = getattr(mesh, method)()
                if isinstance(val, int):
                    type_vtk = val
                    break
            except Exception:
                pass

    group = {
        "dimension": dim,
        "count": count,
        "category": category,
        "type_vtk": int(type_vtk),
        "points": points,
        "connectivity": connectivity,
        "normals": normals,
    }
    return group


def extract_to_dict(path: str):
    """Return the parsed MED data as a standard JSON-like dict.

    The result matches the schema defined in the mission protocol.
    """
    filename = os.path.basename(path)
    group_name = os.path.splitext(filename)[0] or "mesh"

    mesh = _load_mesh(path)
    # If the mesh exposes multiple groups, try to fetch them; otherwise create a single synthetic group
    groups = {}
    try:
        # Try to retrieve named groups using a couple of common API aliases
        group_names = []
        for attr in ("getGroupsNames", "getGroupsName", "getGroupNames"):
            if hasattr(mesh, attr):
                try:
                    names = getattr(mesh, attr)()
                    if isinstance(names, (list, tuple)):
                        group_names = list(names)
                        break
                except Exception:
                    pass
        if not group_names:
            group_names = [group_name]
    except Exception:
        group_names = [group_name]

    # Build data for the first available group (fallbacks handle missing per-group APIs)
    for g in group_names:
        groups[g] = _extract_one_group(mesh, g)

        # If we only discovered a synthetic group, break after first one
        # (the current frontend schema expects at least one group per file)
        break

    data = {"groups": groups}
    payload = {"status": "success", "filename": filename, "data": data}
    return payload


def main():
    if len(sys.argv) < 2:
        print("Usage: med_extractor.py <path_to_med> [output.json]", flush=True)
        sys.exit(2)
    path = sys.argv[1]
    outpath = sys.argv[2] if len(sys.argv) > 2 else None
    payload = None
    try:
        payload = extract_to_dict(path)
    except Exception as exc:
        payload = {"status": "error", "message": str(exc)}

    json_text = json.dumps(payload, separators=(",", ":"))
    # Always print to stdout (minified)
    print(json_text)
    if outpath:
        with open(outpath, "w", encoding="utf-8") as f:
            f.write(json_text)


if __name__ == "__main__":
    main()

--- ARQUIVO: test_med_groups_v2.py ---
import sys
import os

try:
    from MEDLoader import GetMeshNames, GetMeshGroupsNames, ReadUMeshFromGroups
    import medcoupling as mc
    
    file_path = r"c:\Users\jorge\OneDrive\ProSolveSimulation\test01\beam.med"
    print(f"Testing with: {file_path}")
    
    mesh_names = GetMeshNames(file_path)
    print(f"Meshes: {mesh_names}")
    
    if mesh_names:
        mesh_name = mesh_names[0]
        groups = GetMeshGroupsNames(file_path, mesh_name)
        print(f"Groups: {groups}")
        
        for g in groups:
            # Read mesh subset for this group
            # ReadUMeshFromGroups(fileName, meshName, iteration=0, groupNames=[])
            mesh_group = ReadUMeshFromGroups(file_path, mesh_name, 0, [g])
            print(f"Group '{g}': {mesh_group.getNumberOfCells()} cells, Type: {mesh_group.getMeshDimension()}D")
            
except Exception as e:
    print(f"Error: {e}")
    import traceback
    traceback.print_exc()

--- ARQUIVO: test_normals.py ---
import subprocess
import json

med_dir = r"c:\Users\jorge\OneDrive\ProSolveSimulation\MEDCOUPLING-9.15.0\MEDCOUPLING-9.15.0"
extractor = r"c:\Users\jorge\OneDrive\ProSolveSimulation\backend\services\med_extractor.py"
target = r"c:\Users\jorge\OneDrive\ProSolveSimulation\test01\shell.med"

cmd = f'cmd /c "cd /d {med_dir} && call env_launch.bat && python {extractor} {target}"'
print(f"Testing normal extraction on {target}...")
res = subprocess.run(cmd, capture_output=True, text=True, shell=True)

if res.returncode == 0:
    out = res.stdout.strip()
    lines = out.split('\n')
    data = None
    for line in reversed(lines):
        if line.strip().startswith('{'):
            try:
                data = json.loads(line)
                break
            except: continue
    
    if data and data['status'] == 'success':
        print(f"✓ Groups found: {list(data['cells'].keys())}")
        for k, v in data['cells'].items():
            has_normals = 'normals' in v
            normal_count = len(v['normals']) if has_normals else 0
            print(f"  - {k}: {v['count']} cells, normals: {has_normals} ({normal_count})")
            if has_normals and normal_count > 0:
                print(f"    Sample normal[0]: {v['normals'][0]}")
    else:
        print("Failed to parse JSON")
        print(res.stdout)
else:
    print(res.stderr)

--- ARQUIVO: verify_med_extrusion_v2.py ---
import sys
import os
import json
import subprocess

def verify():
    base_dir = r"c:\Users\jorge\OneDrive\ProSolveSimulation"
    med_dir = os.path.join(base_dir, "MEDCOUPLING-9.15.0", "MEDCOUPLING-9.15.0")
    extractor_path = os.path.join(base_dir, "backend", "services", "med_extractor.py")
    shell_med = os.path.join(base_dir, "testcases", "shell", "shell.med")
    
    # 1. Create temporary geometries JSON
    geometries = [
        {
            "group": "group_shell",
            "type": "COQUE_3D",
            "section_params": {
                "thickness": 10.0,
                "offset": 0.0
            }
        }
    ]
    
    geom_json_path = os.path.join(base_dir, "temp", "geometries_verify_v2.json")
    os.makedirs(os.path.dirname(geom_json_path), exist_ok=True)
    with open(geom_json_path, 'w') as f:
        json.dump(geometries, f)
        
    print(f"Testing extrusion with {shell_med}")
    
    # 2. Call extractor
    cmd = f'cmd /c "cd /d {med_dir} && call env_launch.bat && python {extractor_path} \"{shell_med}\" \"{geom_json_path}\""'
    result = subprocess.run(cmd, capture_output=True, text=True, shell=True)
    
    if result.stderr:
        print("--- STDERR ---")
        print(result.stderr)
        print("--------------")

    # 3. Parse output
    output = result.stdout.strip()
    lines = output.split('\n')
    data = None
    for line in reversed(lines):
        line = line.strip()
        if line.startswith('{') and line.endswith('}'):
            try:
                data = json.loads(line)
                break
            except: continue
            
    if not data:
        print("FAILED: No JSON found in output")
        return
        
    # 4. Assertions
    cells = data.get("cells", {})
    print(f"Groups found: {list(cells.keys())}")
    
    if "group_shell_EXTRUDED" in cells:
        ext_data = cells["group_shell_EXTRUDED"]
        print(f"SUCCESS: 'group_shell_EXTRUDED' found!")
        print(f"  - Type: {ext_data.get('type')} (Expected: quad/triangle)")
        print(f"  - Cell count: {ext_data.get('count')}")
        
        # Check coordinates length to see if points were accumulated correctly
        points = data.get("points", [])
        print(f"Total points in master list: {len(points)}")
        
        # Verify first connectivity of a non-extruded group to see if offset is correct
        if "Group_Of_All_Faces" in cells:
            f_conn = cells["Group_Of_All_Faces"]["connectivity"][0]
            print(f"Sample Face Connectivity: {f_conn}")
            # If offset bug is fixed, points should be within range of that group's points
    else:
        print("FAILED: 'group_shell_EXTRUDED' not found in result.")

if __name__ == "__main__":
    verify()


################################################################################
# PASTA: .agent/.shared/ui-ux-pro-max/scripts
################################################################################

--- ARQUIVO: .agent/.shared/ui-ux-pro-max/scripts/core.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
UI/UX Pro Max Core - BM25 search engine for UI/UX style guides
"""

import csv
import re
from pathlib import Path
from math import log
from collections import defaultdict

# ============ CONFIGURATION ============
DATA_DIR = Path(__file__).parent.parent / "data"
MAX_RESULTS = 3

CSV_CONFIG = {
    "style": {
        "file": "styles.csv",
        "search_cols": ["Style Category", "Keywords", "Best For", "Type"],
        "output_cols": ["Style Category", "Type", "Keywords", "Primary Colors", "Effects & Animation", "Best For", "Performance", "Accessibility", "Framework Compatibility", "Complexity"]
    },
    "prompt": {
        "file": "prompts.csv",
        "search_cols": ["Style Category", "AI Prompt Keywords (Copy-Paste Ready)", "CSS/Technical Keywords"],
        "output_cols": ["Style Category", "AI Prompt Keywords (Copy-Paste Ready)", "CSS/Technical Keywords", "Implementation Checklist"]
    },
    "color": {
        "file": "colors.csv",
        "search_cols": ["Product Type", "Keywords", "Notes"],
        "output_cols": ["Product Type", "Keywords", "Primary (Hex)", "Secondary (Hex)", "CTA (Hex)", "Background (Hex)", "Text (Hex)", "Border (Hex)", "Notes"]
    },
    "chart": {
        "file": "charts.csv",
        "search_cols": ["Data Type", "Keywords", "Best Chart Type", "Accessibility Notes"],
        "output_cols": ["Data Type", "Keywords", "Best Chart Type", "Secondary Options", "Color Guidance", "Accessibility Notes", "Library Recommendation", "Interactive Level"]
    },
    "landing": {
        "file": "landing.csv",
        "search_cols": ["Pattern Name", "Keywords", "Conversion Optimization", "Section Order"],
        "output_cols": ["Pattern Name", "Keywords", "Section Order", "Primary CTA Placement", "Color Strategy", "Conversion Optimization"]
    },
    "product": {
        "file": "products.csv",
        "search_cols": ["Product Type", "Keywords", "Primary Style Recommendation", "Key Considerations"],
        "output_cols": ["Product Type", "Keywords", "Primary Style Recommendation", "Secondary Styles", "Landing Page Pattern", "Dashboard Style (if applicable)", "Color Palette Focus"]
    },
    "ux": {
        "file": "ux-guidelines.csv",
        "search_cols": ["Category", "Issue", "Description", "Platform"],
        "output_cols": ["Category", "Issue", "Platform", "Description", "Do", "Don't", "Code Example Good", "Code Example Bad", "Severity"]
    },
    "typography": {
        "file": "typography.csv",
        "search_cols": ["Font Pairing Name", "Category", "Mood/Style Keywords", "Best For", "Heading Font", "Body Font"],
        "output_cols": ["Font Pairing Name", "Category", "Heading Font", "Body Font", "Mood/Style Keywords", "Best For", "Google Fonts URL", "CSS Import", "Tailwind Config", "Notes"]
    },
    "icons": {
        "file": "icons.csv",
        "search_cols": ["Category", "Icon Name", "Keywords", "Best For"],
        "output_cols": ["Category", "Icon Name", "Keywords", "Library", "Import Code", "Usage", "Best For", "Style"]
    },
    "react": {
        "file": "react-performance.csv",
        "search_cols": ["Category", "Issue", "Keywords", "Description"],
        "output_cols": ["Category", "Issue", "Platform", "Description", "Do", "Don't", "Code Example Good", "Code Example Bad", "Severity"]
    },
    "web": {
        "file": "web-interface.csv",
        "search_cols": ["Category", "Issue", "Keywords", "Description"],
        "output_cols": ["Category", "Issue", "Platform", "Description", "Do", "Don't", "Code Example Good", "Code Example Bad", "Severity"]
    }
}

STACK_CONFIG = {
    "html-tailwind": {"file": "stacks/html-tailwind.csv"},
    "react": {"file": "stacks/react.csv"},
    "nextjs": {"file": "stacks/nextjs.csv"},
    "vue": {"file": "stacks/vue.csv"},
    "nuxtjs": {"file": "stacks/nuxtjs.csv"},
    "nuxt-ui": {"file": "stacks/nuxt-ui.csv"},
    "svelte": {"file": "stacks/svelte.csv"},
    "swiftui": {"file": "stacks/swiftui.csv"},
    "react-native": {"file": "stacks/react-native.csv"},
    "flutter": {"file": "stacks/flutter.csv"},
    "shadcn": {"file": "stacks/shadcn.csv"},
    "jetpack-compose": {"file": "stacks/jetpack-compose.csv"}
}

# Common columns for all stacks
_STACK_COLS = {
    "search_cols": ["Category", "Guideline", "Description", "Do", "Don't"],
    "output_cols": ["Category", "Guideline", "Description", "Do", "Don't", "Code Good", "Code Bad", "Severity", "Docs URL"]
}

AVAILABLE_STACKS = list(STACK_CONFIG.keys())


# ============ BM25 IMPLEMENTATION ============
class BM25:
    """BM25 ranking algorithm for text search"""

    def __init__(self, k1=1.5, b=0.75):
        self.k1 = k1
        self.b = b
        self.corpus = []
        self.doc_lengths = []
        self.avgdl = 0
        self.idf = {}
        self.doc_freqs = defaultdict(int)
        self.N = 0

    def tokenize(self, text):
        """Lowercase, split, remove punctuation, filter short words"""
        text = re.sub(r'[^\w\s]', ' ', str(text).lower())
        return [w for w in text.split() if len(w) > 2]

    def fit(self, documents):
        """Build BM25 index from documents"""
        self.corpus = [self.tokenize(doc) for doc in documents]
        self.N = len(self.corpus)
        if self.N == 0:
            return
        self.doc_lengths = [len(doc) for doc in self.corpus]
        self.avgdl = sum(self.doc_lengths) / self.N

        for doc in self.corpus:
            seen = set()
            for word in doc:
                if word not in seen:
                    self.doc_freqs[word] += 1
                    seen.add(word)

        for word, freq in self.doc_freqs.items():
            self.idf[word] = log((self.N - freq + 0.5) / (freq + 0.5) + 1)

    def score(self, query):
        """Score all documents against query"""
        query_tokens = self.tokenize(query)
        scores = []

        for idx, doc in enumerate(self.corpus):
            score = 0
            doc_len = self.doc_lengths[idx]
            term_freqs = defaultdict(int)
            for word in doc:
                term_freqs[word] += 1

            for token in query_tokens:
                if token in self.idf:
                    tf = term_freqs[token]
                    idf = self.idf[token]
                    numerator = tf * (self.k1 + 1)
                    denominator = tf + self.k1 * (1 - self.b + self.b * doc_len / self.avgdl)
                    score += idf * numerator / denominator

            scores.append((idx, score))

        return sorted(scores, key=lambda x: x[1], reverse=True)


# ============ SEARCH FUNCTIONS ============
def _load_csv(filepath):
    """Load CSV and return list of dicts"""
    with open(filepath, 'r', encoding='utf-8') as f:
        return list(csv.DictReader(f))


def _search_csv(filepath, search_cols, output_cols, query, max_results):
    """Core search function using BM25"""
    if not filepath.exists():
        return []

    data = _load_csv(filepath)

    # Build documents from search columns
    documents = [" ".join(str(row.get(col, "")) for col in search_cols) for row in data]

    # BM25 search
    bm25 = BM25()
    bm25.fit(documents)
    ranked = bm25.score(query)

    # Get top results with score > 0
    results = []
    for idx, score in ranked[:max_results]:
        if score > 0:
            row = data[idx]
            results.append({col: row.get(col, "") for col in output_cols if col in row})

    return results


def detect_domain(query):
    """Auto-detect the most relevant domain from query"""
    query_lower = query.lower()

    domain_keywords = {
        "color": ["color", "palette", "hex", "#", "rgb"],
        "chart": ["chart", "graph", "visualization", "trend", "bar", "pie", "scatter", "heatmap", "funnel"],
        "landing": ["landing", "page", "cta", "conversion", "hero", "testimonial", "pricing", "section"],
        "product": ["saas", "ecommerce", "e-commerce", "fintech", "healthcare", "gaming", "portfolio", "crypto", "dashboard"],
        "prompt": ["prompt", "css", "implementation", "variable", "checklist", "tailwind"],
        "style": ["style", "design", "ui", "minimalism", "glassmorphism", "neumorphism", "brutalism", "dark mode", "flat", "aurora"],
        "ux": ["ux", "usability", "accessibility", "wcag", "touch", "scroll", "animation", "keyboard", "navigation", "mobile"],
        "typography": ["font", "typography", "heading", "serif", "sans"],
        "icons": ["icon", "icons", "lucide", "heroicons", "symbol", "glyph", "pictogram", "svg icon"],
        "react": ["react", "next.js", "nextjs", "suspense", "memo", "usecallback", "useeffect", "rerender", "bundle", "waterfall", "barrel", "dynamic import", "rsc", "server component"],
        "web": ["aria", "focus", "outline", "semantic", "virtualize", "autocomplete", "form", "input type", "preconnect"]
    }

    scores = {domain: sum(1 for kw in keywords if kw in query_lower) for domain, keywords in domain_keywords.items()}
    best = max(scores, key=scores.get)
    return best if scores[best] > 0 else "style"


def search(query, domain=None, max_results=MAX_RESULTS):
    """Main search function with auto-domain detection"""
    if domain is None:
        domain = detect_domain(query)

    config = CSV_CONFIG.get(domain, CSV_CONFIG["style"])
    filepath = DATA_DIR / config["file"]

    if not filepath.exists():
        return {"error": f"File not found: {filepath}", "domain": domain}

    results = _search_csv(filepath, config["search_cols"], config["output_cols"], query, max_results)

    return {
        "domain": domain,
        "query": query,
        "file": config["file"],
        "count": len(results),
        "results": results
    }


def search_stack(query, stack, max_results=MAX_RESULTS):
    """Search stack-specific guidelines"""
    if stack not in STACK_CONFIG:
        return {"error": f"Unknown stack: {stack}. Available: {', '.join(AVAILABLE_STACKS)}"}

    filepath = DATA_DIR / STACK_CONFIG[stack]["file"]

    if not filepath.exists():
        return {"error": f"Stack file not found: {filepath}", "stack": stack}

    results = _search_csv(filepath, _STACK_COLS["search_cols"], _STACK_COLS["output_cols"], query, max_results)

    return {
        "domain": "stack",
        "stack": stack,
        "query": query,
        "file": STACK_CONFIG[stack]["file"],
        "count": len(results),
        "results": results
    }

--- ARQUIVO: .agent/.shared/ui-ux-pro-max/scripts/design_system.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Design System Generator - Aggregates search results and applies reasoning
to generate comprehensive design system recommendations.

Usage:
    from design_system import generate_design_system
    result = generate_design_system("SaaS dashboard", "My Project")
    
    # With persistence (Master + Overrides pattern)
    result = generate_design_system("SaaS dashboard", "My Project", persist=True)
    result = generate_design_system("SaaS dashboard", "My Project", persist=True, page="dashboard")
"""

import csv
import json
import os
from datetime import datetime
from pathlib import Path
from core import search, DATA_DIR


# ============ CONFIGURATION ============
REASONING_FILE = "ui-reasoning.csv"

SEARCH_CONFIG = {
    "product": {"max_results": 1},
    "style": {"max_results": 3},
    "color": {"max_results": 2},
    "landing": {"max_results": 2},
    "typography": {"max_results": 2}
}


# ============ DESIGN SYSTEM GENERATOR ============
class DesignSystemGenerator:
    """Generates design system recommendations from aggregated searches."""

    def __init__(self):
        self.reasoning_data = self._load_reasoning()

    def _load_reasoning(self) -> list:
        """Load reasoning rules from CSV."""
        filepath = DATA_DIR / REASONING_FILE
        if not filepath.exists():
            return []
        with open(filepath, 'r', encoding='utf-8') as f:
            return list(csv.DictReader(f))

    def _multi_domain_search(self, query: str, style_priority: list = None) -> dict:
        """Execute searches across multiple domains."""
        results = {}
        for domain, config in SEARCH_CONFIG.items():
            if domain == "style" and style_priority:
                # For style, also search with priority keywords
                priority_query = " ".join(style_priority[:2]) if style_priority else query
                combined_query = f"{query} {priority_query}"
                results[domain] = search(combined_query, domain, config["max_results"])
            else:
                results[domain] = search(query, domain, config["max_results"])
        return results

    def _find_reasoning_rule(self, category: str) -> dict:
        """Find matching reasoning rule for a category."""
        category_lower = category.lower()

        # Try exact match first
        for rule in self.reasoning_data:
            if rule.get("UI_Category", "").lower() == category_lower:
                return rule

        # Try partial match
        for rule in self.reasoning_data:
            ui_cat = rule.get("UI_Category", "").lower()
            if ui_cat in category_lower or category_lower in ui_cat:
                return rule

        # Try keyword match
        for rule in self.reasoning_data:
            ui_cat = rule.get("UI_Category", "").lower()
            keywords = ui_cat.replace("/", " ").replace("-", " ").split()
            if any(kw in category_lower for kw in keywords):
                return rule

        return {}

    def _apply_reasoning(self, category: str, search_results: dict) -> dict:
        """Apply reasoning rules to search results."""
        rule = self._find_reasoning_rule(category)

        if not rule:
            return {
                "pattern": "Hero + Features + CTA",
                "style_priority": ["Minimalism", "Flat Design"],
                "color_mood": "Professional",
                "typography_mood": "Clean",
                "key_effects": "Subtle hover transitions",
                "anti_patterns": "",
                "decision_rules": {},
                "severity": "MEDIUM"
            }

        # Parse decision rules JSON
        decision_rules = {}
        try:
            decision_rules = json.loads(rule.get("Decision_Rules", "{}"))
        except json.JSONDecodeError:
            pass

        return {
            "pattern": rule.get("Recommended_Pattern", ""),
            "style_priority": [s.strip() for s in rule.get("Style_Priority", "").split("+")],
            "color_mood": rule.get("Color_Mood", ""),
            "typography_mood": rule.get("Typography_Mood", ""),
            "key_effects": rule.get("Key_Effects", ""),
            "anti_patterns": rule.get("Anti_Patterns", ""),
            "decision_rules": decision_rules,
            "severity": rule.get("Severity", "MEDIUM")
        }

    def _select_best_match(self, results: list, priority_keywords: list) -> dict:
        """Select best matching result based on priority keywords."""
        if not results:
            return {}

        if not priority_keywords:
            return results[0]

        # First: try exact style name match
        for priority in priority_keywords:
            priority_lower = priority.lower().strip()
            for result in results:
                style_name = result.get("Style Category", "").lower()
                if priority_lower in style_name or style_name in priority_lower:
                    return result

        # Second: score by keyword match in all fields
        scored = []
        for result in results:
            result_str = str(result).lower()
            score = 0
            for kw in priority_keywords:
                kw_lower = kw.lower().strip()
                # Higher score for style name match
                if kw_lower in result.get("Style Category", "").lower():
                    score += 10
                # Lower score for keyword field match
                elif kw_lower in result.get("Keywords", "").lower():
                    score += 3
                # Even lower for other field matches
                elif kw_lower in result_str:
                    score += 1
            scored.append((score, result))

        scored.sort(key=lambda x: x[0], reverse=True)
        return scored[0][1] if scored and scored[0][0] > 0 else results[0]

    def _extract_results(self, search_result: dict) -> list:
        """Extract results list from search result dict."""
        return search_result.get("results", [])

    def generate(self, query: str, project_name: str = None) -> dict:
        """Generate complete design system recommendation."""
        # Step 1: First search product to get category
        product_result = search(query, "product", 1)
        product_results = product_result.get("results", [])
        category = "General"
        if product_results:
            category = product_results[0].get("Product Type", "General")

        # Step 2: Get reasoning rules for this category
        reasoning = self._apply_reasoning(category, {})
        style_priority = reasoning.get("style_priority", [])

        # Step 3: Multi-domain search with style priority hints
        search_results = self._multi_domain_search(query, style_priority)
        search_results["product"] = product_result  # Reuse product search

        # Step 4: Select best matches from each domain using priority
        style_results = self._extract_results(search_results.get("style", {}))
        color_results = self._extract_results(search_results.get("color", {}))
        typography_results = self._extract_results(search_results.get("typography", {}))
        landing_results = self._extract_results(search_results.get("landing", {}))

        best_style = self._select_best_match(style_results, reasoning.get("style_priority", []))
        best_color = color_results[0] if color_results else {}
        best_typography = typography_results[0] if typography_results else {}
        best_landing = landing_results[0] if landing_results else {}

        # Step 5: Build final recommendation
        # Combine effects from both reasoning and style search
        style_effects = best_style.get("Effects & Animation", "")
        reasoning_effects = reasoning.get("key_effects", "")
        combined_effects = style_effects if style_effects else reasoning_effects

        return {
            "project_name": project_name or query.upper(),
            "category": category,
            "pattern": {
                "name": best_landing.get("Pattern Name", reasoning.get("pattern", "Hero + Features + CTA")),
                "sections": best_landing.get("Section Order", "Hero > Features > CTA"),
                "cta_placement": best_landing.get("Primary CTA Placement", "Above fold"),
                "color_strategy": best_landing.get("Color Strategy", ""),
                "conversion": best_landing.get("Conversion Optimization", "")
            },
            "style": {
                "name": best_style.get("Style Category", "Minimalism"),
                "type": best_style.get("Type", "General"),
                "effects": style_effects,
                "keywords": best_style.get("Keywords", ""),
                "best_for": best_style.get("Best For", ""),
                "performance": best_style.get("Performance", ""),
                "accessibility": best_style.get("Accessibility", "")
            },
            "colors": {
                "primary": best_color.get("Primary (Hex)", "#2563EB"),
                "secondary": best_color.get("Secondary (Hex)", "#3B82F6"),
                "cta": best_color.get("CTA (Hex)", "#F97316"),
                "background": best_color.get("Background (Hex)", "#F8FAFC"),
                "text": best_color.get("Text (Hex)", "#1E293B"),
                "notes": best_color.get("Notes", "")
            },
            "typography": {
                "heading": best_typography.get("Heading Font", "Inter"),
                "body": best_typography.get("Body Font", "Inter"),
                "mood": best_typography.get("Mood/Style Keywords", reasoning.get("typography_mood", "")),
                "best_for": best_typography.get("Best For", ""),
                "google_fonts_url": best_typography.get("Google Fonts URL", ""),
                "css_import": best_typography.get("CSS Import", "")
            },
            "key_effects": combined_effects,
            "anti_patterns": reasoning.get("anti_patterns", ""),
            "decision_rules": reasoning.get("decision_rules", {}),
            "severity": reasoning.get("severity", "MEDIUM")
        }


# ============ OUTPUT FORMATTERS ============
BOX_WIDTH = 90  # Wider box for more content

def format_ascii_box(design_system: dict) -> str:
    """Format design system as ASCII box with emojis (MCP-style)."""
    project = design_system.get("project_name", "PROJECT")
    pattern = design_system.get("pattern", {})
    style = design_system.get("style", {})
    colors = design_system.get("colors", {})
    typography = design_system.get("typography", {})
    effects = design_system.get("key_effects", "")
    anti_patterns = design_system.get("anti_patterns", "")

    def wrap_text(text: str, prefix: str, width: int) -> list:
        """Wrap long text into multiple lines."""
        if not text:
            return []
        words = text.split()
        lines = []
        current_line = prefix
        for word in words:
            if len(current_line) + len(word) + 1 <= width - 2:
                current_line += (" " if current_line != prefix else "") + word
            else:
                if current_line != prefix:
                    lines.append(current_line)
                current_line = prefix + word
        if current_line != prefix:
            lines.append(current_line)
        return lines

    # Build sections from pattern
    sections = pattern.get("sections", "").split(">")
    sections = [s.strip() for s in sections if s.strip()]

    # Build output lines
    lines = []
    w = BOX_WIDTH - 1

    lines.append("+" + "-" * w + "+")
    lines.append(f"|  TARGET: {project} - RECOMMENDED DESIGN SYSTEM".ljust(BOX_WIDTH) + "|")
    lines.append("+" + "-" * w + "+")
    lines.append("|" + " " * BOX_WIDTH + "|")

    # Pattern section
    lines.append(f"|  PATTERN: {pattern.get('name', '')}".ljust(BOX_WIDTH) + "|")
    if pattern.get('conversion'):
        lines.append(f"|     Conversion: {pattern.get('conversion', '')}".ljust(BOX_WIDTH) + "|")
    if pattern.get('cta_placement'):
        lines.append(f"|     CTA: {pattern.get('cta_placement', '')}".ljust(BOX_WIDTH) + "|")
    lines.append("|     Sections:".ljust(BOX_WIDTH) + "|")
    for i, section in enumerate(sections, 1):
        lines.append(f"|       {i}. {section}".ljust(BOX_WIDTH) + "|")
    lines.append("|" + " " * BOX_WIDTH + "|")

    # Style section
    lines.append(f"|  STYLE: {style.get('name', '')}".ljust(BOX_WIDTH) + "|")
    if style.get("keywords"):
        for line in wrap_text(f"Keywords: {style.get('keywords', '')}", "|     ", BOX_WIDTH):
            lines.append(line.ljust(BOX_WIDTH) + "|")
    if style.get("best_for"):
        for line in wrap_text(f"Best For: {style.get('best_for', '')}", "|     ", BOX_WIDTH):
            lines.append(line.ljust(BOX_WIDTH) + "|")
    if style.get("performance") or style.get("accessibility"):
        perf_a11y = f"Performance: {style.get('performance', '')} | Accessibility: {style.get('accessibility', '')}"
        lines.append(f"|     {perf_a11y}".ljust(BOX_WIDTH) + "|")
    lines.append("|" + " " * BOX_WIDTH + "|")

    # Colors section
    lines.append("|  COLORS:".ljust(BOX_WIDTH) + "|")
    lines.append(f"|     Primary:    {colors.get('primary', '')}".ljust(BOX_WIDTH) + "|")
    lines.append(f"|     Secondary:  {colors.get('secondary', '')}".ljust(BOX_WIDTH) + "|")
    lines.append(f"|     CTA:        {colors.get('cta', '')}".ljust(BOX_WIDTH) + "|")
    lines.append(f"|     Background: {colors.get('background', '')}".ljust(BOX_WIDTH) + "|")
    lines.append(f"|     Text:       {colors.get('text', '')}".ljust(BOX_WIDTH) + "|")
    if colors.get("notes"):
        for line in wrap_text(f"Notes: {colors.get('notes', '')}", "|     ", BOX_WIDTH):
            lines.append(line.ljust(BOX_WIDTH) + "|")
    lines.append("|" + " " * BOX_WIDTH + "|")

    # Typography section
    lines.append(f"|  TYPOGRAPHY: {typography.get('heading', '')} / {typography.get('body', '')}".ljust(BOX_WIDTH) + "|")
    if typography.get("mood"):
        for line in wrap_text(f"Mood: {typography.get('mood', '')}", "|     ", BOX_WIDTH):
            lines.append(line.ljust(BOX_WIDTH) + "|")
    if typography.get("best_for"):
        for line in wrap_text(f"Best For: {typography.get('best_for', '')}", "|     ", BOX_WIDTH):
            lines.append(line.ljust(BOX_WIDTH) + "|")
    if typography.get("google_fonts_url"):
        lines.append(f"|     Google Fonts: {typography.get('google_fonts_url', '')}".ljust(BOX_WIDTH) + "|")
    if typography.get("css_import"):
        lines.append(f"|     CSS Import: {typography.get('css_import', '')[:70]}...".ljust(BOX_WIDTH) + "|")
    lines.append("|" + " " * BOX_WIDTH + "|")

    # Key Effects section
    if effects:
        lines.append("|  KEY EFFECTS:".ljust(BOX_WIDTH) + "|")
        for line in wrap_text(effects, "|     ", BOX_WIDTH):
            lines.append(line.ljust(BOX_WIDTH) + "|")
        lines.append("|" + " " * BOX_WIDTH + "|")

    # Anti-patterns section
    if anti_patterns:
        lines.append("|  AVOID (Anti-patterns):".ljust(BOX_WIDTH) + "|")
        for line in wrap_text(anti_patterns, "|     ", BOX_WIDTH):
            lines.append(line.ljust(BOX_WIDTH) + "|")
        lines.append("|" + " " * BOX_WIDTH + "|")

    # Pre-Delivery Checklist section
    lines.append("|  PRE-DELIVERY CHECKLIST:".ljust(BOX_WIDTH) + "|")
    checklist_items = [
        "[ ] No emojis as icons (use SVG: Heroicons/Lucide)",
        "[ ] cursor-pointer on all clickable elements",
        "[ ] Hover states with smooth transitions (150-300ms)",
        "[ ] Light mode: text contrast 4.5:1 minimum",
        "[ ] Focus states visible for keyboard nav",
        "[ ] prefers-reduced-motion respected",
        "[ ] Responsive: 375px, 768px, 1024px, 1440px"
    ]
    for item in checklist_items:
        lines.append(f"|     {item}".ljust(BOX_WIDTH) + "|")
    lines.append("|" + " " * BOX_WIDTH + "|")

    lines.append("+" + "-" * w + "+")

    return "\n".join(lines)


def format_markdown(design_system: dict) -> str:
    """Format design system as markdown."""
    project = design_system.get("project_name", "PROJECT")
    pattern = design_system.get("pattern", {})
    style = design_system.get("style", {})
    colors = design_system.get("colors", {})
    typography = design_system.get("typography", {})
    effects = design_system.get("key_effects", "")
    anti_patterns = design_system.get("anti_patterns", "")

    lines = []
    lines.append(f"## Design System: {project}")
    lines.append("")

    # Pattern section
    lines.append("### Pattern")
    lines.append(f"- **Name:** {pattern.get('name', '')}")
    if pattern.get('conversion'):
        lines.append(f"- **Conversion Focus:** {pattern.get('conversion', '')}")
    if pattern.get('cta_placement'):
        lines.append(f"- **CTA Placement:** {pattern.get('cta_placement', '')}")
    if pattern.get('color_strategy'):
        lines.append(f"- **Color Strategy:** {pattern.get('color_strategy', '')}")
    lines.append(f"- **Sections:** {pattern.get('sections', '')}")
    lines.append("")

    # Style section
    lines.append("### Style")
    lines.append(f"- **Name:** {style.get('name', '')}")
    if style.get('keywords'):
        lines.append(f"- **Keywords:** {style.get('keywords', '')}")
    if style.get('best_for'):
        lines.append(f"- **Best For:** {style.get('best_for', '')}")
    if style.get('performance') or style.get('accessibility'):
        lines.append(f"- **Performance:** {style.get('performance', '')} | **Accessibility:** {style.get('accessibility', '')}")
    lines.append("")

    # Colors section
    lines.append("### Colors")
    lines.append(f"| Role | Hex |")
    lines.append(f"|------|-----|")
    lines.append(f"| Primary | {colors.get('primary', '')} |")
    lines.append(f"| Secondary | {colors.get('secondary', '')} |")
    lines.append(f"| CTA | {colors.get('cta', '')} |")
    lines.append(f"| Background | {colors.get('background', '')} |")
    lines.append(f"| Text | {colors.get('text', '')} |")
    if colors.get("notes"):
        lines.append(f"\n*Notes: {colors.get('notes', '')}*")
    lines.append("")

    # Typography section
    lines.append("### Typography")
    lines.append(f"- **Heading:** {typography.get('heading', '')}")
    lines.append(f"- **Body:** {typography.get('body', '')}")
    if typography.get("mood"):
        lines.append(f"- **Mood:** {typography.get('mood', '')}")
    if typography.get("best_for"):
        lines.append(f"- **Best For:** {typography.get('best_for', '')}")
    if typography.get("google_fonts_url"):
        lines.append(f"- **Google Fonts:** {typography.get('google_fonts_url', '')}")
    if typography.get("css_import"):
        lines.append(f"- **CSS Import:**")
        lines.append(f"```css")
        lines.append(f"{typography.get('css_import', '')}")
        lines.append(f"```")
    lines.append("")

    # Key Effects section
    if effects:
        lines.append("### Key Effects")
        lines.append(f"{effects}")
        lines.append("")

    # Anti-patterns section
    if anti_patterns:
        lines.append("### Avoid (Anti-patterns)")
        newline_bullet = '\n- '
        lines.append(f"- {anti_patterns.replace(' + ', newline_bullet)}")
        lines.append("")

    # Pre-Delivery Checklist section
    lines.append("### Pre-Delivery Checklist")
    lines.append("- [ ] No emojis as icons (use SVG: Heroicons/Lucide)")
    lines.append("- [ ] cursor-pointer on all clickable elements")
    lines.append("- [ ] Hover states with smooth transitions (150-300ms)")
    lines.append("- [ ] Light mode: text contrast 4.5:1 minimum")
    lines.append("- [ ] Focus states visible for keyboard nav")
    lines.append("- [ ] prefers-reduced-motion respected")
    lines.append("- [ ] Responsive: 375px, 768px, 1024px, 1440px")
    lines.append("")

    return "\n".join(lines)


# ============ MAIN ENTRY POINT ============
def generate_design_system(query: str, project_name: str = None, output_format: str = "ascii", 
                           persist: bool = False, page: str = None, output_dir: str = None) -> str:
    """
    Main entry point for design system generation.

    Args:
        query: Search query (e.g., "SaaS dashboard", "e-commerce luxury")
        project_name: Optional project name for output header
        output_format: "ascii" (default) or "markdown"
        persist: If True, save design system to design-system/ folder
        page: Optional page name for page-specific override file
        output_dir: Optional output directory (defaults to current working directory)

    Returns:
        Formatted design system string
    """
    generator = DesignSystemGenerator()
    design_system = generator.generate(query, project_name)
    
    # Persist to files if requested
    if persist:
        persist_design_system(design_system, page, output_dir, query)

    if output_format == "markdown":
        return format_markdown(design_system)
    return format_ascii_box(design_system)


# ============ PERSISTENCE FUNCTIONS ============
def persist_design_system(design_system: dict, page: str = None, output_dir: str = None, page_query: str = None) -> dict:
    """
    Persist design system to design-system/<project>/ folder using Master + Overrides pattern.
    
    Args:
        design_system: The generated design system dictionary
        page: Optional page name for page-specific override file
        output_dir: Optional output directory (defaults to current working directory)
        page_query: Optional query string for intelligent page override generation
    
    Returns:
        dict with created file paths and status
    """
    base_dir = Path(output_dir) if output_dir else Path.cwd()
    
    # Use project name for project-specific folder
    project_name = design_system.get("project_name", "default")
    project_slug = project_name.lower().replace(' ', '-')
    
    design_system_dir = base_dir / "design-system" / project_slug
    pages_dir = design_system_dir / "pages"
    
    created_files = []
    
    # Create directories
    design_system_dir.mkdir(parents=True, exist_ok=True)
    pages_dir.mkdir(parents=True, exist_ok=True)
    
    master_file = design_system_dir / "MASTER.md"
    
    # Generate and write MASTER.md
    master_content = format_master_md(design_system)
    with open(master_file, 'w', encoding='utf-8') as f:
        f.write(master_content)
    created_files.append(str(master_file))
    
    # If page is specified, create page override file with intelligent content
    if page:
        page_file = pages_dir / f"{page.lower().replace(' ', '-')}.md"
        page_content = format_page_override_md(design_system, page, page_query)
        with open(page_file, 'w', encoding='utf-8') as f:
            f.write(page_content)
        created_files.append(str(page_file))
    
    return {
        "status": "success",
        "design_system_dir": str(design_system_dir),
        "created_files": created_files
    }


def format_master_md(design_system: dict) -> str:
    """Format design system as MASTER.md with hierarchical override logic."""
    project = design_system.get("project_name", "PROJECT")
    pattern = design_system.get("pattern", {})
    style = design_system.get("style", {})
    colors = design_system.get("colors", {})
    typography = design_system.get("typography", {})
    effects = design_system.get("key_effects", "")
    anti_patterns = design_system.get("anti_patterns", "")
    
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    lines = []
    
    # Logic header
    lines.append("# Design System Master File")
    lines.append("")
    lines.append("> **LOGIC:** When building a specific page, first check `design-system/pages/[page-name].md`.")
    lines.append("> If that file exists, its rules **override** this Master file.")
    lines.append("> If not, strictly follow the rules below.")
    lines.append("")
    lines.append("---")
    lines.append("")
    lines.append(f"**Project:** {project}")
    lines.append(f"**Generated:** {timestamp}")
    lines.append(f"**Category:** {design_system.get('category', 'General')}")
    lines.append("")
    lines.append("---")
    lines.append("")
    
    # Global Rules section
    lines.append("## Global Rules")
    lines.append("")
    
    # Color Palette
    lines.append("### Color Palette")
    lines.append("")
    lines.append("| Role | Hex | CSS Variable |")
    lines.append("|------|-----|--------------|")
    lines.append(f"| Primary | `{colors.get('primary', '#2563EB')}` | `--color-primary` |")
    lines.append(f"| Secondary | `{colors.get('secondary', '#3B82F6')}` | `--color-secondary` |")
    lines.append(f"| CTA/Accent | `{colors.get('cta', '#F97316')}` | `--color-cta` |")
    lines.append(f"| Background | `{colors.get('background', '#F8FAFC')}` | `--color-background` |")
    lines.append(f"| Text | `{colors.get('text', '#1E293B')}` | `--color-text` |")
    lines.append("")
    if colors.get("notes"):
        lines.append(f"**Color Notes:** {colors.get('notes', '')}")
        lines.append("")
    
    # Typography
    lines.append("### Typography")
    lines.append("")
    lines.append(f"- **Heading Font:** {typography.get('heading', 'Inter')}")
    lines.append(f"- **Body Font:** {typography.get('body', 'Inter')}")
    if typography.get("mood"):
        lines.append(f"- **Mood:** {typography.get('mood', '')}")
    if typography.get("google_fonts_url"):
        lines.append(f"- **Google Fonts:** [{typography.get('heading', '')} + {typography.get('body', '')}]({typography.get('google_fonts_url', '')})")
    lines.append("")
    if typography.get("css_import"):
        lines.append("**CSS Import:**")
        lines.append("```css")
        lines.append(typography.get("css_import", ""))
        lines.append("```")
        lines.append("")
    
    # Spacing Variables
    lines.append("### Spacing Variables")
    lines.append("")
    lines.append("| Token | Value | Usage |")
    lines.append("|-------|-------|-------|")
    lines.append("| `--space-xs` | `4px` / `0.25rem` | Tight gaps |")
    lines.append("| `--space-sm` | `8px` / `0.5rem` | Icon gaps, inline spacing |")
    lines.append("| `--space-md` | `16px` / `1rem` | Standard padding |")
    lines.append("| `--space-lg` | `24px` / `1.5rem` | Section padding |")
    lines.append("| `--space-xl` | `32px` / `2rem` | Large gaps |")
    lines.append("| `--space-2xl` | `48px` / `3rem` | Section margins |")
    lines.append("| `--space-3xl` | `64px` / `4rem` | Hero padding |")
    lines.append("")
    
    # Shadow Depths
    lines.append("### Shadow Depths")
    lines.append("")
    lines.append("| Level | Value | Usage |")
    lines.append("|-------|-------|-------|")
    lines.append("| `--shadow-sm` | `0 1px 2px rgba(0,0,0,0.05)` | Subtle lift |")
    lines.append("| `--shadow-md` | `0 4px 6px rgba(0,0,0,0.1)` | Cards, buttons |")
    lines.append("| `--shadow-lg` | `0 10px 15px rgba(0,0,0,0.1)` | Modals, dropdowns |")
    lines.append("| `--shadow-xl` | `0 20px 25px rgba(0,0,0,0.15)` | Hero images, featured cards |")
    lines.append("")
    
    # Component Specs section
    lines.append("---")
    lines.append("")
    lines.append("## Component Specs")
    lines.append("")
    
    # Buttons
    lines.append("### Buttons")
    lines.append("")
    lines.append("```css")
    lines.append("/* Primary Button */")
    lines.append(".btn-primary {")
    lines.append(f"  background: {colors.get('cta', '#F97316')};")
    lines.append("  color: white;")
    lines.append("  padding: 12px 24px;")
    lines.append("  border-radius: 8px;")
    lines.append("  font-weight: 600;")
    lines.append("  transition: all 200ms ease;")
    lines.append("  cursor: pointer;")
    lines.append("}")
    lines.append("")
    lines.append(".btn-primary:hover {")
    lines.append("  opacity: 0.9;")
    lines.append("  transform: translateY(-1px);")
    lines.append("}")
    lines.append("")
    lines.append("/* Secondary Button */")
    lines.append(".btn-secondary {")
    lines.append(f"  background: transparent;")
    lines.append(f"  color: {colors.get('primary', '#2563EB')};")
    lines.append(f"  border: 2px solid {colors.get('primary', '#2563EB')};")
    lines.append("  padding: 12px 24px;")
    lines.append("  border-radius: 8px;")
    lines.append("  font-weight: 600;")
    lines.append("  transition: all 200ms ease;")
    lines.append("  cursor: pointer;")
    lines.append("}")
    lines.append("```")
    lines.append("")
    
    # Cards
    lines.append("### Cards")
    lines.append("")
    lines.append("```css")
    lines.append(".card {")
    lines.append(f"  background: {colors.get('background', '#FFFFFF')};")
    lines.append("  border-radius: 12px;")
    lines.append("  padding: 24px;")
    lines.append("  box-shadow: var(--shadow-md);")
    lines.append("  transition: all 200ms ease;")
    lines.append("  cursor: pointer;")
    lines.append("}")
    lines.append("")
    lines.append(".card:hover {")
    lines.append("  box-shadow: var(--shadow-lg);")
    lines.append("  transform: translateY(-2px);")
    lines.append("}")
    lines.append("```")
    lines.append("")
    
    # Inputs
    lines.append("### Inputs")
    lines.append("")
    lines.append("```css")
    lines.append(".input {")
    lines.append("  padding: 12px 16px;")
    lines.append("  border: 1px solid #E2E8F0;")
    lines.append("  border-radius: 8px;")
    lines.append("  font-size: 16px;")
    lines.append("  transition: border-color 200ms ease;")
    lines.append("}")
    lines.append("")
    lines.append(".input:focus {")
    lines.append(f"  border-color: {colors.get('primary', '#2563EB')};")
    lines.append("  outline: none;")
    lines.append(f"  box-shadow: 0 0 0 3px {colors.get('primary', '#2563EB')}20;")
    lines.append("}")
    lines.append("```")
    lines.append("")
    
    # Modals
    lines.append("### Modals")
    lines.append("")
    lines.append("```css")
    lines.append(".modal-overlay {")
    lines.append("  background: rgba(0, 0, 0, 0.5);")
    lines.append("  backdrop-filter: blur(4px);")
    lines.append("}")
    lines.append("")
    lines.append(".modal {")
    lines.append("  background: white;")
    lines.append("  border-radius: 16px;")
    lines.append("  padding: 32px;")
    lines.append("  box-shadow: var(--shadow-xl);")
    lines.append("  max-width: 500px;")
    lines.append("  width: 90%;")
    lines.append("}")
    lines.append("```")
    lines.append("")
    
    # Style section
    lines.append("---")
    lines.append("")
    lines.append("## Style Guidelines")
    lines.append("")
    lines.append(f"**Style:** {style.get('name', 'Minimalism')}")
    lines.append("")
    if style.get("keywords"):
        lines.append(f"**Keywords:** {style.get('keywords', '')}")
        lines.append("")
    if style.get("best_for"):
        lines.append(f"**Best For:** {style.get('best_for', '')}")
        lines.append("")
    if effects:
        lines.append(f"**Key Effects:** {effects}")
        lines.append("")
    
    # Layout Pattern
    lines.append("### Page Pattern")
    lines.append("")
    lines.append(f"**Pattern Name:** {pattern.get('name', '')}")
    lines.append("")
    if pattern.get('conversion'):
        lines.append(f"- **Conversion Strategy:** {pattern.get('conversion', '')}")
    if pattern.get('cta_placement'):
        lines.append(f"- **CTA Placement:** {pattern.get('cta_placement', '')}")
    lines.append(f"- **Section Order:** {pattern.get('sections', '')}")
    lines.append("")
    
    # Anti-Patterns section
    lines.append("---")
    lines.append("")
    lines.append("## Anti-Patterns (Do NOT Use)")
    lines.append("")
    if anti_patterns:
        anti_list = [a.strip() for a in anti_patterns.split("+")]
        for anti in anti_list:
            if anti:
                lines.append(f"- ❌ {anti}")
    lines.append("")
    lines.append("### Additional Forbidden Patterns")
    lines.append("")
    lines.append("- ❌ **Emojis as icons** — Use SVG icons (Heroicons, Lucide, Simple Icons)")
    lines.append("- ❌ **Missing cursor:pointer** — All clickable elements must have cursor:pointer")
    lines.append("- ❌ **Layout-shifting hovers** — Avoid scale transforms that shift layout")
    lines.append("- ❌ **Low contrast text** — Maintain 4.5:1 minimum contrast ratio")
    lines.append("- ❌ **Instant state changes** — Always use transitions (150-300ms)")
    lines.append("- ❌ **Invisible focus states** — Focus states must be visible for a11y")
    lines.append("")
    
    # Pre-Delivery Checklist
    lines.append("---")
    lines.append("")
    lines.append("## Pre-Delivery Checklist")
    lines.append("")
    lines.append("Before delivering any UI code, verify:")
    lines.append("")
    lines.append("- [ ] No emojis used as icons (use SVG instead)")
    lines.append("- [ ] All icons from consistent icon set (Heroicons/Lucide)")
    lines.append("- [ ] `cursor-pointer` on all clickable elements")
    lines.append("- [ ] Hover states with smooth transitions (150-300ms)")
    lines.append("- [ ] Light mode: text contrast 4.5:1 minimum")
    lines.append("- [ ] Focus states visible for keyboard navigation")
    lines.append("- [ ] `prefers-reduced-motion` respected")
    lines.append("- [ ] Responsive: 375px, 768px, 1024px, 1440px")
    lines.append("- [ ] No content hidden behind fixed navbars")
    lines.append("- [ ] No horizontal scroll on mobile")
    lines.append("")
    
    return "\n".join(lines)


def format_page_override_md(design_system: dict, page_name: str, page_query: str = None) -> str:
    """Format a page-specific override file with intelligent AI-generated content."""
    project = design_system.get("project_name", "PROJECT")
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    page_title = page_name.replace("-", " ").replace("_", " ").title()
    
    # Detect page type and generate intelligent overrides
    page_overrides = _generate_intelligent_overrides(page_name, page_query, design_system)
    
    lines = []
    
    lines.append(f"# {page_title} Page Overrides")
    lines.append("")
    lines.append(f"> **PROJECT:** {project}")
    lines.append(f"> **Generated:** {timestamp}")
    lines.append(f"> **Page Type:** {page_overrides.get('page_type', 'General')}")
    lines.append("")
    lines.append("> ⚠️ **IMPORTANT:** Rules in this file **override** the Master file (`design-system/MASTER.md`).")
    lines.append("> Only deviations from the Master are documented here. For all other rules, refer to the Master.")
    lines.append("")
    lines.append("---")
    lines.append("")
    
    # Page-specific rules with actual content
    lines.append("## Page-Specific Rules")
    lines.append("")
    
    # Layout Overrides
    lines.append("### Layout Overrides")
    lines.append("")
    layout = page_overrides.get("layout", {})
    if layout:
        for key, value in layout.items():
            lines.append(f"- **{key}:** {value}")
    else:
        lines.append("- No overrides — use Master layout")
    lines.append("")
    
    # Spacing Overrides
    lines.append("### Spacing Overrides")
    lines.append("")
    spacing = page_overrides.get("spacing", {})
    if spacing:
        for key, value in spacing.items():
            lines.append(f"- **{key}:** {value}")
    else:
        lines.append("- No overrides — use Master spacing")
    lines.append("")
    
    # Typography Overrides
    lines.append("### Typography Overrides")
    lines.append("")
    typography = page_overrides.get("typography", {})
    if typography:
        for key, value in typography.items():
            lines.append(f"- **{key}:** {value}")
    else:
        lines.append("- No overrides — use Master typography")
    lines.append("")
    
    # Color Overrides
    lines.append("### Color Overrides")
    lines.append("")
    colors = page_overrides.get("colors", {})
    if colors:
        for key, value in colors.items():
            lines.append(f"- **{key}:** {value}")
    else:
        lines.append("- No overrides — use Master colors")
    lines.append("")
    
    # Component Overrides
    lines.append("### Component Overrides")
    lines.append("")
    components = page_overrides.get("components", [])
    if components:
        for comp in components:
            lines.append(f"- {comp}")
    else:
        lines.append("- No overrides — use Master component specs")
    lines.append("")
    
    # Page-Specific Components
    lines.append("---")
    lines.append("")
    lines.append("## Page-Specific Components")
    lines.append("")
    unique_components = page_overrides.get("unique_components", [])
    if unique_components:
        for comp in unique_components:
            lines.append(f"- {comp}")
    else:
        lines.append("- No unique components for this page")
    lines.append("")
    
    # Recommendations
    lines.append("---")
    lines.append("")
    lines.append("## Recommendations")
    lines.append("")
    recommendations = page_overrides.get("recommendations", [])
    if recommendations:
        for rec in recommendations:
            lines.append(f"- {rec}")
    lines.append("")
    
    return "\n".join(lines)


def _generate_intelligent_overrides(page_name: str, page_query: str, design_system: dict) -> dict:
    """
    Generate intelligent overrides based on page type using layered search.
    
    Uses the existing search infrastructure to find relevant style, UX, and layout
    data instead of hardcoded page types.
    """
    from core import search
    
    page_lower = page_name.lower()
    query_lower = (page_query or "").lower()
    combined_context = f"{page_lower} {query_lower}"
    
    # Search across multiple domains for page-specific guidance
    style_search = search(combined_context, "style", max_results=1)
    ux_search = search(combined_context, "ux", max_results=3)
    landing_search = search(combined_context, "landing", max_results=1)
    
    # Extract results from search response
    style_results = style_search.get("results", [])
    ux_results = ux_search.get("results", [])
    landing_results = landing_search.get("results", [])
    
    # Detect page type from search results or context
    page_type = _detect_page_type(combined_context, style_results)
    
    # Build overrides from search results
    layout = {}
    spacing = {}
    typography = {}
    colors = {}
    components = []
    unique_components = []
    recommendations = []
    
    # Extract style-based overrides
    if style_results:
        style = style_results[0]
        style_name = style.get("Style Category", "")
        keywords = style.get("Keywords", "")
        best_for = style.get("Best For", "")
        effects = style.get("Effects & Animation", "")
        
        # Infer layout from style keywords
        if any(kw in keywords.lower() for kw in ["data", "dense", "dashboard", "grid"]):
            layout["Max Width"] = "1400px or full-width"
            layout["Grid"] = "12-column grid for data flexibility"
            spacing["Content Density"] = "High — optimize for information display"
        elif any(kw in keywords.lower() for kw in ["minimal", "simple", "clean", "single"]):
            layout["Max Width"] = "800px (narrow, focused)"
            layout["Layout"] = "Single column, centered"
            spacing["Content Density"] = "Low — focus on clarity"
        else:
            layout["Max Width"] = "1200px (standard)"
            layout["Layout"] = "Full-width sections, centered content"
        
        if effects:
            recommendations.append(f"Effects: {effects}")
    
    # Extract UX guidelines as recommendations
    for ux in ux_results:
        category = ux.get("Category", "")
        do_text = ux.get("Do", "")
        dont_text = ux.get("Don't", "")
        if do_text:
            recommendations.append(f"{category}: {do_text}")
        if dont_text:
            components.append(f"Avoid: {dont_text}")
    
    # Extract landing pattern info for section structure
    if landing_results:
        landing = landing_results[0]
        sections = landing.get("Section Order", "")
        cta_placement = landing.get("Primary CTA Placement", "")
        color_strategy = landing.get("Color Strategy", "")
        
        if sections:
            layout["Sections"] = sections
        if cta_placement:
            recommendations.append(f"CTA Placement: {cta_placement}")
        if color_strategy:
            colors["Strategy"] = color_strategy
    
    # Add page-type specific defaults if no search results
    if not layout:
        layout["Max Width"] = "1200px"
        layout["Layout"] = "Responsive grid"
    
    if not recommendations:
        recommendations = [
            "Refer to MASTER.md for all design rules",
            "Add specific overrides as needed for this page"
        ]
    
    return {
        "page_type": page_type,
        "layout": layout,
        "spacing": spacing,
        "typography": typography,
        "colors": colors,
        "components": components,
        "unique_components": unique_components,
        "recommendations": recommendations
    }


def _detect_page_type(context: str, style_results: list) -> str:
    """Detect page type from context and search results."""
    context_lower = context.lower()
    
    # Check for common page type patterns
    page_patterns = [
        (["dashboard", "admin", "analytics", "data", "metrics", "stats", "monitor", "overview"], "Dashboard / Data View"),
        (["checkout", "payment", "cart", "purchase", "order", "billing"], "Checkout / Payment"),
        (["settings", "profile", "account", "preferences", "config"], "Settings / Profile"),
        (["landing", "marketing", "homepage", "hero", "home", "promo"], "Landing / Marketing"),
        (["login", "signin", "signup", "register", "auth", "password"], "Authentication"),
        (["pricing", "plans", "subscription", "tiers", "packages"], "Pricing / Plans"),
        (["blog", "article", "post", "news", "content", "story"], "Blog / Article"),
        (["product", "item", "detail", "pdp", "shop", "store"], "Product Detail"),
        (["search", "results", "browse", "filter", "catalog", "list"], "Search Results"),
        (["empty", "404", "error", "not found", "zero"], "Empty State"),
    ]
    
    for keywords, page_type in page_patterns:
        if any(kw in context_lower for kw in keywords):
            return page_type
    
    # Fallback: try to infer from style results
    if style_results:
        style_name = style_results[0].get("Style Category", "").lower()
        best_for = style_results[0].get("Best For", "").lower()
        
        if "dashboard" in best_for or "data" in best_for:
            return "Dashboard / Data View"
        elif "landing" in best_for or "marketing" in best_for:
            return "Landing / Marketing"
    
    return "General"


# ============ CLI SUPPORT ============
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Generate Design System")
    parser.add_argument("query", help="Search query (e.g., 'SaaS dashboard')")
    parser.add_argument("--project-name", "-p", type=str, default=None, help="Project name")
    parser.add_argument("--format", "-f", choices=["ascii", "markdown"], default="ascii", help="Output format")

    args = parser.parse_args()

    result = generate_design_system(args.query, args.project_name, args.format)
    print(result)

--- ARQUIVO: .agent/.shared/ui-ux-pro-max/scripts/search.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
UI/UX Pro Max Search - BM25 search engine for UI/UX style guides
Usage: python search.py "<query>" [--domain <domain>] [--stack <stack>] [--max-results 3]
       python search.py "<query>" --design-system [-p "Project Name"]
       python search.py "<query>" --design-system --persist [-p "Project Name"] [--page "dashboard"]

Domains: style, prompt, color, chart, landing, product, ux, typography
Stacks: html-tailwind, react, nextjs

Persistence (Master + Overrides pattern):
  --persist    Save design system to design-system/MASTER.md
  --page       Also create a page-specific override file in design-system/pages/
"""

import argparse
from core import CSV_CONFIG, AVAILABLE_STACKS, MAX_RESULTS, search, search_stack
from design_system import generate_design_system, persist_design_system


def format_output(result):
    """Format results for Claude consumption (token-optimized)"""
    if "error" in result:
        return f"Error: {result['error']}"

    output = []
    if result.get("stack"):
        output.append(f"## UI Pro Max Stack Guidelines")
        output.append(f"**Stack:** {result['stack']} | **Query:** {result['query']}")
    else:
        output.append(f"## UI Pro Max Search Results")
        output.append(f"**Domain:** {result['domain']} | **Query:** {result['query']}")
    output.append(f"**Source:** {result['file']} | **Found:** {result['count']} results\n")

    for i, row in enumerate(result['results'], 1):
        output.append(f"### Result {i}")
        for key, value in row.items():
            value_str = str(value)
            if len(value_str) > 300:
                value_str = value_str[:300] + "..."
            output.append(f"- **{key}:** {value_str}")
        output.append("")

    return "\n".join(output)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="UI Pro Max Search")
    parser.add_argument("query", help="Search query")
    parser.add_argument("--domain", "-d", choices=list(CSV_CONFIG.keys()), help="Search domain")
    parser.add_argument("--stack", "-s", choices=AVAILABLE_STACKS, help="Stack-specific search (html-tailwind, react, nextjs)")
    parser.add_argument("--max-results", "-n", type=int, default=MAX_RESULTS, help="Max results (default: 3)")
    parser.add_argument("--json", action="store_true", help="Output as JSON")
    # Design system generation
    parser.add_argument("--design-system", "-ds", action="store_true", help="Generate complete design system recommendation")
    parser.add_argument("--project-name", "-p", type=str, default=None, help="Project name for design system output")
    parser.add_argument("--format", "-f", choices=["ascii", "markdown"], default="ascii", help="Output format for design system")
    # Persistence (Master + Overrides pattern)
    parser.add_argument("--persist", action="store_true", help="Save design system to design-system/MASTER.md (creates hierarchical structure)")
    parser.add_argument("--page", type=str, default=None, help="Create page-specific override file in design-system/pages/")
    parser.add_argument("--output-dir", "-o", type=str, default=None, help="Output directory for persisted files (default: current directory)")

    args = parser.parse_args()

    # Design system takes priority
    if args.design_system:
        result = generate_design_system(
            args.query, 
            args.project_name, 
            args.format,
            persist=args.persist,
            page=args.page,
            output_dir=args.output_dir
        )
        print(result)
        
        # Print persistence confirmation
        if args.persist:
            project_slug = args.project_name.lower().replace(' ', '-') if args.project_name else "default"
            print("\n" + "=" * 60)
            print(f"✅ Design system persisted to design-system/{project_slug}/")
            print(f"   📄 design-system/{project_slug}/MASTER.md (Global Source of Truth)")
            if args.page:
                page_filename = args.page.lower().replace(' ', '-')
                print(f"   📄 design-system/{project_slug}/pages/{page_filename}.md (Page Overrides)")
            print("")
            print(f"📖 Usage: When building a page, check design-system/{project_slug}/pages/[page].md first.")
            print(f"   If exists, its rules override MASTER.md. Otherwise, use MASTER.md.")
            print("=" * 60)
    # Stack search
    elif args.stack:
        result = search_stack(args.query, args.stack, args.max_results)
        if args.json:
            import json
            print(json.dumps(result, indent=2, ensure_ascii=False))
        else:
            print(format_output(result))
    # Domain search
    else:
        result = search(args.query, args.domain, args.max_results)
        if args.json:
            import json
            print(json.dumps(result, indent=2, ensure_ascii=False))
        else:
            print(format_output(result))


################################################################################
# PASTA: .agent/scripts
################################################################################

--- ARQUIVO: .agent/scripts/checklist.py ---
#!/usr/bin/env python3
"""
Master Checklist Runner - Antigravity Kit
==========================================

Orchestrates all validation scripts in priority order.
Use this for incremental validation during development.

Usage:
    python scripts/checklist.py .                    # Run core checks
    python scripts/checklist.py . --url <URL>        # Include performance checks

Priority Order:
    P0: Security Scan (vulnerabilities, secrets)
    P1: Lint & Type Check (code quality)
    P2: Schema Validation (if database exists)
    P3: Test Runner (unit/integration tests)
    P4: UX Audit (psychology laws, accessibility)
    P5: SEO Check (meta tags, structure)
    P6: Performance (lighthouse - requires URL)
"""

import sys
import subprocess
import argparse
from pathlib import Path
from typing import List, Tuple, Optional

# ANSI colors for terminal output
class Colors:
    HEADER = '\033[95m'
    BLUE = '\033[94m'
    CYAN = '\033[96m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    RED = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'

def print_header(text: str):
    print(f"\n{Colors.BOLD}{Colors.CYAN}{'='*60}{Colors.ENDC}")
    print(f"{Colors.BOLD}{Colors.CYAN}{text.center(60)}{Colors.ENDC}")
    print(f"{Colors.BOLD}{Colors.CYAN}{'='*60}{Colors.ENDC}\n")

def print_step(text: str):
    print(f"{Colors.BOLD}{Colors.BLUE}🔄 {text}{Colors.ENDC}")

def print_success(text: str):
    print(f"{Colors.GREEN}✅ {text}{Colors.ENDC}")

def print_warning(text: str):
    print(f"{Colors.YELLOW}⚠️  {text}{Colors.ENDC}")

def print_error(text: str):
    print(f"{Colors.RED}❌ {text}{Colors.ENDC}")

# Define priority-ordered checks
CORE_CHECKS = [
    ("Security Scan", ".agent/skills/vulnerability-scanner/scripts/security_scan.py", True),
    ("Lint Check", ".agent/skills/lint-and-validate/scripts/lint_runner.py", True),
    ("Schema Validation", ".agent/skills/database-design/scripts/schema_validator.py", False),
    ("Test Runner", ".agent/skills/testing-patterns/scripts/test_runner.py", False),
    ("UX Audit", ".agent/skills/frontend-design/scripts/ux_audit.py", False),
    ("SEO Check", ".agent/skills/seo-fundamentals/scripts/seo_checker.py", False),
]

PERFORMANCE_CHECKS = [
    ("Lighthouse Audit", ".agent/skills/performance-profiling/scripts/lighthouse_audit.py", True),
    ("Playwright E2E", ".agent/skills/webapp-testing/scripts/playwright_runner.py", False),
]

def check_script_exists(script_path: Path) -> bool:
    """Check if script file exists"""
    return script_path.exists() and script_path.is_file()

def run_script(name: str, script_path: Path, project_path: str, url: Optional[str] = None) -> dict:
    """
    Run a validation script and capture results
    
    Returns:
        dict with keys: name, passed, output, skipped
    """
    if not check_script_exists(script_path):
        print_warning(f"{name}: Script not found, skipping")
        return {"name": name, "passed": True, "output": "", "skipped": True}
    
    print_step(f"Running: {name}")
    
    # Build command
    cmd = ["python", str(script_path), project_path]
    if url and ("lighthouse" in script_path.name.lower() or "playwright" in script_path.name.lower()):
        cmd.append(url)
    
    # Run script
    try:
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=300  # 5 minute timeout
        )
        
        passed = result.returncode == 0
        
        if passed:
            print_success(f"{name}: PASSED")
        else:
            print_error(f"{name}: FAILED")
            if result.stderr:
                print(f"  Error: {result.stderr[:200]}")
        
        return {
            "name": name,
            "passed": passed,
            "output": result.stdout,
            "error": result.stderr,
            "skipped": False
        }
    
    except subprocess.TimeoutExpired:
        print_error(f"{name}: TIMEOUT (>5 minutes)")
        return {"name": name, "passed": False, "output": "", "error": "Timeout", "skipped": False}
    
    except Exception as e:
        print_error(f"{name}: ERROR - {str(e)}")
        return {"name": name, "passed": False, "output": "", "error": str(e), "skipped": False}

def print_summary(results: List[dict]):
    """Print final summary report"""
    print_header("📊 CHECKLIST SUMMARY")
    
    passed_count = sum(1 for r in results if r["passed"] and not r.get("skipped"))
    failed_count = sum(1 for r in results if not r["passed"] and not r.get("skipped"))
    skipped_count = sum(1 for r in results if r.get("skipped"))
    
    print(f"Total Checks: {len(results)}")
    print(f"{Colors.GREEN}✅ Passed: {passed_count}{Colors.ENDC}")
    print(f"{Colors.RED}❌ Failed: {failed_count}{Colors.ENDC}")
    print(f"{Colors.YELLOW}⏭️  Skipped: {skipped_count}{Colors.ENDC}")
    print()
    
    # Detailed results
    for r in results:
        if r.get("skipped"):
            status = f"{Colors.YELLOW}⏭️ {Colors.ENDC}"
        elif r["passed"]:
            status = f"{Colors.GREEN}✅{Colors.ENDC}"
        else:
            status = f"{Colors.RED}❌{Colors.ENDC}"
        
        print(f"{status} {r['name']}")
    
    print()
    
    if failed_count > 0:
        print_error(f"{failed_count} check(s) FAILED - Please fix before proceeding")
        return False
    else:
        print_success("All checks PASSED ✨")
        return True

def main():
    parser = argparse.ArgumentParser(
        description="Run Antigravity Kit validation checklist",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python scripts/checklist.py .                      # Core checks only
  python scripts/checklist.py . --url http://localhost:3000  # Include performance
        """
    )
    parser.add_argument("project", help="Project path to validate")
    parser.add_argument("--url", help="URL for performance checks (lighthouse, playwright)")
    parser.add_argument("--skip-performance", action="store_true", help="Skip performance checks even if URL provided")
    
    args = parser.parse_args()
    
    project_path = Path(args.project).resolve()
    
    if not project_path.exists():
        print_error(f"Project path does not exist: {project_path}")
        sys.exit(1)
    
    print_header("🚀 ANTIGRAVITY KIT - MASTER CHECKLIST")
    print(f"Project: {project_path}")
    print(f"URL: {args.url if args.url else 'Not provided (performance checks skipped)'}")
    
    results = []
    
    # Run core checks
    print_header("📋 CORE CHECKS")
    for name, script_path, required in CORE_CHECKS:
        script = project_path / script_path
        result = run_script(name, script, str(project_path))
        results.append(result)
        
        # If required check fails, stop
        if required and not result["passed"] and not result.get("skipped"):
            print_error(f"CRITICAL: {name} failed. Stopping checklist.")
            print_summary(results)
            sys.exit(1)
    
    # Run performance checks if URL provided
    if args.url and not args.skip_performance:
        print_header("⚡ PERFORMANCE CHECKS")
        for name, script_path, required in PERFORMANCE_CHECKS:
            script = project_path / script_path
            result = run_script(name, script, str(project_path), args.url)
            results.append(result)
    
    # Print summary
    all_passed = print_summary(results)
    
    sys.exit(0 if all_passed else 1)

if __name__ == "__main__":
    main()

--- ARQUIVO: .agent/scripts/verify_all.py ---
#!/usr/bin/env python3
"""
Full Verification Suite - Antigravity Kit
==========================================

Runs COMPLETE validation including all checks + performance + E2E.
Use this before deployment or major releases.

Usage:
    python scripts/verify_all.py . --url <URL>

Includes ALL checks:
    ✅ Security Scan (OWASP, secrets, dependencies)
    ✅ Lint & Type Coverage
    ✅ Schema Validation
    ✅ Test Suite (unit + integration)
    ✅ UX Audit (psychology, accessibility)
    ✅ SEO Check
    ✅ Lighthouse (Core Web Vitals)
    ✅ Playwright E2E
    ✅ Bundle Analysis (if applicable)
    ✅ Mobile Audit (if applicable)
"""

import sys
import subprocess
import argparse
from pathlib import Path
from typing import List, Dict, Optional
from datetime import datetime

# ANSI colors
class Colors:
    HEADER = '\033[95m'
    BLUE = '\033[94m'
    CYAN = '\033[96m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    RED = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'

def print_header(text: str):
    print(f"\n{Colors.BOLD}{Colors.CYAN}{'='*70}{Colors.ENDC}")
    print(f"{Colors.BOLD}{Colors.CYAN}{text.center(70)}{Colors.ENDC}")
    print(f"{Colors.BOLD}{Colors.CYAN}{'='*70}{Colors.ENDC}\n")

def print_step(text: str):
    print(f"{Colors.BOLD}{Colors.BLUE}🔄 {text}{Colors.ENDC}")

def print_success(text: str):
    print(f"{Colors.GREEN}✅ {text}{Colors.ENDC}")

def print_warning(text: str):
    print(f"{Colors.YELLOW}⚠️  {text}{Colors.ENDC}")

def print_error(text: str):
    print(f"{Colors.RED}❌ {text}{Colors.ENDC}")

# Complete verification suite
VERIFICATION_SUITE = [
    # P0: Security (CRITICAL)
    {
        "category": "Security",
        "checks": [
            ("Security Scan", ".agent/skills/vulnerability-scanner/scripts/security_scan.py", True),
            ("Dependency Analysis", ".agent/skills/vulnerability-scanner/scripts/dependency_analyzer.py", False),
        ]
    },
    
    # P1: Code Quality (CRITICAL)
    {
        "category": "Code Quality",
        "checks": [
            ("Lint Check", ".agent/skills/lint-and-validate/scripts/lint_runner.py", True),
            ("Type Coverage", ".agent/skills/lint-and-validate/scripts/type_coverage.py", False),
        ]
    },
    
    # P2: Data Layer
    {
        "category": "Data Layer",
        "checks": [
            ("Schema Validation", ".agent/skills/database-design/scripts/schema_validator.py", False),
        ]
    },
    
    # P3: Testing
    {
        "category": "Testing",
        "checks": [
            ("Test Suite", ".agent/skills/testing-patterns/scripts/test_runner.py", False),
        ]
    },
    
    # P4: UX & Accessibility
    {
        "category": "UX & Accessibility",
        "checks": [
            ("UX Audit", ".agent/skills/frontend-design/scripts/ux_audit.py", False),
            ("Accessibility Check", ".agent/skills/frontend-design/scripts/accessibility_checker.py", False),
        ]
    },
    
    # P5: SEO & Content
    {
        "category": "SEO & Content",
        "checks": [
            ("SEO Check", ".agent/skills/seo-fundamentals/scripts/seo_checker.py", False),
            ("GEO Check", ".agent/skills/geo-fundamentals/scripts/geo_checker.py", False),
        ]
    },
    
    # P6: Performance (requires URL)
    {
        "category": "Performance",
        "requires_url": True,
        "checks": [
            ("Lighthouse Audit", ".agent/skills/performance-profiling/scripts/lighthouse_audit.py", True),
            ("Bundle Analysis", ".agent/skills/performance-profiling/scripts/bundle_analyzer.py", False),
        ]
    },
    
    # P7: E2E Testing (requires URL)
    {
        "category": "E2E Testing",
        "requires_url": True,
        "checks": [
            ("Playwright E2E", ".agent/skills/webapp-testing/scripts/playwright_runner.py", False),
        ]
    },
    
    # P8: Mobile (if applicable)
    {
        "category": "Mobile",
        "checks": [
            ("Mobile Audit", ".agent/skills/mobile-design/scripts/mobile_audit.py", False),
        ]
    },
    
    # P9: Internationalization
    {
        "category": "Internationalization",
        "checks": [
            ("i18n Check", ".agent/skills/i18n-localization/scripts/i18n_checker.py", False),
        ]
    },
]

def run_script(name: str, script_path: Path, project_path: str, url: Optional[str] = None) -> dict:
    """Run validation script"""
    if not script_path.exists():
        print_warning(f"{name}: Script not found, skipping")
        return {"name": name, "passed": True, "skipped": True, "duration": 0}
    
    print_step(f"Running: {name}")
    start_time = datetime.now()
    
    # Build command
    cmd = ["python", str(script_path), project_path]
    if url and ("lighthouse" in script_path.name.lower() or "playwright" in script_path.name.lower()):
        cmd.append(url)
    
    # Run
    try:
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=600  # 10 minute timeout for slow checks
        )
        
        duration = (datetime.now() - start_time).total_seconds()
        passed = result.returncode == 0
        
        if passed:
            print_success(f"{name}: PASSED ({duration:.1f}s)")
        else:
            print_error(f"{name}: FAILED ({duration:.1f}s)")
            if result.stderr:
                print(f"  {result.stderr[:300]}")
        
        return {
            "name": name,
            "passed": passed,
            "output": result.stdout,
            "error": result.stderr,
            "skipped": False,
            "duration": duration
        }
    
    except subprocess.TimeoutExpired:
        duration = (datetime.now() - start_time).total_seconds()
        print_error(f"{name}: TIMEOUT (>{duration:.0f}s)")
        return {"name": name, "passed": False, "skipped": False, "duration": duration, "error": "Timeout"}
    
    except Exception as e:
        duration = (datetime.now() - start_time).total_seconds()
        print_error(f"{name}: ERROR - {str(e)}")
        return {"name": name, "passed": False, "skipped": False, "duration": duration, "error": str(e)}

def print_final_report(results: List[dict], start_time: datetime):
    """Print comprehensive final report"""
    total_duration = (datetime.now() - start_time).total_seconds()
    
    print_header("📊 FULL VERIFICATION REPORT")
    
    # Statistics
    total = len(results)
    passed = sum(1 for r in results if r["passed"] and not r.get("skipped"))
    failed = sum(1 for r in results if not r["passed"] and not r.get("skipped"))
    skipped = sum(1 for r in results if r.get("skipped"))
    
    print(f"Total Duration: {total_duration:.1f}s")
    print(f"Total Checks: {total}")
    print(f"{Colors.GREEN}✅ Passed: {passed}{Colors.ENDC}")
    print(f"{Colors.RED}❌ Failed: {failed}{Colors.ENDC}")
    print(f"{Colors.YELLOW}⏭️  Skipped: {skipped}{Colors.ENDC}")
    print()
    
    # Category breakdown
    print(f"{Colors.BOLD}Results by Category:{Colors.ENDC}")
    current_category = None
    for r in results:
        # Print category header if changed
        if r.get("category") and r["category"] != current_category:
            current_category = r["category"]
            print(f"\n{Colors.BOLD}{Colors.CYAN}{current_category}:{Colors.ENDC}")
        
        # Print result
        if r.get("skipped"):
            status = f"{Colors.YELLOW}⏭️ {Colors.ENDC}"
        elif r["passed"]:
            status = f"{Colors.GREEN}✅{Colors.ENDC}"
        else:
            status = f"{Colors.RED}❌{Colors.ENDC}"
        
        duration_str = f"({r.get('duration', 0):.1f}s)" if not r.get("skipped") else ""
        print(f"  {status} {r['name']} {duration_str}")
    
    print()
    
    # Failed checks detail
    if failed > 0:
        print(f"{Colors.BOLD}{Colors.RED}❌ FAILED CHECKS:{Colors.ENDC}")
        for r in results:
            if not r["passed"] and not r.get("skipped"):
                print(f"\n{Colors.RED}✗ {r['name']}{Colors.ENDC}")
                if r.get("error"):
                    error_preview = r["error"][:200]
                    print(f"  Error: {error_preview}")
        print()
    
    # Final verdict
    if failed > 0:
        print_error(f"VERIFICATION FAILED - {failed} check(s) need attention")
        print(f"\n{Colors.YELLOW}💡 Tip: Fix critical (security, lint) issues first{Colors.ENDC}")
        return False
    else:
        print_success("✨ ALL CHECKS PASSED - Ready for deployment! ✨")
        return True

def main():
    parser = argparse.ArgumentParser(
        description="Run complete Antigravity Kit verification suite",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python scripts/verify_all.py . --url http://localhost:3000
  python scripts/verify_all.py . --url https://staging.example.com --no-e2e
        """
    )
    parser.add_argument("project", help="Project path to validate")
    parser.add_argument("--url", required=True, help="URL for performance & E2E checks")
    parser.add_argument("--no-e2e", action="store_true", help="Skip E2E tests")
    parser.add_argument("--stop-on-fail", action="store_true", help="Stop on first failure")
    
    args = parser.parse_args()
    
    project_path = Path(args.project).resolve()
    
    if not project_path.exists():
        print_error(f"Project path does not exist: {project_path}")
        sys.exit(1)
    
    print_header("🚀 ANTIGRAVITY KIT - FULL VERIFICATION SUITE")
    print(f"Project: {project_path}")
    print(f"URL: {args.url}")
    print(f"Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    start_time = datetime.now()
    results = []
    
    # Run all verification categories
    for suite in VERIFICATION_SUITE:
        category = suite["category"]
        requires_url = suite.get("requires_url", False)
        
        # Skip if requires URL and not provided
        if requires_url and not args.url:
            continue
        
        # Skip E2E if flag set
        if args.no_e2e and category == "E2E Testing":
            continue
        
        print_header(f"📋 {category.upper()}")
        
        for name, script_path, required in suite["checks"]:
            script = project_path / script_path
            result = run_script(name, script, str(project_path), args.url)
            result["category"] = category
            results.append(result)
            
            # Stop on critical failure if flag set
            if args.stop_on_fail and required and not result["passed"] and not result.get("skipped"):
                print_error(f"CRITICAL: {name} failed. Stopping verification.")
                print_final_report(results, start_time)
                sys.exit(1)
    
    # Print final report
    all_passed = print_final_report(results, start_time)
    
    sys.exit(0 if all_passed else 1)

if __name__ == "__main__":
    main()


################################################################################
# PASTA: .agent/skills/api-patterns/scripts
################################################################################

--- ARQUIVO: .agent/skills/api-patterns/scripts/api_validator.py ---
#!/usr/bin/env python3
"""
API Validator - Checks API endpoints for best practices.
Validates OpenAPI specs, response formats, and common issues.
"""
import sys
import json
import re
from pathlib import Path

# Fix Windows console encoding for Unicode output
try:
    sys.stdout.reconfigure(encoding='utf-8', errors='replace')
    sys.stderr.reconfigure(encoding='utf-8', errors='replace')
except AttributeError:
    pass  # Python < 3.7

def find_api_files(project_path: Path) -> list:
    """Find API-related files."""
    patterns = [
        "**/*api*.ts", "**/*api*.js", "**/*api*.py",
        "**/routes/*.ts", "**/routes/*.js", "**/routes/*.py",
        "**/controllers/*.ts", "**/controllers/*.js",
        "**/endpoints/*.ts", "**/endpoints/*.py",
        "**/*.openapi.json", "**/*.openapi.yaml",
        "**/swagger.json", "**/swagger.yaml",
        "**/openapi.json", "**/openapi.yaml"
    ]
    
    files = []
    for pattern in patterns:
        files.extend(project_path.glob(pattern))
    
    # Exclude node_modules, etc.
    return [f for f in files if not any(x in str(f) for x in ['node_modules', '.git', 'dist', 'build', '__pycache__'])]

def check_openapi_spec(file_path: Path) -> dict:
    """Check OpenAPI/Swagger specification."""
    issues = []
    passed = []
    
    try:
        content = file_path.read_text(encoding='utf-8')
        
        if file_path.suffix == '.json':
            spec = json.loads(content)
        else:
            # Basic YAML check
            if 'openapi:' in content or 'swagger:' in content:
                passed.append("[OK] OpenAPI/Swagger version defined")
            else:
                issues.append("[X] No OpenAPI version found")
            
            if 'paths:' in content:
                passed.append("[OK] Paths section exists")
            else:
                issues.append("[X] No paths defined")
            
            if 'components:' in content or 'definitions:' in content:
                passed.append("[OK] Schema components defined")
            
            return {'file': str(file_path), 'passed': passed, 'issues': issues, 'type': 'openapi'}
        
        # JSON OpenAPI checks
        if 'openapi' in spec or 'swagger' in spec:
            passed.append("[OK] OpenAPI version defined")
        
        if 'info' in spec:
            if 'title' in spec['info']:
                passed.append("[OK] API title defined")
            if 'version' in spec['info']:
                passed.append("[OK] API version defined")
            if 'description' not in spec['info']:
                issues.append("[!] API description missing")
        
        if 'paths' in spec:
            path_count = len(spec['paths'])
            passed.append(f"[OK] {path_count} endpoints defined")
            
            # Check each path
            for path, methods in spec['paths'].items():
                for method, details in methods.items():
                    if method in ['get', 'post', 'put', 'patch', 'delete']:
                        if 'responses' not in details:
                            issues.append(f"[X] {method.upper()} {path}: No responses defined")
                        if 'summary' not in details and 'description' not in details:
                            issues.append(f"[!] {method.upper()} {path}: No description")
        
    except Exception as e:
        issues.append(f"[X] Parse error: {e}")
    
    return {'file': str(file_path), 'passed': passed, 'issues': issues, 'type': 'openapi'}

def check_api_code(file_path: Path) -> dict:
    """Check API code for common issues."""
    issues = []
    passed = []
    
    try:
        content = file_path.read_text(encoding='utf-8')
        
        # Check for error handling
        error_patterns = [
            r'try\s*{', r'try:', r'\.catch\(',
            r'except\s+', r'catch\s*\('
        ]
        has_error_handling = any(re.search(p, content) for p in error_patterns)
        if has_error_handling:
            passed.append("[OK] Error handling present")
        else:
            issues.append("[X] No error handling found")
        
        # Check for status codes
        status_patterns = [
            r'status\s*\(\s*\d{3}\s*\)', r'statusCode\s*[=:]\s*\d{3}',
            r'HttpStatus\.', r'status_code\s*=\s*\d{3}',
            r'\.status\(\d{3}\)', r'res\.status\('
        ]
        has_status = any(re.search(p, content) for p in status_patterns)
        if has_status:
            passed.append("[OK] HTTP status codes used")
        else:
            issues.append("[!] No explicit HTTP status codes")
        
        # Check for validation
        validation_patterns = [
            r'validate', r'schema', r'zod', r'joi', r'yup',
            r'pydantic', r'@Body\(', r'@Query\('
        ]
        has_validation = any(re.search(p, content, re.I) for p in validation_patterns)
        if has_validation:
            passed.append("[OK] Input validation present")
        else:
            issues.append("[!] No input validation detected")
        
        # Check for auth middleware
        auth_patterns = [
            r'auth', r'jwt', r'bearer', r'token',
            r'middleware', r'guard', r'@Authenticated'
        ]
        has_auth = any(re.search(p, content, re.I) for p in auth_patterns)
        if has_auth:
            passed.append("[OK] Authentication/authorization detected")
        
        # Check for rate limiting
        rate_patterns = [r'rateLimit', r'throttle', r'rate.?limit']
        has_rate = any(re.search(p, content, re.I) for p in rate_patterns)
        if has_rate:
            passed.append("[OK] Rate limiting present")
        
        # Check for logging
        log_patterns = [r'console\.log', r'logger\.', r'logging\.', r'log\.']
        has_logging = any(re.search(p, content) for p in log_patterns)
        if has_logging:
            passed.append("[OK] Logging present")
        
    except Exception as e:
        issues.append(f"[X] Read error: {e}")
    
    return {'file': str(file_path), 'passed': passed, 'issues': issues, 'type': 'code'}

def main():
    target = sys.argv[1] if len(sys.argv) > 1 else "."
    project_path = Path(target)
    
    print("\n" + "=" * 60)
    print("  API VALIDATOR - Endpoint Best Practices Check")
    print("=" * 60 + "\n")
    
    api_files = find_api_files(project_path)
    
    if not api_files:
        print("[!] No API files found.")
        print("   Looking for: routes/, controllers/, api/, openapi.json/yaml")
        sys.exit(0)
    
    results = []
    for file_path in api_files[:15]:  # Limit
        if 'openapi' in file_path.name.lower() or 'swagger' in file_path.name.lower():
            result = check_openapi_spec(file_path)
        else:
            result = check_api_code(file_path)
        results.append(result)
    
    # Print results
    total_issues = 0
    total_passed = 0
    
    for result in results:
        print(f"\n[FILE] {result['file']} [{result['type']}]")
        for item in result['passed']:
            print(f"   {item}")
            total_passed += 1
        for item in result['issues']:
            print(f"   {item}")
            if item.startswith("[X]"):
                total_issues += 1
    
    print("\n" + "=" * 60)
    print(f"[RESULTS] {total_passed} passed, {total_issues} critical issues")
    print("=" * 60)
    
    if total_issues == 0:
        print("[OK] API validation passed")
        sys.exit(0)
    else:
        print("[X] Fix critical issues before deployment")
        sys.exit(1)

if __name__ == "__main__":
    main()


################################################################################
# PASTA: .agent/skills/database-design/scripts
################################################################################

--- ARQUIVO: .agent/skills/database-design/scripts/schema_validator.py ---
#!/usr/bin/env python3
"""
Schema Validator - Database schema validation
Validates Prisma schemas and checks for common issues.

Usage:
    python schema_validator.py <project_path>

Checks:
    - Prisma schema syntax
    - Missing relations
    - Index recommendations
    - Naming conventions
"""

import sys
import json
import re
from pathlib import Path
from datetime import datetime

# Fix Windows console encoding
try:
    sys.stdout.reconfigure(encoding='utf-8', errors='replace')
except:
    pass


def find_schema_files(project_path: Path) -> list:
    """Find database schema files."""
    schemas = []
    
    # Prisma schema
    prisma_files = list(project_path.glob('**/prisma/schema.prisma'))
    schemas.extend([('prisma', f) for f in prisma_files])
    
    # Drizzle schema files
    drizzle_files = list(project_path.glob('**/drizzle/*.ts'))
    drizzle_files.extend(project_path.glob('**/schema/*.ts'))
    for f in drizzle_files:
        if 'schema' in f.name.lower() or 'table' in f.name.lower():
            schemas.append(('drizzle', f))
    
    return schemas[:10]  # Limit


def validate_prisma_schema(file_path: Path) -> list:
    """Validate Prisma schema file."""
    issues = []
    
    try:
        content = file_path.read_text(encoding='utf-8', errors='ignore')
        
        # Find all models
        models = re.findall(r'model\s+(\w+)\s*{([^}]+)}', content, re.DOTALL)
        
        for model_name, model_body in models:
            # Check naming convention (PascalCase)
            if not model_name[0].isupper():
                issues.append(f"Model '{model_name}' should be PascalCase")
            
            # Check for id field
            if '@id' not in model_body and 'id' not in model_body.lower():
                issues.append(f"Model '{model_name}' might be missing @id field")
            
            # Check for createdAt/updatedAt
            if 'createdAt' not in model_body and 'created_at' not in model_body:
                issues.append(f"Model '{model_name}' missing createdAt field (recommended)")
            
            # Check for @relation without fields
            relations = re.findall(r'@relation\([^)]*\)', model_body)
            for rel in relations:
                if 'fields:' not in rel and 'references:' not in rel:
                    pass  # Implicit relation, ok
            
            # Check for @@index suggestions
            foreign_keys = re.findall(r'(\w+Id)\s+\w+', model_body)
            for fk in foreign_keys:
                if f'@@index([{fk}])' not in content and f'@@index(["{fk}"])' not in content:
                    issues.append(f"Consider adding @@index([{fk}]) for better query performance in {model_name}")
        
        # Check for enum definitions
        enums = re.findall(r'enum\s+(\w+)\s*{', content)
        for enum_name in enums:
            if not enum_name[0].isupper():
                issues.append(f"Enum '{enum_name}' should be PascalCase")
        
    except Exception as e:
        issues.append(f"Error reading schema: {str(e)[:50]}")
    
    return issues


def main():
    project_path = Path(sys.argv[1] if len(sys.argv) > 1 else ".").resolve()
    
    print(f"\n{'='*60}")
    print(f"[SCHEMA VALIDATOR] Database Schema Validation")
    print(f"{'='*60}")
    print(f"Project: {project_path}")
    print(f"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("-"*60)
    
    # Find schema files
    schemas = find_schema_files(project_path)
    print(f"Found {len(schemas)} schema files")
    
    if not schemas:
        output = {
            "script": "schema_validator",
            "project": str(project_path),
            "schemas_checked": 0,
            "issues_found": 0,
            "passed": True,
            "message": "No schema files found"
        }
        print(json.dumps(output, indent=2))
        sys.exit(0)
    
    # Validate each schema
    all_issues = []
    
    for schema_type, file_path in schemas:
        print(f"\nValidating: {file_path.name} ({schema_type})")
        
        if schema_type == 'prisma':
            issues = validate_prisma_schema(file_path)
        else:
            issues = []  # Drizzle validation could be added
        
        if issues:
            all_issues.append({
                "file": str(file_path.name),
                "type": schema_type,
                "issues": issues
            })
    
    # Summary
    print("\n" + "="*60)
    print("SCHEMA ISSUES")
    print("="*60)
    
    if all_issues:
        for item in all_issues:
            print(f"\n{item['file']} ({item['type']}):")
            for issue in item["issues"][:5]:  # Limit per file
                print(f"  - {issue}")
            if len(item["issues"]) > 5:
                print(f"  ... and {len(item['issues']) - 5} more issues")
    else:
        print("No schema issues found!")
    
    total_issues = sum(len(item["issues"]) for item in all_issues)
    # Schema issues are warnings, not failures
    passed = True
    
    output = {
        "script": "schema_validator",
        "project": str(project_path),
        "schemas_checked": len(schemas),
        "issues_found": total_issues,
        "passed": passed,
        "issues": all_issues
    }
    
    print("\n" + json.dumps(output, indent=2))
    
    sys.exit(0)


if __name__ == "__main__":
    main()


################################################################################
# PASTA: .agent/skills/frontend-design/scripts
################################################################################

--- ARQUIVO: .agent/skills/frontend-design/scripts/accessibility_checker.py ---
#!/usr/bin/env python3
"""
Accessibility Checker - WCAG compliance audit
Checks HTML files for accessibility issues.

Usage:
    python accessibility_checker.py <project_path>

Checks:
    - Form labels
    - ARIA attributes
    - Color contrast hints
    - Keyboard navigation
    - Semantic HTML
"""

import sys
import json
import re
from pathlib import Path
from datetime import datetime

# Fix Windows console encoding
try:
    sys.stdout.reconfigure(encoding='utf-8', errors='replace')
except:
    pass


def find_html_files(project_path: Path) -> list:
    """Find all HTML/JSX/TSX files."""
    patterns = ['**/*.html', '**/*.jsx', '**/*.tsx']
    skip_dirs = {'node_modules', '.next', 'dist', 'build', '.git'}
    
    files = []
    for pattern in patterns:
        for f in project_path.glob(pattern):
            if not any(skip in f.parts for skip in skip_dirs):
                files.append(f)
    
    return files[:50]


def check_accessibility(file_path: Path) -> list:
    """Check a single file for accessibility issues."""
    issues = []
    
    try:
        content = file_path.read_text(encoding='utf-8', errors='ignore')
        
        # Check for form inputs without labels
        inputs = re.findall(r'<input[^>]*>', content, re.IGNORECASE)
        for inp in inputs:
            if 'type="hidden"' not in inp.lower():
                if 'aria-label' not in inp.lower() and 'id=' not in inp.lower():
                    issues.append("Input without label or aria-label")
                    break
        
        # Check for buttons without accessible text
        buttons = re.findall(r'<button[^>]*>[^<]*</button>', content, re.IGNORECASE)
        for btn in buttons:
            # Check if button has text content or aria-label
            if 'aria-label' not in btn.lower():
                text = re.sub(r'<[^>]+>', '', btn)
                if not text.strip():
                    issues.append("Button without accessible text")
                    break
        
        # Check for missing lang attribute
        if '<html' in content.lower() and 'lang=' not in content.lower():
            issues.append("Missing lang attribute on <html>")
        
        # Check for missing skip link
        if '<main' in content.lower() or '<body' in content.lower():
            if 'skip' not in content.lower() and '#main' not in content.lower():
                issues.append("Consider adding skip-to-main-content link")
        
        # Check for click handlers without keyboard support
        onclick_count = content.lower().count('onclick=')
        onkeydown_count = content.lower().count('onkeydown=') + content.lower().count('onkeyup=')
        if onclick_count > 0 and onkeydown_count == 0:
            issues.append("onClick without keyboard handler (onKeyDown)")
        
        # Check for tabIndex misuse
        if 'tabindex=' in content.lower():
            if 'tabindex="-1"' not in content.lower() and 'tabindex="0"' not in content.lower():
                positive_tabindex = re.findall(r'tabindex="([1-9]\d*)"', content, re.IGNORECASE)
                if positive_tabindex:
                    issues.append("Avoid positive tabIndex values")
        
        # Check for autoplay media
        if 'autoplay' in content.lower():
            if 'muted' not in content.lower():
                issues.append("Autoplay media should be muted")
        
        # Check for role usage
        if 'role="button"' in content.lower():
            # Divs with role button should have tabindex
            div_buttons = re.findall(r'<div[^>]*role="button"[^>]*>', content, re.IGNORECASE)
            for div in div_buttons:
                if 'tabindex' not in div.lower():
                    issues.append("role='button' without tabindex")
                    break
        
    except Exception as e:
        issues.append(f"Error reading file: {str(e)[:50]}")
    
    return issues


def main():
    project_path = Path(sys.argv[1] if len(sys.argv) > 1 else ".").resolve()
    
    print(f"\n{'='*60}")
    print(f"[ACCESSIBILITY CHECKER] WCAG Compliance Audit")
    print(f"{'='*60}")
    print(f"Project: {project_path}")
    print(f"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("-"*60)
    
    # Find HTML files
    files = find_html_files(project_path)
    print(f"Found {len(files)} HTML/JSX/TSX files")
    
    if not files:
        output = {
            "script": "accessibility_checker",
            "project": str(project_path),
            "files_checked": 0,
            "issues_found": 0,
            "passed": True,
            "message": "No HTML files found"
        }
        print(json.dumps(output, indent=2))
        sys.exit(0)
    
    # Check each file
    all_issues = []
    
    for f in files:
        issues = check_accessibility(f)
        if issues:
            all_issues.append({
                "file": str(f.name),
                "issues": issues
            })
    
    # Summary
    print("\n" + "="*60)
    print("ACCESSIBILITY ISSUES")
    print("="*60)
    
    if all_issues:
        for item in all_issues[:10]:
            print(f"\n{item['file']}:")
            for issue in item["issues"]:
                print(f"  - {issue}")
        
        if len(all_issues) > 10:
            print(f"\n... and {len(all_issues) - 10} more files with issues")
    else:
        print("No accessibility issues found!")
    
    total_issues = sum(len(item["issues"]) for item in all_issues)
    # Accessibility issues are important but not blocking
    passed = total_issues < 5  # Allow minor issues
    
    output = {
        "script": "accessibility_checker",
        "project": str(project_path),
        "files_checked": len(files),
        "files_with_issues": len(all_issues),
        "issues_found": total_issues,
        "passed": passed
    }
    
    print("\n" + json.dumps(output, indent=2))
    
    sys.exit(0 if passed else 1)


if __name__ == "__main__":
    main()

--- ARQUIVO: .agent/skills/frontend-design/scripts/ux_audit.py ---
#!/usr/bin/env python3
"""
UX Audit Script - Full Frontend Design Coverage

Analyzes code for compliance with:

1. CORE PSYCHOLOGY LAWS:
   - Hick's Law (nav items, form complexity)
   - Fitts' Law (target sizes, touch targets)
   - Miller's Law (chunking, memory limits)
   - Von Restorff Effect (primary CTA visibility)
   - Serial Position Effect (important items at start/end)

2. EMOTIONAL DESIGN (Don Norman):
   - Visceral (first impressions, gradients, animations)
   - Behavioral (feedback, usability, performance)
   - Reflective (brand story, values, identity)

3. TRUST BUILDING:
   - Security signals (SSL, encryption on forms)
   - Social proof (testimonials, reviews, logos)
   - Authority indicators (certifications, awards, media)

4. COGNITIVE LOAD MANAGEMENT:
   - Progressive disclosure (accordion, tabs, "Advanced")
   - Visual noise (too many colors/borders)
   - Familiar patterns (labels, standard conventions)

5. PERSUASIVE DESIGN (Ethical):
   - Smart defaults (pre-selected options)
   - Anchoring (original vs discount price)
   - Social proof (live indicators, numbers)
   - Progress indicators (progress bars, steps)

6. TYPOGRAPHY SYSTEM (9 sections):
   - Font Pairing (max 3 families)
   - Line Length (45-75ch)
   - Line Height (proper ratios)
   - Letter Spacing (uppercase, display text)
   - Weight and Emphasis (contrast levels)
   - Responsive Typography (clamp())
   - Hierarchy (sequential headings)
   - Modular Scale (consistent ratios)
   - Readability (chunking, subheadings)

7. VISUAL EFFECTS (10 sections):
   - Glassmorphism (blur + transparency)
   - Neomorphism (dual shadows, inset)
   - Shadow Hierarchy (elevation levels)
   - Gradients (usage, overuse)
   - Border Effects (complexity check)
   - Glow Effects (text-shadow, box-shadow)
   - Overlay Techniques (image text readability)
   - GPU Acceleration (transform/opacity vs layout)
   - Performance (will-change usage)
   - Effect Selection (purpose over decoration)

8. COLOR SYSTEM (7 sections):
   - PURPLE BAN (Critical Maestro rule - #8B5CF6, #A855F7, etc.)
   - 60-30-10 Rule (dominant, secondary, accent)
   - Color Scheme Patterns (monochromatic, analogous)
   - Dark Mode Compliance (no pure black/white)
   - WCAG Contrast (low-contrast detection)
   - Color Psychology Context (food + blue = bad)
   - HSL-Based Palettes (recommended approach)

9. ANIMATION GUIDE (6 sections):
   - Duration Appropriateness (50ms minimum, 1s max transitions)
   - Easing Functions (ease-out for entry, ease-in for exit)
   - Micro-interactions (hover/focus feedback)
   - Loading States (skeleton, spinner, progress)
   - Page Transitions (fade/slide for routing)
   - Scroll Animation Performance (no layout properties)

10. MOTION GRAPHICS (7 sections):
   - Lottie Animations (reduced motion fallbacks)
   - GSAP Memory Leaks (kill/revert on unmount)
   - SVG Animation Performance (stroke-dashoffset sparingly)
   - 3D Transforms (perspective parent, mobile warning)
   - Particle Effects (mobile fallback)
   - Scroll-Driven Animations (throttle with rAF)
   - Motion Decision Tree (functional vs decorative)

11. ACCESSIBILITY:
   - Alt text for images
   - Reduced motion checks
   - Form labels

Total: 80+ checks across all design principles
"""

import sys
import os
import re
import json
from pathlib import Path

class UXAuditor:
    def __init__(self):
        self.issues = []
        self.warnings = []
        self.passed_count = 0
        self.files_checked = 0
    
    def audit_file(self, filepath: str) -> None:
        try:
            with open(filepath, 'r', encoding='utf-8', errors='replace') as f:
                content = f.read()
        except: return
        
        self.files_checked += 1
        filename = os.path.basename(filepath)

        # Pre-calculate common flags
        has_long_text = bool(re.search(r'<p|<div.*class=.*text|article|<span.*text', content, re.IGNORECASE))
        has_form = bool(re.search(r'<form|<input|password|credit|card|payment', content, re.IGNORECASE))
        complex_elements = len(re.findall(r'<input|<select|<textarea|<option', content, re.IGNORECASE))

        # --- 1. PSYCHOLOGY LAWS ---
        # Hick's Law
        nav_items = len(re.findall(r'<NavLink|<Link|<a\s+href|nav-item', content, re.IGNORECASE))
        if nav_items > 7:
            self.issues.append(f"[Hick's Law] {filename}: {nav_items} nav items (Max 7)")
        
        # Fitts' Law
        if re.search(r'height:\s*([0-3]\d)px', content) or re.search(r'h-[1-9]\b|h-10\b', content):
            self.warnings.append(f"[Fitts' Law] {filename}: Small targets (< 44px)")
        
        # Miller's Law
        form_fields = len(re.findall(r'<input|<select|<textarea', content, re.IGNORECASE))
        if form_fields > 7 and not re.search(r'step|wizard|stage', content, re.IGNORECASE):
            self.warnings.append(f"[Miller's Law] {filename}: Complex form ({form_fields} fields)")
            
        # Von Restorff
        if 'button' in content.lower() and not re.search(r'primary|bg-primary|Button.*primary|variant=["\']primary', content, re.IGNORECASE):
            self.warnings.append(f"[Von Restorff] {filename}: No primary CTA")

        # Serial Position Effect - Important items at beginning/end
        if nav_items > 3:
            # Check if last nav item is important (contact, login, etc.)
            nav_content = re.findall(r'<NavLink|<Link|<a\s+href[^>]*>([^<]+)</a>', content, re.IGNORECASE)
            if nav_content and len(nav_content) > 2:
                last_item = nav_content[-1].lower() if nav_content else ''
                if not any(x in last_item for x in ['contact', 'login', 'sign', 'get started', 'cta', 'button']):
                    self.warnings.append(f"[Serial Position] {filename}: Last nav item may not be important. Place key actions at start/end.")

        # --- 1.5 EMOTIONAL DESIGN (Don Norman) ---

        # Visceral: First impressions (aesthetics, gradients, animations)
        has_hero = bool(re.search(r'hero|<h1|banner', content, re.IGNORECASE))
        if has_hero:
            # Check for visual appeal elements
            has_gradient = bool(re.search(r'gradient|linear-gradient|radial-gradient', content))
            has_animation = bool(re.search(r'@keyframes|transition:|animate-', content))
            has_visual_interest = has_gradient or has_animation

            if not has_visual_interest and not re.search(r'background:|bg-', content):
                self.warnings.append(f"[Visceral] {filename}: Hero section lacks visual appeal. Consider gradients or subtle animations.")

        # Behavioral: Instant feedback and usability
        if 'onClick' in content or '@click' in content or 'onclick' in content:
            has_feedback = re.search(r'transition|animate|hover:|focus:|disabled|loading|spinner', content, re.IGNORECASE)
            has_state_change = re.search(r'setState|useState|disabled|loading', content)

            if not has_feedback and not has_state_change:
                self.warnings.append(f"[Behavioral] {filename}: Interactive elements lack immediate feedback. Add hover/focus/disabled states.")

        # Reflective: Brand story, values, identity
        has_reflective = bool(re.search(r'about|story|mission|values|why we|our journey|testimonials', content, re.IGNORECASE))
        if has_long_text and not has_reflective:
            self.warnings.append(f"[Reflective] {filename}: Long-form content without brand story/values. Add 'About' or 'Why We Exist' section.")

        # --- 1.6 TRUST BUILDING (Enhanced) ---

        # Security signals
        if has_form:
            security_signals = re.findall(r'ssl|secure|encrypt|lock|padlock|https', content, re.IGNORECASE)
            if len(security_signals) == 0 and not re.search(r'checkout|payment', content, re.IGNORECASE):
                self.warnings.append(f"[Trust] {filename}: Form without security indicators. Add 'SSL Secure' or lock icon.")

        # Social proof elements
        social_proof = re.findall(r'review|testimonial|rating|star|trust|trusted by|customer|logo', content, re.IGNORECASE)
        if len(social_proof) > 0:
            self.passed_count += 1
        else:
            if has_long_text:
                self.warnings.append(f"[Trust] {filename}: No social proof detected. Consider adding testimonials, ratings, or 'Trusted by' logos.")

        # Authority indicators
        has_footer = bool(re.search(r'footer|<footer', content, re.IGNORECASE))
        if has_footer:
            authority = re.findall(r'certif|award|media|press|featured|as seen in', content, re.IGNORECASE)
            if len(authority) == 0:
                self.warnings.append(f"[Trust] {filename}: Footer lacks authority signals. Add certifications, awards, or media mentions.")

        # --- 1.7 COGNITIVE LOAD MANAGEMENT ---

        # Progressive disclosure
        if complex_elements > 5:
            has_progressive = re.search(r'step|wizard|stage|accordion|collapsible|tab|more\.\.\.|advanced|show more', content, re.IGNORECASE)
            if not has_progressive:
                self.warnings.append(f"[Cognitive Load] {filename}: Many form elements without progressive disclosure. Consider accordion, tabs, or 'Advanced' toggle.")

        # Visual noise check
        has_many_colors = len(re.findall(r'#[0-9a-fA-F]{3,6}|rgb|hsl', content)) > 15
        has_many_borders = len(re.findall(r'border:|border-', content)) > 10
        if has_many_colors and has_many_borders:
            self.warnings.append(f"[Cognitive Load] {filename}: High visual noise detected. Many colors and borders increase cognitive load.")

        # Familiar patterns
        if has_form:
            has_standard_labels = bool(re.search(r'<label|placeholder|aria-label', content, re.IGNORECASE))
            if not has_standard_labels:
                self.issues.append(f"[Cognitive Load] {filename}: Form inputs without labels. Use <label> for accessibility and clarity.")

        # --- 1.8 PERSUASIVE DESIGN (Ethical) ---

        # Smart defaults
        if has_form:
            has_defaults = bool(re.search(r'checked|selected|default|value=["\'].*["\']', content))
            radio_inputs = len(re.findall(r'type=["\']radio', content, re.IGNORECASE))
            if radio_inputs > 0 and not has_defaults:
                self.warnings.append(f"[Persuasion] {filename}: Radio buttons without default selection. Pre-select recommended option.")

        # Anchoring (showing original price)
        if re.search(r'price|pricing|cost|\$\d+', content, re.IGNORECASE):
            has_anchor = bool(re.search(r'original|was|strike|del|save \d+%', content, re.IGNORECASE))
            if not has_anchor:
                self.warnings.append(f"[Persuasion] {filename}: Prices without anchoring. Show original price to frame discount value.")

        # Social proof live indicators
        has_social = bool(re.search(r'join|subscriber|member|user', content, re.IGNORECASE))
        if has_social:
            has_count = bool(re.findall(r'\d+[+kmb]|\d+,\d+', content))
            if not has_count:
                self.warnings.append(f"[Persuasion] {filename}: Social proof without specific numbers. Use 'Join 10,000+' format.")

        # Progress indicators
        if has_form:
            has_progress = bool(re.search(r'progress|step \d+|complete|%|bar', content, re.IGNORECASE))
            if complex_elements > 5 and not has_progress:
                self.warnings.append(f"[Persuasion] {filename}: Long form without progress indicator. Add progress bar or 'Step X of Y'.")

        # --- 2. TYPOGRAPHY SYSTEM (Complete Coverage) ---

        # 2.1 Font Pairing - Too many font families
        font_families = set()
        # Check for @font-face, Google Fonts, font-family declarations
        font_faces = re.findall(r'@font-face\s*\{[^}]*family:\s*["\']?([^;"\'\s}]+)', content, re.IGNORECASE)
        google_fonts = re.findall(r'fonts\.googleapis\.com[^"\']*family=([^"&]+)', content, re.IGNORECASE)
        font_family_css = re.findall(r'font-family:\s*([^;]+)', content, re.IGNORECASE)

        for font in font_faces: font_families.add(font.strip().lower())
        for font in google_fonts:
            for f in font.replace('+', ' ').split('|'):
                font_families.add(f.split(':')[0].strip().lower())
        for family in font_family_css:
            # Extract first font from stack
            first_font = family.split(',')[0].strip().strip('"\'')

            if first_font.lower() not in {'sans-serif', 'serif', 'monospace', 'cursive', 'fantasy', 'system-ui', 'inherit', 'arial', 'georgia', 'times new roman', 'courier new', 'verdana', 'helvetica', 'tahoma'}:
                font_families.add(first_font.lower())

        if len(font_families) > 3:
            self.issues.append(f"[Typography] {filename}: {len(font_families)} font families detected. Limit to 2-3 for cohesion.")

        # 2.2 Line Length - Character-based width
        if has_long_text and not re.search(r'max-w-(?:prose|[\[\\]?\d+ch[\]\\]?)|max-width:\s*\d+ch', content):
            self.warnings.append(f"[Typography] {filename}: No line length constraint (45-75ch). Use max-w-prose or max-w-[65ch].")

        # 2.3 Line Height - Proper leading ratios
        # Check for text without proper line-height
        text_elements = len(re.findall(r'<p|<span|<div.*text|<h[1-6]', content, re.IGNORECASE))
        if text_elements > 0 and not re.search(r'leading-|line-height:', content):
            self.warnings.append(f"[Typography] {filename}: Text elements found without line-height. Body: 1.4-1.6, Headings: 1.1-1.3")

        # Check for heading-specific line height issues
        if re.search(r'<h[1-6]|text-(?:xl|2xl|3xl|4xl|5xl|6xl)', content, re.IGNORECASE):
            # Extract line-height values
            line_heights = re.findall(r'(?:leading-|line-height:\s*)([\d.]+)', content)
            for lh in line_heights:
                if float(lh) > 1.5:
                    self.warnings.append(f"[Typography] {filename}: Heading has line-height {lh} (>1.3). Headings should be tighter (1.1-1.3).")

        # 2.4 Letter Spacing (Tracking)
        # Uppercase without tracking
        if re.search(r'uppercase|text-transform:\s*uppercase', content, re.IGNORECASE):
            if not re.search(r'tracking-|letter-spacing:', content):
                self.warnings.append(f"[Typography] {filename}: Uppercase text without tracking. ALL CAPS needs +5-10% spacing.")

        # Large text (display/hero) should have negative tracking
        if re.search(r'text-(?:4xl|5xl|6xl|7xl|8xl|9xl)|font-size:\s*[3-9]\dpx', content):
            if not re.search(r'tracking-tight|letter-spacing:\s*-[0-9]', content):
                self.warnings.append(f"[Typography] {filename}: Large display text without tracking-tight. Big text needs -1% to -4% spacing.")

        # 2.5 Weight and Emphasis - Contrast levels
        # Check for adjacent weight levels (poor contrast)
        weights = re.findall(r'font-weight:\s*(\d+)|font-(?:thin|extralight|light|normal|medium|semibold|bold|extrabold|black)|fw-(\d+)', content, re.IGNORECASE)
        weight_values = []
        for w in weights:
            val = w[0] or w[1]
            if val:
                # Map named weights to numbers
                weight_map = {'thin': '100', 'extralight': '200', 'light': '300', 'normal': '400', 'medium': '500', 'semibold': '600', 'bold': '700', 'extrabold': '800', 'black': '900'}
                val = weight_map.get(val.lower(), val)
                try:
                    weight_values.append(int(val))
                except: pass

        # Check for adjacent weights (400/500, 500/600, etc.)
        for i in range(len(weight_values) - 1):
            diff = abs(weight_values[i] - weight_values[i+1])
            if diff == 100:
                self.warnings.append(f"[Typography] {filename}: Adjacent font weights ({weight_values[i]}/{weight_values[i+1]}). Skip at least 2 levels for contrast.")

        # Too many weight levels
        unique_weights = set(weight_values)
        if len(unique_weights) > 4:
            self.warnings.append(f"[Typography] {filename}: {len(unique_weights)} font weights. Limit to 3-4 per page.")

        # 2.6 Responsive Typography - Fluid sizing with clamp()
        has_font_sizes = bool(re.search(r'font-size:|text-(?:xs|sm|base|lg|xl|2xl)', content))
        if has_font_sizes and not re.search(r'clamp\(|responsive:', content):
            self.warnings.append(f"[Typography] {filename}: Fixed font sizes without clamp(). Consider fluid typography: clamp(MIN, PREFERRED, MAX)")

        # 2.7 Hierarchy - Heading structure
        headings = re.findall(r'<(h[1-6])', content, re.IGNORECASE)
        if headings:
            # Check for skipped levels (h1 -> h3)
            for i in range(len(headings) - 1):
                curr = int(headings[i][1])
                next_h = int(headings[i+1][1])
                if next_h > curr + 1:
                    self.warnings.append(f"[Typography] {filename}: Skipped heading level (h{curr} -> h{next_h}). Maintain sequential hierarchy.")

            # Check if h1 exists for main content
            if 'h1' not in [h.lower() for h in headings] and has_long_text:
                self.warnings.append(f"[Typography] {filename}: No h1 found. Each page should have one primary heading.")

        # 2.8 Modular Scale - Consistent sizing
        # Extract font-size values
        font_sizes = re.findall(r'font-size:\s*(\d+(?:\.\d+)?)(px|rem|em)', content)
        size_values = []
        for size, unit in font_sizes:
            if unit == 'rem' or unit == 'em':
                size_values.append(float(size))
            elif unit == 'px':
                size_values.append(float(size) / 16)  # Normalize to rem

        if len(size_values) > 2:
            # Check if sizes follow a modular scale roughly
            sorted_sizes = sorted(set(size_values))
            ratios = []
            for i in range(1, len(sorted_sizes)):
                if sorted_sizes[i-1] > 0:
                    ratios.append(sorted_sizes[i] / sorted_sizes[i-1])

            # Common scale ratios: 1.067, 1.125, 1.2, 1.25, 1.333, 1.5, 1.618
            common_ratios = {1.067, 1.125, 1.2, 1.25, 1.333, 1.5, 1.618}
            for ratio in ratios[:3]:  # Check first 3 ratios
                if not any(abs(ratio - cr) < 0.05 for cr in common_ratios):
                    self.warnings.append(f"[Typography] {filename}: Font sizes may not follow modular scale (ratio: {ratio:.2f}). Consider consistent ratio like 1.25 (Major Third).")
                    break

        # 2.9 Readability - Content chunking
        # Check for very long paragraphs (>5 lines estimated)
        paragraphs = re.findall(r'<p[^>]*>([^<]+)</p>', content, re.IGNORECASE)
        for p in paragraphs:
            word_count = len(p.split())
            if word_count > 100:  # ~5-6 lines
                self.warnings.append(f"[Typography] {filename}: Long paragraph detected ({word_count} words). Break into 3-4 line chunks for readability.")

        # Check for missing subheadings in long content
        if len(paragraphs) > 5:
            subheadings = len(re.findall(r'<h[2-6]', content, re.IGNORECASE))
            if subheadings == 0:
                self.warnings.append(f"[Typography] {filename}: Long content without subheadings. Add h2/h3 to break up text.")

        # --- 3. VISUAL EFFECTS (visual-effects.md) ---
        
        # Glassmorphism Check
        if 'backdrop-filter' in content or 'blur(' in content:
            if not re.search(r'background:\s*rgba|bg-opacity|bg-[a-z0-9]+\/\d+', content):
                self.warnings.append(f"[Visual] {filename}: Blur used without semi-transparent background (Glassmorphism fail)")
        
        # GPU Acceleration / Performance
        if re.search(r'@keyframes|transition:', content):
            expensive_props = re.findall(r'width|height|top|left|right|bottom|margin|padding', content)
            if expensive_props:
                self.warnings.append(f"[Performance] {filename}: Animating expensive properties ({', '.join(set(expensive_props))}). Use transform/opacity where possible.")
            
            # Reduced Motion
            if not re.search(r'prefers-reduced-motion', content):
                self.warnings.append(f"[Accessibility] {filename}: Animations found without prefers-reduced-motion check")

        # Natural Shadows
        shadows = re.findall(r'box-shadow:\s*([^;]+)', content)
        for shadow in shadows:
            # Check if natural (Y > X) or multiple layers
            if ',' not in shadow and not re.search(r'\d+px\s+[1-9]\d*px', shadow): # Simple heuristic for Y-offset
                 self.warnings.append(f"[Visual] {filename}: Simple/Unnatural shadow detected. Consider multiple layers or Y > X offset for realism.")

        # --- 3.1 NEOMORPHISM CHECK ---
        # Check for neomorphism patterns (dual shadows with opposite directions)
        neo_shadows = re.findall(r'box-shadow:\s*([^;]+)', content)
        for shadow in neo_shadows:
            # Neomorphism has two shadows: positive offset + negative offset
            if ',' in shadow and '-' in shadow:
                # Check for inset pattern (pressed state)
                if 'inset' in shadow:
                    self.warnings.append(f"[Visual] {filename}: Neomorphism inset detected. Ensure adequate contrast for accessibility.")

        # --- 3.2 SHADOW HIERARCHY ---
        # Count shadow levels to check for elevation consistency
        shadow_count = len(shadows)
        if shadow_count > 0:
            # Check for shadow opacity levels (should indicate hierarchy)
            opacities = re.findall(r'rgba?\([^)]+,\s*([\d.]+)\)', content)
            shadow_opacities = [float(o) for o in opacities if float(o) < 0.5]
            if shadow_count >= 3 and len(shadow_opacities) > 0:
                # Check if there's variety in shadow opacities for different elevations
                unique_opacities = len(set(shadow_opacities))
                if unique_opacities < 2:
                    self.warnings.append(f"[Visual] {filename}: All shadows at same opacity level. Vary shadow intensity for elevation hierarchy.")

        # --- 3.3 GRADIENT CHECKS ---
        # Check for gradient usage
        has_gradient = bool(re.search(r'gradient|linear-gradient|radial-gradient|conic-gradient', content))
        if has_gradient:
            # Warn about mesh/aurora gradients (can be overused)
            gradient_count = len(re.findall(r'gradient', content, re.IGNORECASE))
            if gradient_count > 5:
                self.warnings.append(f"[Visual] {filename}: Many gradients detected ({gradient_count}). Ensure this serves purpose, not decoration.")
        else:
            # Check if hero section exists without gradient
            if has_hero and not re.search(r'background:|bg-', content):
                self.warnings.append(f"[Visual] {filename}: Hero section without visual interest. Consider gradient for depth.")

        # --- 3.4 BORDER EFFECTS ---
        # Check for gradient borders or animated borders
        has_border = bool(re.search(r'border:|border-', content))
        if has_border:
            # Check for overly complex borders
            border_count = len(re.findall(r'border:', content))
            if border_count > 8:
                self.warnings.append(f"[Visual] {filename}: Many border declarations ({border_count}). Simplify for cleaner look.")

        # --- 3.5 GLOW EFFECTS ---
        # Check for text-shadow or multiple box-shadow layers (glow effects)
        text_shadows = re.findall(r'text-shadow:', content)
        for ts in text_shadows:
            # Multiple text-shadow layers indicate glow
            if ',' in ts:
                self.warnings.append(f"[Visual] {filename}: Text glow effect detected. Ensure readability is maintained.")

        # Check for box-shadow glow (multiple layers with 0 offset)
        glow_shadows = re.findall(r'box-shadow:\s*[^;]*0\s+0\s+', content)
        if len(glow_shadows) > 2:
            self.warnings.append(f"[Visual] {filename}: Multiple glow effects detected. Use sparingly for emphasis only.")

        # --- 3.6 OVERLAY TECHNIQUES ---
        # Check for image overlays (for readability)
        has_images = bool(re.search(r'<img|background-image:|bg-\[url', content))
        if has_images and has_long_text:
            has_overlay = bool(re.search(r'overlay|rgba\(0|gradient.*transparent|::after|::before', content))
            if not has_overlay:
                self.warnings.append(f"[Visual] {filename}: Text over image without overlay. Add gradient overlay for readability.")

        # --- 3.7 PERFORMANCE: will-change ---
        # Check for will-change usage
        if re.search(r'will-change:', content):
            will_change_props = re.findall(r'will-change:\s*([^;]+)', content)
            for prop in will_change_props:
                prop = prop.strip().lower()
                if prop in ['width', 'height', 'top', 'left', 'right', 'bottom', 'margin', 'padding']:
                    self.issues.append(f"[Performance] {filename}: will-change on '{prop}' (layout property). Use only for transform/opacity.")

        # Check for excessive will-change usage
        will_change_count = len(re.findall(r'will-change:', content))
        if will_change_count > 3:
            self.warnings.append(f"[Performance] {filename}: Many will-change declarations ({will_change_count}). Use sparingly, only for heavy animations.")

        # --- 3.8 EFFECT SELECTION ---
        # Check for effect overuse (too many visual effects)
        effect_count = (
            (1 if has_gradient else 0) +
            shadow_count +
            len(re.findall(r'backdrop-filter|blur\(', content)) +
            len(re.findall(r'text-shadow:', content))
        )
        if effect_count > 10:
            self.warnings.append(f"[Visual] {filename}: Many visual effects ({effect_count}). Ensure effects serve purpose, not decoration.")

        # Check for static/flat design (no depth)
        if has_long_text and effect_count == 0:
            self.warnings.append(f"[Visual] {filename}: Flat design with no depth. Consider shadows or subtle gradients for hierarchy.")

        # --- 4. COLOR SYSTEM (color-system.md) ---

        # 4.1 PURPLE BAN - Critical check from color-system.md
        purple_hexes = ['#8B5CF6', '#A855F7', '#9333EA', '#7C3AED', '#6D28D9',
                        '#8B5CF6', '#A78BFA', '#C4B5FD', '#DDD6FE', '#EDE9FE',
                        '#8b5cf6', '#a855f7', '#9333ea', '#7c3aed', '#6d28d9',
                        'purple', 'violet', 'fuchsia', 'magenta', 'lavender']
        for purple in purple_hexes:
            if purple.lower() in content.lower():
                self.issues.append(f"[Color] {filename}: PURPLE DETECTED ('{purple}'). Banned by Maestro rules. Use Teal/Cyan/Emerald instead.")
                break

        # 4.2 60-30-10 Rule check
        # Count color usage to estimate ratio
        color_hex_count = len(re.findall(r'#[0-9a-fA-F]{3,6}', content))
        hsl_count = len(re.findall(r'hsl\(', content))
        total_colors = color_hex_count + hsl_count
        if total_colors > 3:
            # Check for dominant colors (should be ~60%)
            bg_declarations = re.findall(r'(?:background|bg-|bg\[)([^;}\s]+)', content)
            text_declarations = re.findall(r'(?:color|text-)([^;}\s]+)', content)
            if len(bg_declarations) > 0 and len(text_declarations) > 0:
                # Just warn if too many distinct colors
                unique_hexes = set(re.findall(r'#[0-9a-fA-F]{6}', content))
                if len(unique_hexes) > 5:
                    self.warnings.append(f"[Color] {filename}: {len(unique_hexes)} distinct colors. Consider 60-30-10 rule: dominant (60%), secondary (30%), accent (10%).")

        # 4.3 Color Scheme Pattern Detection
        # Detect monochromatic (same hue, different lightness)
        hsl_matches = re.findall(r'hsl\((\d+),\s*\d+%,\s*\d+%\)', content)
        if len(hsl_matches) >= 3:
            hues = [int(h) for h in hsl_matches]
            hue_range = max(hues) - min(hues)
            if hue_range < 10:
                self.warnings.append(f"[Color] {filename}: Monochromatic palette detected (hue variance: {hue_range}deg). Ensure adequate contrast.")

        # 4.4 Dark Mode Compliance
        # Check for pure black (#000000) or pure white (#FFFFFF) text (forbidden)
        if re.search(r'color:\s*#000000|#000\b', content):
            self.warnings.append(f"[Color] {filename}: Pure black (#000000) detected. Use #1a1a1a or darker grays for better dark mode.")
        if re.search(r'background:\s*#ffffff|#fff\b', content) and re.search(r'dark:\s*|dark:', content):
            self.warnings.append(f"[Color] {filename}: Pure white background in dark mode context. Use slight off-white (#f9fafb) for reduced eye strain.")

        # 4.5 WCAG Contrast Pattern Check
        # Look for potential low-contrast combinations
        light_bg_light_text = bool(re.search(r'bg-(?:gray|slate|zinc)-50|bg-white.*text-(?:gray|slate)-[12]', content))
        dark_bg_dark_text = bool(re.search(r'bg-(?:gray|slate|zinct)-9|bg-black.*text-(?:gray|slate)-[89]', content))
        if light_bg_light_text or dark_bg_dark_text:
            self.warnings.append(f"[Color] {filename}: Possible low-contrast combination detected. Verify WCAG AA (4.5:1 for text).")

        # 4.6 Color Psychology Context Check
        # Warn if blue used for food/restaurant context
        has_blue = bool(re.search(r'bg-blue|text-blue|from-blue|#[0-9a-fA-F]*00[0-9A-Fa-f]{2}|#[0-9a-fA-F]*1[0-9A-Fa-f]{2}', content))
        has_food_context = bool(re.search(r'restaurant|food|cooking|recipe|menu|dish|meal', content, re.IGNORECASE))
        if has_blue and has_food_context:
            self.warnings.append(f"[Color] {filename}: Blue color in food context. Blue suppresses appetite; consider warm colors (red, orange, yellow).")

        # 4.7 HSL-Based Palette Detection
        # Check if using HSL for palette (recommended in color-system.md)
        has_color_vars = bool(re.search(r'--color-|color-|primary-|secondary-', content))
        if has_color_vars and not re.search(r'hsl\(', content):
            self.warnings.append(f"[Color] {filename}: Color variables without HSL. Consider HSL for easier palette adjustment (Hue, Saturation, Lightness).")

        # --- 5. ANIMATION GUIDE (animation-guide.md) ---

        # 5.1 Duration Appropriateness
        # Check for excessively long or short animations
        durations = re.findall(r'(?:duration|animation-duration|transition-duration):\s*([\d.]+)(s|ms)', content)
        for duration, unit in durations:
            duration_ms = float(duration) * (1000 if unit == 's' else 1)
            if duration_ms < 50:
                self.warnings.append(f"[Animation] {filename}: Very fast animation ({duration}{unit}). Minimum 50ms for visibility.")
            elif duration_ms > 1000 and 'transition' in content.lower():
                self.warnings.append(f"[Animation] {filename}: Long transition ({duration}{unit}). Transitions should be 100-300ms for responsiveness.")

        # 5.2 Easing Function Correctness
        # Check for incorrect easing patterns
        if re.search(r'ease-in\s+.*entry|fade-in.*ease-in', content):
            self.warnings.append(f"[Animation] {filename}: Entry animation with ease-in. Entry should use ease-out for snappy feel.")
        if re.search(r'ease-out\s+.*exit|fade-out.*ease-out', content):
            self.warnings.append(f"[Animation] {filename}: Exit animation with ease-out. Exit should use ease-in for natural feel.")

        # 5.3 Micro-interaction Feedback Patterns
        # Check for interactive elements without hover/focus states
        interactive_elements = len(re.findall(r'<button|<a\s+href|onClick|@click', content))
        has_hover_focus = bool(re.search(r'hover:|focus:|:hover|:focus', content))
        if interactive_elements > 2 and not has_hover_focus:
            self.warnings.append(f"[Animation] {filename}: Interactive elements without hover/focus states. Add micro-interactions for feedback.")

        # 5.4 Loading State Indicators
        # Check for loading patterns
        has_async = bool(re.search(r'async|await|fetch|axios|loading|isLoading', content))
        has_loading_indicator = bool(re.search(r'skeleton|spinner|progress|loading|<circle.*animate', content))
        if has_async and not has_loading_indicator:
            self.warnings.append(f"[Animation] {filename}: Async operations without loading indicator. Add skeleton or spinner for perceived performance.")

        # 5.5 Page Transition Patterns
        # Check for page/view transitions
        has_routing = bool(re.search(r'router|navigate|Link.*to|useHistory', content))
        has_page_transition = bool(re.search(r'AnimatePresence|motion\.|transition.*page|fade.*route', content))
        if has_routing and not has_page_transition:
            self.warnings.append(f"[Animation] {filename}: Routing detected without page transitions. Consider fade/slide for context continuity.")

        # 5.6 Scroll Animation Performance
        # Check for scroll-driven animations
        has_scroll_anim = bool(re.search(r'onScroll|scroll.*trigger|IntersectionObserver', content))
        if has_scroll_anim:
            # Check if using expensive properties in scroll handlers
            if re.search(r'onScroll.*[^\w](width|height|top|left)', content):
                self.issues.append(f"[Animation] {filename}: Scroll handler animating layout properties. Use transform/opacity for 60fps.")

        # --- 6. MOTION GRAPHICS (motion-graphics.md) ---

        # 6.1 Lottie Animation Checks
        has_lottie = bool(re.search(r'lottie|Lottie|@lottie-react', content))
        if has_lottie:
            # Check for reduced motion fallback
            has_lottie_fallback = bool(re.search(r'prefers-reduced-motion.*lottie|lottie.*isPaused|lottie.*stop', content))
            if not has_lottie_fallback:
                self.warnings.append(f"[Motion] {filename}: Lottie animation without reduced-motion fallback. Add pause/stop for accessibility.")

        # 6.2 GSAP Memory Leak Risks
        has_gsap = bool(re.search(r'gsap|ScrollTrigger|from\(.*gsap', content))
        if has_gsap:
            # Check for cleanup patterns
            has_gsap_cleanup = bool(re.search(r'kill\(|revert\(|useEffect.*return.*gsap', content))
            if not has_gsap_cleanup:
                self.issues.append(f"[Motion] {filename}: GSAP animation without cleanup (kill/revert). Memory leak risk on unmount.")

        # 6.3 SVG Animation Performance
        svg_animations = re.findall(r'<animate|<animateTransform|stroke-dasharray|stroke-dashoffset', content)
        if len(svg_animations) > 3:
            self.warnings.append(f"[Motion] {filename}: Multiple SVG animations detected. Ensure stroke-dashoffset is used sparingly for mobile performance.")

        # 6.4 3D Transform Performance
        has_3d_transform = bool(re.search(r'transform3d|perspective\(|rotate3d|translate3d', content))
        if has_3d_transform:
            # Check for perspective on parent
            has_perspective_parent = bool(re.search(r'perspective:\s*\d+px|perspective\s*\(', content))
            if not has_perspective_parent:
                self.warnings.append(f"[Motion] {filename}: 3D transform without perspective parent. Add perspective: 1000px for realistic depth.")

            # Warn about mobile performance
            self.warnings.append(f"[Motion] {filename}: 3D transforms detected. Test on mobile; can impact performance on low-end devices.")

        # 6.5 Particle Effect Warnings
        # Check for canvas/WebGL particle systems
        has_particles = bool(re.search(r'particle|canvas.*loop|requestAnimationFrame.*draw|Three\.js', content))
        if has_particles:
            self.warnings.append(f"[Motion] {filename}: Particle effects detected. Ensure fallback or reduced-quality option for mobile devices.")

        # 6.6 Scroll-Driven Animation Performance
        has_scroll_driven = bool(re.search(r'IntersectionObserver.*animate|scroll.*progress|view-timeline', content))
        if has_scroll_driven:
            # Check for throttling/debouncing
            has_throttle = bool(re.search(r'throttle|debounce|requestAnimationFrame', content))
            if not has_throttle:
                self.issues.append(f"[Motion] {filename}: Scroll-driven animation without throttling. Add requestAnimationFrame for 60fps.")

        # 6.7 Motion Decision Tree - Context Check
        # Check if animation serves purpose (not just decoration)
        total_animations = (
            len(re.findall(r'@keyframes|transition:|animate-', content)) +
            (1 if has_lottie else 0) +
            (1 if has_gsap else 0)
        )
        if total_animations > 5:
            # Check if animations are functional
            functional_animations = len(re.findall(r'hover:|focus:|disabled|loading|error|success', content))
            if functional_animations < total_animations / 2:
                self.warnings.append(f"[Motion] {filename}: Many animations ({total_animations}). Ensure majority serve functional purpose (feedback, guidance), not decoration.")

        # --- 7. ACCESSIBILITY ---
        if re.search(r'<img(?![^>]*alt=)[^>]*>', content):
            self.issues.append(f"[Accessibility] {filename}: Missing img alt text")

    def audit_directory(self, directory: str) -> None:
        extensions = {'.tsx', '.jsx', '.html', '.vue', '.svelte', '.css'}
        for root, dirs, files in os.walk(directory):
            dirs[:] = [d for d in dirs if d not in {'node_modules', '.git', 'dist', 'build', '.next'}]
            for file in files:
                if Path(file).suffix in extensions:
                    self.audit_file(os.path.join(root, file))

    def get_report(self):
        return {
            "files_checked": self.files_checked,
            "issues": self.issues,
            "warnings": self.warnings,
            "passed_checks": self.passed_count,
            "compliant": len(self.issues) == 0
        }

def main():
    if len(sys.argv) < 2: sys.exit(1)
    
    path = sys.argv[1]
    is_json = "--json" in sys.argv
    
    auditor = UXAuditor()
    if os.path.isfile(path): auditor.audit_file(path)
    else: auditor.audit_directory(path)
    
    report = auditor.get_report()
    
    if is_json:
        print(json.dumps(report))
    else:
        # Use ASCII-safe output for Windows console compatibility
        print(f"\n[UX AUDIT] {report['files_checked']} files checked")
        print("-" * 50)
        if report['issues']:
            print(f"[!] ISSUES ({len(report['issues'])}):")
            for i in report['issues'][:10]: print(f"  - {i}")
        if report['warnings']:
            print(f"[*] WARNINGS ({len(report['warnings'])}):")
            for w in report['warnings'][:15]: print(f"  - {w}")
        print(f"[+] PASSED CHECKS: {report['passed_checks']}")
        status = "PASS" if report['compliant'] else "FAIL"
        print(f"STATUS: {status}")

    sys.exit(0 if report['compliant'] else 1)

if __name__ == "__main__":
    main()


################################################################################
# PASTA: .agent/skills/geo-fundamentals/scripts
################################################################################

--- ARQUIVO: .agent/skills/geo-fundamentals/scripts/geo_checker.py ---
#!/usr/bin/env python3
"""
GEO Checker - Generative Engine Optimization Audit
Checks PUBLIC WEB CONTENT for AI citation readiness.

PURPOSE:
    - Analyze pages that will be INDEXED by AI engines (ChatGPT, Perplexity, etc.)
    - Check for structured data, author info, dates, FAQ sections
    - Help content rank in AI-generated answers

WHAT IT CHECKS:
    - HTML files (actual web pages)
    - JSX/TSX files (React page components)
    - NOT markdown files (those are developer docs, not public content)

Usage:
    python geo_checker.py <project_path>
"""
import sys
import re
import json
from pathlib import Path

# Fix Windows console encoding
try:
    sys.stdout.reconfigure(encoding='utf-8', errors='replace')
    sys.stderr.reconfigure(encoding='utf-8', errors='replace')
except AttributeError:
    pass


# Directories to skip (not public content)
SKIP_DIRS = {
    'node_modules', '.next', 'dist', 'build', '.git', '.github',
    '__pycache__', '.vscode', '.idea', 'coverage', 'test', 'tests',
    '__tests__', 'spec', 'docs', 'documentation'
}

# Files to skip (not public pages)
SKIP_FILES = {
    'jest.config', 'webpack.config', 'vite.config', 'tsconfig',
    'package.json', 'package-lock', 'yarn.lock', '.eslintrc',
    'tailwind.config', 'postcss.config', 'next.config'
}


def is_page_file(file_path: Path) -> bool:
    """Check if this file is likely a public-facing page."""
    name = file_path.stem.lower()
    
    # Skip config/utility files
    if any(skip in name for skip in SKIP_FILES):
        return False
    
    # Skip test files
    if name.endswith('.test') or name.endswith('.spec'):
        return False
    if name.startswith('test_') or name.startswith('spec_'):
        return False
    
    # Likely page indicators
    page_indicators = ['page', 'index', 'home', 'about', 'contact', 'blog', 
                       'post', 'article', 'product', 'service', 'landing']
    
    # Check if it's in a pages/app directory (Next.js, etc.)
    parts = [p.lower() for p in file_path.parts]
    if 'pages' in parts or 'app' in parts or 'routes' in parts:
        return True
    
    # Check filename indicators
    if any(ind in name for ind in page_indicators):
        return True
    
    # HTML files are usually pages
    if file_path.suffix.lower() == '.html':
        return True
    
    return False


def find_web_pages(project_path: Path) -> list:
    """Find public-facing web pages only."""
    patterns = ['**/*.html', '**/*.htm', '**/*.jsx', '**/*.tsx']
    
    files = []
    for pattern in patterns:
        for f in project_path.glob(pattern):
            # Skip excluded directories
            if any(skip in f.parts for skip in SKIP_DIRS):
                continue
            
            # Check if it's likely a page
            if is_page_file(f):
                files.append(f)
    
    return files[:30]  # Limit to 30 pages


def check_page(file_path: Path) -> dict:
    """Check a single web page for GEO elements."""
    try:
        content = file_path.read_text(encoding='utf-8', errors='ignore')
    except Exception as e:
        return {'file': str(file_path.name), 'passed': [], 'issues': [f"Error: {e}"], 'score': 0}
    
    issues = []
    passed = []
    
    # 1. JSON-LD Structured Data (Critical for AI)
    if 'application/ld+json' in content:
        passed.append("JSON-LD structured data found")
        if '"@type"' in content:
            if 'Article' in content:
                passed.append("Article schema present")
            if 'FAQPage' in content:
                passed.append("FAQ schema present")
            if 'Organization' in content or 'Person' in content:
                passed.append("Entity schema present")
    else:
        issues.append("No JSON-LD structured data (AI engines prefer structured content)")
    
    # 2. Heading Structure
    h1_count = len(re.findall(r'<h1[^>]*>', content, re.I))
    h2_count = len(re.findall(r'<h2[^>]*>', content, re.I))
    
    if h1_count == 1:
        passed.append("Single H1 heading (clear topic)")
    elif h1_count == 0:
        issues.append("No H1 heading - page topic unclear")
    else:
        issues.append(f"Multiple H1 headings ({h1_count}) - confusing for AI")
    
    if h2_count >= 2:
        passed.append(f"{h2_count} H2 subheadings (good structure)")
    else:
        issues.append("Add more H2 subheadings for scannable content")
    
    # 3. Author Attribution (E-E-A-T signal)
    author_patterns = ['author', 'byline', 'written-by', 'contributor', 'rel="author"']
    has_author = any(p in content.lower() for p in author_patterns)
    if has_author:
        passed.append("Author attribution found")
    else:
        issues.append("No author info (AI prefers attributed content)")
    
    # 4. Publication Date (Freshness signal)
    date_patterns = ['datePublished', 'dateModified', 'datetime=', 'pubdate', 'article:published']
    has_date = any(re.search(p, content, re.I) for p in date_patterns)
    if has_date:
        passed.append("Publication date found")
    else:
        issues.append("No publication date (freshness matters for AI)")
    
    # 5. FAQ Section (Highly citable)
    faq_patterns = [r'<details', r'faq', r'frequently.?asked', r'"FAQPage"']
    has_faq = any(re.search(p, content, re.I) for p in faq_patterns)
    if has_faq:
        passed.append("FAQ section detected (highly citable)")
    
    # 6. Lists (Structured content)
    list_count = len(re.findall(r'<(ul|ol)[^>]*>', content, re.I))
    if list_count >= 2:
        passed.append(f"{list_count} lists (structured content)")
    
    # 7. Tables (Comparison data)
    table_count = len(re.findall(r'<table[^>]*>', content, re.I))
    if table_count >= 1:
        passed.append(f"{table_count} table(s) (comparison data)")
    
    # 8. Entity Recognition (E-E-A-T signal) - NEW 2025
    entity_patterns = [
        r'"@type"\s*:\s*"Organization"',
        r'"@type"\s*:\s*"LocalBusiness"', 
        r'"@type"\s*:\s*"Brand"',
        r'itemtype.*schema\.org/(Organization|Person|Brand)',
        r'rel="author"'
    ]
    has_entity = any(re.search(p, content, re.I) for p in entity_patterns)
    if has_entity:
        passed.append("Entity/Brand recognition (E-E-A-T)")
    
    # 9. Original Statistics/Data (AI citation magnet) - NEW 2025
    stat_patterns = [
        r'\d+%',                    # Percentages
        r'\$[\d,]+',                # Dollar amounts
        r'study\s+(shows|found)',   # Research citations
        r'according to',            # Source attribution
        r'data\s+(shows|reveals)',  # Data-backed claims
        r'\d+x\s+(faster|better|more)', # Comparison stats
        r'(million|billion|trillion)', # Large numbers
    ]
    stat_matches = sum(1 for p in stat_patterns if re.search(p, content, re.I))
    if stat_matches >= 2:
        passed.append("Original statistics/data (citation magnet)")
    
    # 10. Conversational/Direct answers - NEW 2025
    direct_answer_patterns = [
        r'is defined as',
        r'refers to',
        r'means that',
        r'the answer is',
        r'in short,',
        r'simply put,',
        r'<dfn'
    ]
    has_direct = any(re.search(p, content, re.I) for p in direct_answer_patterns)
    if has_direct:
        passed.append("Direct answer patterns (LLM-friendly)")
    
    # Calculate score
    total = len(passed) + len(issues)
    score = (len(passed) / total * 100) if total > 0 else 0
    
    return {
        'file': str(file_path.name),
        'passed': passed,
        'issues': issues,
        'score': round(score)
    }


def main():
    target = sys.argv[1] if len(sys.argv) > 1 else "."
    target_path = Path(target).resolve()
    
    print("\n" + "=" * 60)
    print("  GEO CHECKER - AI Citation Readiness Audit")
    print("=" * 60)
    print(f"Project: {target_path}")
    print("-" * 60)
    
    # Find web pages only
    pages = find_web_pages(target_path)
    
    if not pages:
        print("\n[!] No public web pages found.")
        print("    Looking for: HTML, JSX, TSX files in pages/app directories")
        print("    Skipping: docs, tests, config files, node_modules")
        output = {"script": "geo_checker", "pages_found": 0, "passed": True}
        print("\n" + json.dumps(output, indent=2))
        sys.exit(0)
    
    print(f"Found {len(pages)} public pages to analyze\n")
    
    # Check each page
    results = []
    for page in pages:
        result = check_page(page)
        results.append(result)
    
    # Print results
    for result in results:
        status = "[OK]" if result['score'] >= 60 else "[!]"
        print(f"{status} {result['file']}: {result['score']}%")
        if result['issues'] and result['score'] < 60:
            for issue in result['issues'][:2]:  # Show max 2 issues
                print(f"    - {issue}")
    
    # Average score
    avg_score = sum(r['score'] for r in results) / len(results) if results else 0
    
    print("\n" + "=" * 60)
    print(f"AVERAGE GEO SCORE: {avg_score:.0f}%")
    print("=" * 60)
    
    if avg_score >= 80:
        print("[OK] Excellent - Content well-optimized for AI citations")
    elif avg_score >= 60:
        print("[OK] Good - Some improvements recommended")
    elif avg_score >= 40:
        print("[!] Needs work - Add structured elements")
    else:
        print("[X] Poor - Content needs GEO optimization")
    
    # JSON output
    output = {
        "script": "geo_checker",
        "project": str(target_path),
        "pages_checked": len(results),
        "average_score": round(avg_score),
        "passed": avg_score >= 60
    }
    print("\n" + json.dumps(output, indent=2))
    
    sys.exit(0 if avg_score >= 60 else 1)


if __name__ == "__main__":
    main()


################################################################################
# PASTA: .agent/skills/i18n-localization/scripts
################################################################################

--- ARQUIVO: .agent/skills/i18n-localization/scripts/i18n_checker.py ---
#!/usr/bin/env python3
"""
i18n Checker - Detects hardcoded strings and missing translations.
Scans for untranslated text in React, Vue, and Python files.
"""
import sys
import re
import json
from pathlib import Path

# Fix Windows console encoding for Unicode output
try:
    sys.stdout.reconfigure(encoding='utf-8', errors='replace')
    sys.stderr.reconfigure(encoding='utf-8', errors='replace')
except AttributeError:
    pass  # Python < 3.7

# Patterns that indicate hardcoded strings (should be translated)
HARDCODED_PATTERNS = {
    'jsx': [
        # Text directly in JSX: <div>Hello World</div>
        r'>\s*[A-Z][a-zA-Z\s]{3,30}\s*</',
        # JSX attribute strings: title="Welcome"
        r'(title|placeholder|label|alt|aria-label)="[A-Z][a-zA-Z\s]{2,}"',
        # Button/heading text
        r'<(button|h[1-6]|p|span|label)[^>]*>\s*[A-Z][a-zA-Z\s!?.,]{3,}\s*</',
    ],
    'vue': [
        # Vue template text
        r'>\s*[A-Z][a-zA-Z\s]{3,30}\s*</',
        r'(placeholder|label|title)="[A-Z][a-zA-Z\s]{2,}"',
    ],
    'python': [
        # print/raise with string literals
        r'(print|raise\s+\w+)\s*\(\s*["\'][A-Z][^"\']{5,}["\']',
        # Flask flash messages
        r'flash\s*\(\s*["\'][A-Z][^"\']{5,}["\']',
    ]
}

# Patterns that indicate proper i18n usage
I18N_PATTERNS = [
    r't\(["\']',           # t('key') - react-i18next
    r'useTranslation',     # React hook
    r'\$t\(',              # Vue i18n
    r'_\(["\']',           # Python gettext
    r'gettext\(',          # Python gettext
    r'useTranslations',    # next-intl
    r'FormattedMessage',   # react-intl
    r'i18n\.',             # Generic i18n
]

def find_locale_files(project_path: Path) -> list:
    """Find translation/locale files."""
    patterns = [
        "**/locales/**/*.json",
        "**/translations/**/*.json",
        "**/lang/**/*.json",
        "**/i18n/**/*.json",
        "**/messages/*.json",
        "**/*.po",  # gettext
    ]
    
    files = []
    for pattern in patterns:
        files.extend(project_path.glob(pattern))
    
    return [f for f in files if 'node_modules' not in str(f)]

def check_locale_completeness(locale_files: list) -> dict:
    """Check if all locales have the same keys."""
    issues = []
    passed = []
    
    if not locale_files:
        return {'passed': [], 'issues': ["[!] No locale files found"]}
    
    # Group by parent folder (language)
    locales = {}
    for f in locale_files:
        if f.suffix == '.json':
            try:
                lang = f.parent.name
                content = json.loads(f.read_text(encoding='utf-8'))
                if lang not in locales:
                    locales[lang] = {}
                locales[lang][f.stem] = set(flatten_keys(content))
            except:
                continue
    
    if len(locales) < 2:
        passed.append(f"[OK] Found {len(locale_files)} locale file(s)")
        return {'passed': passed, 'issues': issues}
    
    passed.append(f"[OK] Found {len(locales)} language(s): {', '.join(locales.keys())}")
    
    # Compare keys across locales
    all_langs = list(locales.keys())
    base_lang = all_langs[0]
    
    for namespace in locales.get(base_lang, {}):
        base_keys = locales[base_lang].get(namespace, set())
        
        for lang in all_langs[1:]:
            other_keys = locales.get(lang, {}).get(namespace, set())
            
            missing = base_keys - other_keys
            if missing:
                issues.append(f"[X] {lang}/{namespace}: Missing {len(missing)} keys")
            
            extra = other_keys - base_keys
            if extra:
                issues.append(f"[!] {lang}/{namespace}: {len(extra)} extra keys")
    
    if not issues:
        passed.append("[OK] All locales have matching keys")
    
    return {'passed': passed, 'issues': issues}

def flatten_keys(d, prefix=''):
    """Flatten nested dict keys."""
    keys = set()
    for k, v in d.items():
        new_key = f"{prefix}.{k}" if prefix else k
        if isinstance(v, dict):
            keys.update(flatten_keys(v, new_key))
        else:
            keys.add(new_key)
    return keys

def check_hardcoded_strings(project_path: Path) -> dict:
    """Check for hardcoded strings in code files."""
    issues = []
    passed = []
    
    # Find code files
    extensions = {
        '.tsx': 'jsx', '.jsx': 'jsx', '.ts': 'jsx', '.js': 'jsx',
        '.vue': 'vue',
        '.py': 'python'
    }
    
    code_files = []
    for ext in extensions:
        code_files.extend(project_path.rglob(f"*{ext}"))
    
    code_files = [f for f in code_files if not any(x in str(f) for x in 
                  ['node_modules', '.git', 'dist', 'build', '__pycache__', 'venv', 'test', 'spec'])]
    
    if not code_files:
        return {'passed': ["[!] No code files found"], 'issues': []}
    
    files_with_i18n = 0
    files_with_hardcoded = 0
    hardcoded_examples = []
    
    for file_path in code_files[:50]:  # Limit
        try:
            content = file_path.read_text(encoding='utf-8', errors='ignore')
            ext = file_path.suffix
            file_type = extensions.get(ext, 'jsx')
            
            # Check for i18n usage
            has_i18n = any(re.search(p, content) for p in I18N_PATTERNS)
            if has_i18n:
                files_with_i18n += 1
            
            # Check for hardcoded strings
            patterns = HARDCODED_PATTERNS.get(file_type, [])
            hardcoded_found = False
            
            for pattern in patterns:
                matches = re.findall(pattern, content)
                if matches and not has_i18n:
                    hardcoded_found = True
                    if len(hardcoded_examples) < 5:
                        hardcoded_examples.append(f"{file_path.name}: {str(matches[0])[:40]}...")
            
            if hardcoded_found:
                files_with_hardcoded += 1
                
        except:
            continue
    
    passed.append(f"[OK] Analyzed {len(code_files)} code files")
    
    if files_with_i18n > 0:
        passed.append(f"[OK] {files_with_i18n} files use i18n")
    
    if files_with_hardcoded > 0:
        issues.append(f"[X] {files_with_hardcoded} files may have hardcoded strings")
        for ex in hardcoded_examples:
            issues.append(f"   → {ex}")
    else:
        passed.append("[OK] No obvious hardcoded strings detected")
    
    return {'passed': passed, 'issues': issues}

def main():
    target = sys.argv[1] if len(sys.argv) > 1 else "."
    project_path = Path(target)
    
    print("\n" + "=" * 60)
    print("  i18n CHECKER - Internationalization Audit")
    print("=" * 60 + "\n")
    
    # Check locale files
    locale_files = find_locale_files(project_path)
    locale_result = check_locale_completeness(locale_files)
    
    # Check hardcoded strings
    code_result = check_hardcoded_strings(project_path)
    
    # Print results
    print("[LOCALE FILES]")
    print("-" * 40)
    for item in locale_result['passed']:
        print(f"  {item}")
    for item in locale_result['issues']:
        print(f"  {item}")
    
    print("\n[CODE ANALYSIS]")
    print("-" * 40)
    for item in code_result['passed']:
        print(f"  {item}")
    for item in code_result['issues']:
        print(f"  {item}")
    
    # Summary
    critical_issues = sum(1 for i in locale_result['issues'] + code_result['issues'] if i.startswith("[X]"))
    
    print("\n" + "=" * 60)
    if critical_issues == 0:
        print("[OK] i18n CHECK: PASSED")
        sys.exit(0)
    else:
        print(f"[X] i18n CHECK: {critical_issues} issues found")
        sys.exit(1)

if __name__ == "__main__":
    main()


################################################################################
# PASTA: .agent/skills/lint-and-validate/scripts
################################################################################

--- ARQUIVO: .agent/skills/lint-and-validate/scripts/lint_runner.py ---
#!/usr/bin/env python3
"""
Lint Runner - Unified linting and type checking
Runs appropriate linters based on project type.

Usage:
    python lint_runner.py <project_path>

Supports:
    - Node.js: npm run lint, npx tsc --noEmit
    - Python: ruff check, mypy
"""

import subprocess
import sys
import json
from pathlib import Path
from datetime import datetime

# Fix Windows console encoding
try:
    sys.stdout.reconfigure(encoding='utf-8', errors='replace')
except:
    pass


def detect_project_type(project_path: Path) -> dict:
    """Detect project type and available linters."""
    result = {
        "type": "unknown",
        "linters": []
    }
    
    # Node.js project
    package_json = project_path / "package.json"
    if package_json.exists():
        result["type"] = "node"
        try:
            pkg = json.loads(package_json.read_text(encoding='utf-8'))
            scripts = pkg.get("scripts", {})
            deps = {**pkg.get("dependencies", {}), **pkg.get("devDependencies", {})}
            
            # Check for lint script
            if "lint" in scripts:
                result["linters"].append({"name": "npm lint", "cmd": ["npm", "run", "lint"]})
            elif "eslint" in deps:
                result["linters"].append({"name": "eslint", "cmd": ["npx", "eslint", "."]})
            
            # Check for TypeScript
            if "typescript" in deps or (project_path / "tsconfig.json").exists():
                result["linters"].append({"name": "tsc", "cmd": ["npx", "tsc", "--noEmit"]})
                
        except:
            pass
    
    # Python project
    if (project_path / "pyproject.toml").exists() or (project_path / "requirements.txt").exists():
        result["type"] = "python"
        
        # Check for ruff
        result["linters"].append({"name": "ruff", "cmd": ["ruff", "check", "."]})
        
        # Check for mypy
        if (project_path / "mypy.ini").exists() or (project_path / "pyproject.toml").exists():
            result["linters"].append({"name": "mypy", "cmd": ["mypy", "."]})
    
    return result


def run_linter(linter: dict, cwd: Path) -> dict:
    """Run a single linter and return results."""
    result = {
        "name": linter["name"],
        "passed": False,
        "output": "",
        "error": ""
    }
    
    try:
        proc = subprocess.run(
            linter["cmd"],
            cwd=str(cwd),
            capture_output=True,
            text=True,
            encoding='utf-8',
            errors='replace',
            timeout=120
        )
        
        result["output"] = proc.stdout[:2000] if proc.stdout else ""
        result["error"] = proc.stderr[:500] if proc.stderr else ""
        result["passed"] = proc.returncode == 0
        
    except FileNotFoundError:
        result["error"] = f"Command not found: {linter['cmd'][0]}"
    except subprocess.TimeoutExpired:
        result["error"] = "Timeout after 120s"
    except Exception as e:
        result["error"] = str(e)
    
    return result


def main():
    project_path = Path(sys.argv[1] if len(sys.argv) > 1 else ".").resolve()
    
    print(f"\n{'='*60}")
    print(f"[LINT RUNNER] Unified Linting")
    print(f"{'='*60}")
    print(f"Project: {project_path}")
    print(f"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Detect project type
    project_info = detect_project_type(project_path)
    print(f"Type: {project_info['type']}")
    print(f"Linters: {len(project_info['linters'])}")
    print("-"*60)
    
    if not project_info["linters"]:
        print("No linters found for this project type.")
        output = {
            "script": "lint_runner",
            "project": str(project_path),
            "type": project_info["type"],
            "checks": [],
            "passed": True,
            "message": "No linters configured"
        }
        print(json.dumps(output, indent=2))
        sys.exit(0)
    
    # Run each linter
    results = []
    all_passed = True
    
    for linter in project_info["linters"]:
        print(f"\nRunning: {linter['name']}...")
        result = run_linter(linter, project_path)
        results.append(result)
        
        if result["passed"]:
            print(f"  [PASS] {linter['name']}")
        else:
            print(f"  [FAIL] {linter['name']}")
            if result["error"]:
                print(f"  Error: {result['error'][:200]}")
            all_passed = False
    
    # Summary
    print("\n" + "="*60)
    print("SUMMARY")
    print("="*60)
    
    for r in results:
        icon = "[PASS]" if r["passed"] else "[FAIL]"
        print(f"{icon} {r['name']}")
    
    output = {
        "script": "lint_runner",
        "project": str(project_path),
        "type": project_info["type"],
        "checks": results,
        "passed": all_passed
    }
    
    print("\n" + json.dumps(output, indent=2))
    
    sys.exit(0 if all_passed else 1)


if __name__ == "__main__":
    main()

--- ARQUIVO: .agent/skills/lint-and-validate/scripts/type_coverage.py ---
#!/usr/bin/env python3
"""
Type Coverage Checker - Measures TypeScript/Python type coverage.
Identifies untyped functions, any usage, and type safety issues.
"""
import sys
import re
import subprocess
from pathlib import Path

# Fix Windows console encoding for Unicode output
try:
    sys.stdout.reconfigure(encoding='utf-8', errors='replace')
    sys.stderr.reconfigure(encoding='utf-8', errors='replace')
except AttributeError:
    pass  # Python < 3.7

def check_typescript_coverage(project_path: Path) -> dict:
    """Check TypeScript type coverage."""
    issues = []
    passed = []
    stats = {'any_count': 0, 'untyped_functions': 0, 'total_functions': 0}
    
    ts_files = list(project_path.rglob("*.ts")) + list(project_path.rglob("*.tsx"))
    ts_files = [f for f in ts_files if 'node_modules' not in str(f) and '.d.ts' not in str(f)]
    
    if not ts_files:
        return {'type': 'typescript', 'files': 0, 'passed': [], 'issues': ["[!] No TypeScript files found"], 'stats': stats}
    
    for file_path in ts_files[:30]:  # Limit
        try:
            content = file_path.read_text(encoding='utf-8', errors='ignore')
            
            # Count 'any' usage
            any_matches = re.findall(r':\s*any\b', content)
            stats['any_count'] += len(any_matches)
            
            # Find functions without return types
            # function name(params) { - no return type
            untyped = re.findall(r'function\s+\w+\s*\([^)]*\)\s*{', content)
            # Arrow functions without types: const fn = (x) => or (x) =>
            untyped += re.findall(r'=\s*\([^:)]*\)\s*=>', content)
            stats['untyped_functions'] += len(untyped)
            
            # Count typed functions
            typed = re.findall(r'function\s+\w+\s*\([^)]*\)\s*:\s*\w+', content)
            typed += re.findall(r':\s*\([^)]*\)\s*=>\s*\w+', content)
            stats['total_functions'] += len(typed) + len(untyped)
            
        except Exception:
            continue
    
    # Analyze results
    if stats['any_count'] == 0:
        passed.append("[OK] No 'any' types found")
    elif stats['any_count'] <= 5:
        issues.append(f"[!] {stats['any_count']} 'any' types found (acceptable)")
    else:
        issues.append(f"[X] {stats['any_count']} 'any' types found (too many)")
    
    if stats['total_functions'] > 0:
        typed_ratio = (stats['total_functions'] - stats['untyped_functions']) / stats['total_functions'] * 100
        if typed_ratio >= 80:
            passed.append(f"[OK] Type coverage: {typed_ratio:.0f}%")
        elif typed_ratio >= 50:
            issues.append(f"[!] Type coverage: {typed_ratio:.0f}% (improve)")
        else:
            issues.append(f"[X] Type coverage: {typed_ratio:.0f}% (too low)")
    
    passed.append(f"[OK] Analyzed {len(ts_files)} TypeScript files")
    
    return {'type': 'typescript', 'files': len(ts_files), 'passed': passed, 'issues': issues, 'stats': stats}

def check_python_coverage(project_path: Path) -> dict:
    """Check Python type hints coverage."""
    issues = []
    passed = []
    stats = {'untyped_functions': 0, 'typed_functions': 0, 'any_count': 0}
    
    py_files = list(project_path.rglob("*.py"))
    py_files = [f for f in py_files if not any(x in str(f) for x in ['venv', '__pycache__', '.git', 'node_modules'])]
    
    if not py_files:
        return {'type': 'python', 'files': 0, 'passed': [], 'issues': ["[!] No Python files found"], 'stats': stats}
    
    for file_path in py_files[:30]:  # Limit
        try:
            content = file_path.read_text(encoding='utf-8', errors='ignore')
            
            # Count Any usage
            any_matches = re.findall(r':\s*Any\b', content)
            stats['any_count'] += len(any_matches)
            
            # Find functions with type hints
            typed_funcs = re.findall(r'def\s+\w+\s*\([^)]*:[^)]+\)', content)
            typed_funcs += re.findall(r'def\s+\w+\s*\([^)]*\)\s*->', content)
            stats['typed_functions'] += len(typed_funcs)
            
            # Find functions without type hints
            all_funcs = re.findall(r'def\s+\w+\s*\(', content)
            stats['untyped_functions'] += len(all_funcs) - len(typed_funcs)
            
        except Exception:
            continue
    
    total = stats['typed_functions'] + stats['untyped_functions']
    
    if total > 0:
        typed_ratio = stats['typed_functions'] / total * 100
        if typed_ratio >= 70:
            passed.append(f"[OK] Type hints coverage: {typed_ratio:.0f}%")
        elif typed_ratio >= 40:
            issues.append(f"[!] Type hints coverage: {typed_ratio:.0f}%")
        else:
            issues.append(f"[X] Type hints coverage: {typed_ratio:.0f}% (add type hints)")
    
    if stats['any_count'] == 0:
        passed.append("[OK] No 'Any' types found")
    elif stats['any_count'] <= 3:
        issues.append(f"[!] {stats['any_count']} 'Any' types found")
    else:
        issues.append(f"[X] {stats['any_count']} 'Any' types found")
    
    passed.append(f"[OK] Analyzed {len(py_files)} Python files")
    
    return {'type': 'python', 'files': len(py_files), 'passed': passed, 'issues': issues, 'stats': stats}

def main():
    target = sys.argv[1] if len(sys.argv) > 1 else "."
    project_path = Path(target)
    
    print("\n" + "=" * 60)
    print("  TYPE COVERAGE CHECKER")
    print("=" * 60 + "\n")
    
    results = []
    
    # Check TypeScript
    ts_result = check_typescript_coverage(project_path)
    if ts_result['files'] > 0:
        results.append(ts_result)
    
    # Check Python
    py_result = check_python_coverage(project_path)
    if py_result['files'] > 0:
        results.append(py_result)
    
    if not results:
        print("[!] No TypeScript or Python files found.")
        sys.exit(0)
    
    # Print results
    critical_issues = 0
    for result in results:
        print(f"\n[{result['type'].upper()}]")
        print("-" * 40)
        for item in result['passed']:
            print(f"  {item}")
        for item in result['issues']:
            print(f"  {item}")
            if item.startswith("[X]"):
                critical_issues += 1
    
    print("\n" + "=" * 60)
    if critical_issues == 0:
        print("[OK] TYPE COVERAGE: ACCEPTABLE")
        sys.exit(0)
    else:
        print(f"[X] TYPE COVERAGE: {critical_issues} critical issues")
        sys.exit(1)

if __name__ == "__main__":
    main()


################################################################################
# PASTA: .agent/skills/mobile-design/scripts
################################################################################

--- ARQUIVO: .agent/skills/mobile-design/scripts/mobile_audit.py ---
#!/usr/bin/env python3
"""
Mobile UX Audit Script - Full Mobile Design Coverage

Analyzes React Native / Flutter code for compliance with:

1. TOUCH PSYCHOLOGY (touch-psychology.md):
   - Touch Target Sizes (44pt iOS, 48dp Android, 44px WCAG)
   - Touch Target Spacing (8px minimum gap)
   - Thumb Zone Placement (primary CTAs at bottom)
   - Gesture Alternatives (visible buttons for swipe)
   - Haptic Feedback Patterns
   - Touch Feedback Timing (<50ms)
   - Touch Accessibility (motor impairment support)

2. MOBILE PERFORMANCE (mobile-performance.md):
   - ScrollView vs FlatList (CRITICAL)
   - React.memo for List Items
   - useCallback for renderItem
   - Stable keyExtractor (NOT index)
   - useNativeDriver for Animations
   - Memory Leak Prevention (cleanup)
   - Console.log Detection
   - Inline Function Detection
   - Animation Performance (transform/opacity only)

3. MOBILE NAVIGATION (mobile-navigation.md):
   - Tab Bar Max Items (5)
   - Tab State Preservation
   - Proper Back Handling
   - Deep Link Support
   - Navigation Structure

4. MOBILE TYPOGRAPHY (mobile-typography.md):
   - System Font Usage
   - Dynamic Type Support (iOS)
   - Text Scaling Constraints
   - Mobile Line Height
   - Font Size Limits

5. MOBILE COLOR SYSTEM (mobile-color-system.md):
   - Pure Black Avoidance (#000000)
   - OLED Optimization
   - Dark Mode Support
   - Contrast Ratios

6. PLATFORM iOS (platform-ios.md):
   - SF Symbols Usage
   - iOS Navigation Patterns
   - iOS Haptic Types
   - iOS-Specific Components

7. PLATFORM ANDROID (platform-android.md):
   - Material Icons Usage
   - Android Navigation Patterns
   - Ripple Effects
   - Android-Specific Components

8. MOBILE BACKEND (mobile-backend.md):
   - Secure Storage (NOT AsyncStorage)
   - Offline Handling
   - Push Notification Support
   - API Response Caching

Total: 50+ mobile-specific checks
"""

import sys
import os
import re
import json
from pathlib import Path

class MobileAuditor:
    def __init__(self):
        self.issues = []
        self.warnings = []
        self.passed_count = 0
        self.files_checked = 0

    def audit_file(self, filepath: str) -> None:
        try:
            with open(filepath, 'r', encoding='utf-8', errors='replace') as f:
                content = f.read()
        except:
            return

        self.files_checked += 1
        filename = os.path.basename(filepath)

        # Detect framework
        is_react_native = bool(re.search(r'react-native|@react-navigation|React\.Native', content))
        is_flutter = bool(re.search(r'import \'package:flutter|MaterialApp|Widget\.build', content))

        if not (is_react_native or is_flutter):
            return  # Skip non-mobile files

        # --- 1. TOUCH PSYCHOLOGY CHECKS ---

        # 1.1 Touch Target Size Check
        # Look for small touch targets
        small_sizes = re.findall(r'(?:width|height|size):\s*([0-3]\d)', content)
        for size in small_sizes:
            if int(size) < 44:
                self.issues.append(f"[Touch Target] {filename}: Touch target size {size}px < 44px minimum (iOS: 44pt, Android: 48dp)")

        # 1.2 Touch Target Spacing Check
        # Look for inadequate spacing between touchable elements
        small_gaps = re.findall(r'(?:margin|gap):\s*([0-7])\s*(?:px|dp)', content)
        for gap in small_gaps:
            if int(gap) < 8:
                self.warnings.append(f"[Touch Spacing] {filename}: Touch target spacing {gap}px < 8px minimum. Accidental taps risk.")

        # 1.3 Thumb Zone Placement Check
        # Primary CTAs should be at bottom (easy thumb reach)
        primary_buttons = re.findall(r'(?:testID|id):\s*["\'](?:.*(?:primary|cta|submit|confirm)[^"\']*)["\']', content, re.IGNORECASE)
        has_bottom_placement = bool(re.search(r'position:\s*["\']?absolute["\']?|bottom:\s*\d+|style.*bottom|justifyContent:\s*["\']?flex-end', content))
        if primary_buttons and not has_bottom_placement:
            self.warnings.append(f"[Thumb Zone] {filename}: Primary CTA may not be in thumb zone (bottom). Place primary actions at bottom for easy reach.")

        # 1.4 Gesture Alternatives Check
        # Swipe actions should have visible button alternatives
        has_swipe_gestures = bool(re.search(r'Swipeable|onSwipe|PanGestureHandler|swipe', content))
        has_visible_buttons = bool(re.search(r'Button.*(?:delete|archive|more)|TouchableOpacity|Pressable', content))
        if has_swipe_gestures and not has_visible_buttons:
            self.warnings.append(f"[Gestures] {filename}: Swipe gestures detected without visible button alternatives. Motor impaired users need alternatives.")

        # 1.5 Haptic Feedback Check
        # Important actions should have haptic feedback
        has_important_actions = bool(re.search(r'(?:onPress|onSubmit|delete|remove|confirm|purchase)', content))
        has_haptics = bool(re.search(r'Haptics|Vibration|react-native-haptic-feedback|FeedbackManager', content))
        if has_important_actions and not has_haptics:
            self.warnings.append(f"[Haptics] {filename}: Important actions without haptic feedback. Consider adding haptic confirmation.")

        # 1.6 Touch Feedback Timing Check
        # Touch feedback should be immediate (<50ms)
        if is_react_native:
            has_pressable = bool(re.search(r'Pressable|TouchableOpacity', content))
            has_feedback_state = bool(re.search(r'pressed|style.*opacity|underlay', content))
            if has_pressable and not has_feedback_state:
                self.warnings.append(f"[Touch Feedback] {filename}: Pressable without visual feedback state. Add opacity/scale change for tap confirmation.")

        # --- 2. MOBILE PERFORMANCE CHECKS ---

        # 2.1 CRITICAL: ScrollView vs FlatList
        has_scrollview = bool(re.search(r'<ScrollView|ScrollView\.', content))
        has_map_in_scrollview = bool(re.search(r'ScrollView.*\.map\(|ScrollView.*\{.*\.map', content))
        if has_scrollview and has_map_in_scrollview:
            self.issues.append(f"[Performance CRITICAL] {filename}: ScrollView with .map() detected. Use FlatList for lists to prevent memory explosion.")

        # 2.2 React.memo Check
        if is_react_native:
            has_list = bool(re.search(r'FlatList|FlashList|SectionList', content))
            has_react_memo = bool(re.search(r'React\.memo|memo\(', content))
            if has_list and not has_react_memo:
                self.warnings.append(f"[Performance] {filename}: FlatList without React.memo on list items. Items will re-render on every parent update.")

        # 2.3 useCallback Check
        if is_react_native:
            has_flatlist = bool(re.search(r'FlatList|FlashList', content))
            has_use_callback = bool(re.search(r'useCallback', content))
            if has_flatlist and not has_use_callback:
                self.warnings.append(f"[Performance] {filename}: FlatList renderItem without useCallback. New function created every render.")

        # 2.4 keyExtractor Check (CRITICAL)
        if is_react_native:
            has_flatlist = bool(re.search(r'FlatList', content))
            has_key_extractor = bool(re.search(r'keyExtractor', content))
            uses_index_key = bool(re.search(r'key=\{.*index.*\}|key:\s*index', content))
            if has_flatlist and not has_key_extractor:
                self.issues.append(f"[Performance CRITICAL] {filename}: FlatList without keyExtractor. Index-based keys cause bugs on reorder/delete.")
            if uses_index_key:
                self.issues.append(f"[Performance CRITICAL] {filename}: Using index as key. This causes bugs when list changes. Use unique ID from data.")

        # 2.5 useNativeDriver Check
        if is_react_native:
            has_animated = bool(re.search(r'Animated\.', content))
            has_native_driver = bool(re.search(r'useNativeDriver:\s*true', content))
            has_native_driver_false = bool(re.search(r'useNativeDriver:\s*false', content))
            if has_animated and has_native_driver_false:
                self.warnings.append(f"[Performance] {filename}: Animation with useNativeDriver: false. Use true for 60fps (only supports transform/opacity).")
            if has_animated and not has_native_driver:
                self.warnings.append(f"[Performance] {filename}: Animated component without useNativeDriver. Add useNativeDriver: true for 60fps.")

        # 2.6 Memory Leak Check
        if is_react_native:
            has_effect = bool(re.search(r'useEffect', content))
            has_cleanup = bool(re.search(r'return\s*\(\)\s*=>|return\s+function', content))
            has_subscriptions = bool(re.search(r'addEventListener|subscribe|\.focus\(\)|\.off\(', content))
            if has_effect and has_subscriptions and not has_cleanup:
                self.issues.append(f"[Memory Leak] {filename}: useEffect with subscriptions but no cleanup function. Memory leak on unmount.")

        # 2.7 Console.log Detection
        console_logs = len(re.findall(r'console\.log|console\.warn|console\.error|console\.debug', content))
        if console_logs > 5:
            self.warnings.append(f"[Performance] {filename}: {console_logs} console.log statements detected. Remove before production (blocks JS thread).")

        # 2.8 Inline Function Detection
        if is_react_native:
            inline_functions = re.findall(r'(?:onPress|onPressIn|onPressOut|renderItem):\s*\([^)]*\)\s*=>', content)
            if len(inline_functions) > 3:
                self.warnings.append(f"[Performance] {filename}: {len(inline_functions)} inline arrow functions in props. Creates new function every render. Use useCallback.")

        # 2.9 Animation Properties Check
        # Warn if animating expensive properties
        animating_layout = bool(re.search(r'Animated\.timing.*(?:width|height|margin|padding)', content))
        if animating_layout:
            self.issues.append(f"[Performance] {filename}: Animating layout properties (width/height/margin). Use transform/opacity for 60fps.")

        # --- 3. MOBILE NAVIGATION CHECKS ---

        # 3.1 Tab Bar Max Items Check
        tab_bar_items = len(re.findall(r'Tab\.Screen|createBottomTabNavigator|BottomTab', content))
        if tab_bar_items > 5:
            self.warnings.append(f"[Navigation] {filename}: {tab_bar_items} tab bar items (max 5 recommended). More than 5 becomes hard to tap.")

        # 3.2 Tab State Preservation Check
        has_tab_nav = bool(re.search(r'createBottomTabNavigator|Tab\.Navigator', content))
        if has_tab_nav:
            # Look for lazy prop (false preserves state)
            has_lazy_false = bool(re.search(r'lazy:\s*false', content))
            if not has_lazy_false:
                self.warnings.append(f"[Navigation] {filename}: Tab navigation without lazy: false. Tabs may lose state on switch.")

        # 3.3 Back Handling Check
        has_back_listener = bool(re.search(r'BackHandler|useFocusEffect|navigation\.addListener', content))
        has_custom_back = bool(re.search(r'onBackPress|handleBackPress', content))
        if has_custom_back and not has_back_listener:
            self.warnings.append(f"[Navigation] {filename}: Custom back handling without BackHandler listener. May not work correctly.")

        # 3.4 Deep Link Support Check
        has_linking = bool(re.search(r'Linking\.|Linking\.openURL|deepLink|universalLink', content))
        has_config = bool(re.search(r'apollo-link|react-native-screens|navigation\.link', content))
        if not has_linking and not has_config:
            self.passed_count += 1
        else:
            if has_linking and not has_config:
                self.warnings.append(f"[Navigation] {filename}: Deep linking detected but may lack proper configuration. Test notification/share flows.")

        # --- 4. MOBILE TYPOGRAPHY CHECKS ---

        # 4.1 System Font Check
        if is_react_native:
            has_custom_font = bool(re.search(r"fontFamily:\s*[\"'][^\"']+", content))
            has_system_font = bool(re.search(r"fontFamily:\s*[\"']?(?:System|San Francisco|Roboto|-apple-system)", content))
            if has_custom_font and not has_system_font:
                self.warnings.append(f"[Typography] {filename}: Custom font detected. Consider system fonts (iOS: SF Pro, Android: Roboto) for native feel.")

        # 4.2 Text Scaling Check (iOS Dynamic Type)
        if is_react_native:
            has_font_sizes = bool(re.search(r'fontSize:', content))
            has_scaling = bool(re.search(r'allowFontScaling:\s*true|responsiveFontSize|useWindowDimensions', content))
            if has_font_sizes and not has_scaling:
                self.warnings.append(f"[Typography] {filename}: Fixed font sizes without scaling support. Consider allowFontScaling for accessibility.")

        # 4.3 Mobile Line Height Check
        line_heights = re.findall(r'lineHeight:\s*([\d.]+)', content)
        for lh in line_heights:
            if float(lh) > 1.8:
                self.warnings.append(f"[Typography] {filename}: lineHeight {lh} too high for mobile. Mobile text needs tighter spacing (1.3-1.5).")

        # 4.4 Font Size Limits
        font_sizes = re.findall(r'fontSize:\s*([\d.]+)', content)
        for fs in font_sizes:
            size = float(fs)
            if size < 12:
                self.warnings.append(f"[Typography] {filename}: fontSize {size}px below 12px minimum readability.")
            elif size > 32:
                self.warnings.append(f"[Typography] {filename}: fontSize {size}px very large. Consider using responsive scaling.")

        # --- 5. MOBILE COLOR SYSTEM CHECKS ---

        # 5.1 Pure Black Avoidance
        if re.search(r'#000000|color:\s*black|backgroundColor:\s*["\']?black', content):
            self.warnings.append(f"[Color] {filename}: Pure black (#000000) detected. Use dark gray (#1C1C1E iOS, #121212 Android) for better OLED/battery.")

        # 5.2 Dark Mode Support
        has_color_schemes = bool(re.search(r'useColorScheme|colorScheme|appearance:\s*["\']?dark', content))
        has_dark_mode_style = bool(re.search(r'\\\?.*dark|style:\s*.*dark|isDark', content))
        if not has_color_schemes and not has_dark_mode_style:
            self.warnings.append(f"[Color] {filename}: No dark mode support detected. Consider useColorScheme for system dark mode.")

        # --- 6. PLATFORM iOS CHECKS ---

        if is_react_native:
            # 6.1 SF Symbols Check
            has_ios_icons = bool(re.search(r'@expo/vector-icons|ionicons', content))
            has_sf_symbols = bool(re.search(r'sf-symbol|SF Symbols', content))
            if has_ios_icons and not has_sf_symbols:
                self.passed_count += 1

            # 6.2 iOS Haptic Types
            has_haptic_import = bool(re.search(r'expo-haptics|react-native-haptic-feedback', content))
            has_haptic_types = bool(re.search(r'ImpactFeedback|NotificationFeedback|SelectionFeedback', content))
            if has_haptic_import and not has_haptic_types:
                self.warnings.append(f"[iOS Haptics] {filename}: Haptic library imported but not using typed haptics (Impact/Notification/Selection).")

            # 6.3 iOS Safe Area
            has_safe_area = bool(re.search(r'SafeAreaView|useSafeAreaInsets|safeArea', content))
            if not has_safe_area:
                self.warnings.append(f"[iOS] {filename}: No SafeArea detected. Content may be hidden by notch/home indicator.")

        # --- 7. PLATFORM ANDROID CHECKS ---

        if is_react_native:
            # 7.1 Material Icons Check
            has_material_icons = bool(re.search(r'@expo/vector-icons|MaterialIcons', content))
            if has_material_icons:
                self.passed_count += 1

            # 7.2 Ripple Effect
            has_ripple = bool(re.search(r'ripple|android_ripple|foregroundRipple', content))
            has_pressable = bool(re.search(r'Pressable|Touchable', content))
            if has_pressable and not has_ripple:
                self.warnings.append(f"[Android] {filename}: Touchable without ripple effect. Android users expect ripple feedback.")

            # 7.3 Hardware Back Button
            if is_react_native:
                has_back_button = bool(re.search(r'BackHandler|useBackHandler', content))
                has_navigation = bool(re.search(r'@react-navigation', content))
                if has_navigation and not has_back_button:
                    self.warnings.append(f"[Android] {filename}: React Navigation detected without BackHandler listener. Android hardware back may not work correctly.")

        # --- 8. MOBILE BACKEND CHECKS ---

        # 8.1 Secure Storage Check
        has_async_storage = bool(re.search(r'AsyncStorage|@react-native-async-storage', content))
        has_secure_storage = bool(re.search(r'SecureStore|Keychain|EncryptedSharedPreferences', content))
        has_token_storage = bool(re.search(r'token|jwt|auth.*storage', content, re.IGNORECASE))
        if has_token_storage and has_async_storage and not has_secure_storage:
            self.issues.append(f"[Security] {filename}: Storing auth tokens in AsyncStorage (insecure). Use SecureStore (iOS) / EncryptedSharedPreferences (Android).")

        # 8.2 Offline Handling Check
        has_network = bool(re.search(r'fetch|axios|netinfo|@react-native-community/netinfo', content))
        has_offline = bool(re.search(r'offline|isConnected|netInfo|cache.*offline', content))
        if has_network and not has_offline:
            self.warnings.append(f"[Offline] {filename}: Network requests detected without offline handling. Consider NetInfo for connection status.")

        # 8.3 Push Notification Support
        has_push = bool(re.search(r'Notifications|pushNotification|Firebase\.messaging|PushNotificationIOS', content))
        has_push_handler = bool(re.search(r'onNotification|addNotificationListener|notification\.open', content))
        if has_push and not has_push_handler:
            self.warnings.append(f"[Push] {filename}: Push notifications imported but no handler found. May miss notifications.")

        # --- 9. EXTENDED MOBILE TYPOGRAPHY CHECKS ---

        # 9.1 iOS Type Scale Check
        if is_react_native:
            # Check for iOS text styles that match HIG
            has_large_title = bool(re.search(r'fontSize:\s*34|largeTitle|font-weight:\s*["\']?bold', content))
            has_title_1 = bool(re.search(r'fontSize:\s*28', content))
            has_headline = bool(re.search(r'fontSize:\s*17.*semibold|headline', content))
            has_body = bool(re.search(r'fontSize:\s*17.*regular|body', content))

            # Check if following iOS scale roughly
            font_sizes = re.findall(r'fontSize:\s*([\d.]+)', content)
            ios_scale_sizes = [34, 28, 22, 20, 17, 16, 15, 13, 12, 11]
            matching_ios = sum(1 for size in font_sizes if any(abs(float(size) - ios_size) < 1 for ios_size in ios_scale_sizes))

            if len(font_sizes) > 3 and matching_ios < len(font_sizes) / 2:
                self.warnings.append(f"[iOS Typography] {filename}: Font sizes don't match iOS type scale. Consider iOS text styles for native feel.")

        # 9.2 Android Material Type Scale Check
        if is_react_native:
            # Check for Material 3 text styles
            has_display = bool(re.search(r'fontSize:\s*[456][0-9]|display', content))
            has_headline_material = bool(re.search(r'fontSize:\s*[23][0-9]|headline', content))
            has_title_material = bool(re.search(r'fontSize:\s*2[12][0-9].*medium|title', content))
            has_body_material = bool(re.search(r'fontSize:\s*1[456].*regular|body', content))
            has_label = bool(re.search(r'fontSize:\s*1[1234].*medium|label', content))

            # Check if using sp (scale-independent pixels)
            uses_sp = bool(re.search(r'\d+\s*sp\b', content))
            if has_display or has_headline_material:
                if not uses_sp:
                    self.warnings.append(f"[Android Typography] {filename}: Material typography detected without sp units. Use sp for text to respect user font size preferences.")

        # 9.3 Modular Scale Check
        # Check if font sizes follow modular scale
        font_sizes = re.findall(r'fontSize:\s*(\d+(?:\.\d+)?)', content)
        if len(font_sizes) > 3:
            sorted_sizes = sorted(set([float(s) for s in font_sizes]))
            ratios = []
            for i in range(1, len(sorted_sizes)):
                if sorted_sizes[i-1] > 0:
                    ratios.append(sorted_sizes[i] / sorted_sizes[i-1])

            # Common ratios: 1.125, 1.2, 1.25, 1.333, 1.5
            common_ratios = {1.125, 1.2, 1.25, 1.333, 1.5}
            for ratio in ratios[:3]:
                if not any(abs(ratio - cr) < 0.03 for cr in common_ratios):
                    self.warnings.append(f"[Typography] {filename}: Font sizes may not follow modular scale (ratio: {ratio:.2f}). Consider consistent ratio.")
                    break

        # 9.4 Line Length Check (Mobile-specific)
        # Mobile text should be 40-60 characters max
        if is_react_native:
            has_long_text = bool(re.search(r'<Text[^>]*>[^<]{40,}', content))
            has_max_width = bool(re.search(r'maxWidth|max-w-\d+|width:\s*["\']?\d+', content))
            if has_long_text and not has_max_width:
                self.warnings.append(f"[Mobile Typography] {filename}: Text without max-width constraint. Mobile text should be 40-60 characters per line for readability.")

        # 9.5 Font Weight Pattern Check
        # Check for font weight distribution
        if is_react_native:
            font_weights = re.findall(r'fontWeight:\s*["\']?(\d+|normal|bold|medium|light)', content)
            weight_map = {'normal': '400', 'light': '300', 'medium': '500', 'bold': '700'}
            numeric_weights = []
            for w in font_weights:
                val = weight_map.get(w.lower(), w)
                try:
                    numeric_weights.append(int(val))
                except:
                    pass

            # Check if overusing bold (mobile should be regular-dominant)
            bold_count = sum(1 for w in numeric_weights if w >= 700)
            regular_count = sum(1 for w in numeric_weights if 400 <= w < 500)
            if bold_count > regular_count:
                self.warnings.append(f"[Mobile Typography] {filename}: More bold weights than regular. Mobile typography should be regular-dominant for readability.")

        # --- 10. EXTENDED MOBILE COLOR SYSTEM CHECKS ---

        # 10.1 OLED Optimization Check
        # Check for near-black colors instead of pure black
        if re.search(r'#121212|#1A1A1A|#0D0D0D', content):
            self.passed_count += 1  # Good OLED optimization
        elif re.search(r'backgroundColor:\s*["\']?#000000', content):
            # Using pure black for background is OK for OLED
            pass
        elif re.search(r'backgroundColor:\s*["\']?#[0-9A-Fa-f]{6}', content):
            # Check if using light colors in dark mode (bad for OLED)
            self.warnings.append(f"[Mobile Color] {filename}: Consider OLED-optimized dark backgrounds (#121212 Android, #000000 iOS) for battery savings.")

        # 10.2 Saturated Color Detection (Battery)
        # Highly saturated colors consume more power on OLED
        hex_colors = re.findall(r'#([0-9A-Fa-f]{2})([0-9A-Fa-f]{2})([0-9A-Fa-f]{2})', content)
        saturated_count = 0
        for r, g, b in hex_colors:
            # Convert to RGB 0-255
            try:
                r_val, g_val, b_val = int(r, 16), int(g, 16), int(b, 16)
                max_val = max(r_val, g_val, b_val)
                min_val = min(r_val, g_val, b_val)
                # Saturation = (max - min) / max
                if max_val > 0:
                    saturation = (max_val - min_val) / max_val
                    if saturation > 0.8:  # Highly saturated
                        saturated_count += 1
            except:
                pass

        if saturated_count > 10:
            self.warnings.append(f"[Mobile Color] {filename}: {saturated_count} highly saturated colors detected. Desaturated colors save battery on OLED screens.")

        # 10.3 Outdoor Visibility Check
        # Low contrast combinations fail in outdoor sunlight
        light_colors = re.findall(r'#[0-9A-Fa-f]{6}|rgba?\([^)]+\)', content)
        # Check for potential low contrast (light gray on white, dark gray on black)
        potential_low_contrast = bool(re.search(r'#[EeEeEeEe].*#ffffff|#999999.*#ffffff|#333333.*#000000|#666666.*#000000', content))
        if potential_low_contrast:
            self.warnings.append(f"[Mobile Color] {filename}: Possible low contrast combination detected. Critical for outdoor visibility. Ensure WCAG AAA (7:1) for mobile.")

        # 10.4 Dark Mode Text Color Check
        # In dark mode, text should not be pure white
        has_dark_mode = bool(re.search(r'dark:\s*|isDark|useColorScheme|colorScheme:\s*["\']?dark', content))
        if has_dark_mode:
            has_pure_white_text = bool(re.search(r'color:\s*["\']?#ffffff|#fff["\']?\}|textColor:\s*["\']?white', content))
            if has_pure_white_text:
                self.warnings.append(f"[Mobile Color] {filename}: Pure white text (#FFFFFF) in dark mode. Use #E8E8E8 or light gray for better readability.")

        # --- 11. EXTENDED PLATFORM IOS CHECKS ---

        if is_react_native:
            # 11.1 SF Pro Font Detection
            has_sf_pro = bool(re.search(r'SF Pro|SFPro|fontFamily:\s*["\']?[-\s]*SF', content))
            has_custom_font = bool(re.search(r'fontFamily:\s*["\'][^"\']+', content))
            if has_custom_font and not has_sf_pro:
                self.warnings.append(f"[iOS] {filename}: Custom font without SF Pro fallback. Consider SF Pro Text for body, SF Pro Display for headings.")

            # 11.2 iOS System Colors Check
            # Check for semantic color usage
            has_label = bool(re.search(r'color:\s*["\']?label|\.label', content))
            has_secondaryLabel = bool(re.search(r'secondaryLabel|\.secondaryLabel', content))
            has_systemBackground = bool(re.search(r'systemBackground|\.systemBackground', content))

            has_hardcoded_gray = bool(re.search(r'#[78]0{4}', content))
            if has_hardcoded_gray and not (has_label or has_secondaryLabel):
                self.warnings.append(f"[iOS] {filename}: Hardcoded gray colors detected. Consider iOS semantic colors (label, secondaryLabel) for automatic dark mode.")

            # 11.3 iOS Accent Colors Check
            ios_blue = bool(re.search(r'#007AFF|#0A84FF|systemBlue', content))
            ios_green = bool(re.search(r'#34C759|#30D158|systemGreen', content))
            ios_red = bool(re.search(r'#FF3B30|#FF453A|systemRed', content))

            has_custom_primary = bool(re.search(r'primaryColor|theme.*primary|colors\.primary', content))
            if has_custom_primary and not (ios_blue or ios_green or ios_red):
                self.warnings.append(f"[iOS] {filename}: Custom primary color without iOS system color fallback. Consider systemBlue for consistent iOS feel.")

            # 11.4 iOS Navigation Patterns Check
            has_navigation_bar = bool(re.search(r'navigationOptions|headerStyle|cardStyle', content))
            has_header_title = bool(re.search(r'title:\s*["\']|headerTitle|navigation\.setOptions', content))
            if has_navigation_bar and not has_header_title:
                self.warnings.append(f"[iOS] {filename}: Navigation bar detected without title. iOS apps should have clear context in nav bar.")

            # 11.5 iOS Component Patterns Check
            # Check for iOS-specific components
            has_alert = bool(re.search(r'Alert\.alert|showAlert', content))
            has_action_sheet = bool(re.search(r'ActionSheet|ActionSheetIOS|showActionSheetWithOptions', content))
            has_activity_indicator = bool(re.search(r'ActivityIndicator|ActivityIndic', content))

            if has_alert or has_action_sheet or has_activity_indicator:
                self.passed_count += 1  # Good iOS component usage

        # --- 12. EXTENDED PLATFORM ANDROID CHECKS ---

        if is_react_native:
            # 12.1 Roboto Font Detection
            has_roboto = bool(re.search(r'Roboto|fontFamily:\s*["\']?[-\s]*Roboto', content))
            has_custom_font = bool(re.search(r'fontFamily:\s*["\'][^"\']+', content))
            if has_custom_font and not has_roboto:
                self.warnings.append(f"[Android] {filename}: Custom font without Roboto fallback. Roboto is optimized for Android displays.")

            # 12.2 Material 3 Dynamic Color Check
            has_material_colors = bool(re.search(r'MD3|MaterialYou|dynamicColor|useColorScheme', content))
            has_theme_provider = bool(re.search(r'MaterialTheme|ThemeProvider|PaperProvider|ThemeProvider', content))
            if not has_material_colors and not has_theme_provider:
                self.warnings.append(f"[Android] {filename}: No Material 3 dynamic color detected. Consider Material 3 theming for personalized feel.")

            # 12.3 Material Elevation Check
            # Check for elevation values (Material 3 uses elevation for depth)
            has_elevation = bool(re.search(r'elevation:\s*\d+|shadowOpacity|shadowRadius|android:elevation', content))
            has_box_shadow = bool(re.search(r'boxShadow:', content))
            if has_box_shadow and not has_elevation:
                self.warnings.append(f"[Android] {filename}: CSS box-shadow detected without elevation. Consider Material elevation system for consistent depth.")

            # 12.4 Material Component Patterns Check
            # Check for Material components
            has_ripple = bool(re.search(r'ripple|android_ripple|foregroundRipple', content))
            has_card = bool(re.search(r'Card|Paper|elevation.*\d+', content))
            has_fab = bool(re.search(r'FAB|FloatingActionButton|fab', content))
            has_snackbar = bool(re.search(r'Snackbar|showSnackBar|Toast', content))

            material_component_count = sum([has_ripple, has_card, has_fab, has_snackbar])
            if material_component_count >= 2:
                self.passed_count += 1  # Good Material design usage

            # 12.5 Android Navigation Patterns Check
            has_top_app_bar = bool(re.search(r'TopAppBar|AppBar|CollapsingToolbar', content))
            has_bottom_nav = bool(re.search(r'BottomNavigation|BottomNav', content))
            has_navigation_rail = bool(re.search(r'NavigationRail', content))

            if has_bottom_nav:
                self.passed_count += 1  # Good Android pattern
            elif has_top_app_bar and not (has_bottom_nav or has_navigation_rail):
                self.warnings.append(f"[Android] {filename}: TopAppBar without bottom navigation. Consider BottomNavigation for thumb-friendly access.")

        # --- 13. MOBILE TESTING CHECKS ---

        # 13.1 Testing Tool Detection
        has_rntl = bool(re.search(r'react-native-testing-library|@testing-library', content))
        has_detox = bool(re.search(r'detox|element\(|by\.text|by\.id', content))
        has_maestro = bool(re.search(r'maestro|\.yaml$', content))
        has_jest = bool(re.search(r'jest|describe\(|test\(|it\(', content))

        testing_tools = []
        if has_jest: testing_tools.append('Jest')
        if has_rntl: testing_tools.append('RNTL')
        if has_detox: testing_tools.append('Detox')
        if has_maestro: testing_tools.append('Maestro')

        if len(testing_tools) == 0:
            self.warnings.append(f"[Testing] {filename}: No testing framework detected. Consider Jest (unit) + Detox/Maestro (E2E) for mobile.")

        # 13.2 Test Pyramid Balance Check
        test_files = len(re.findall(r'\.test\.(tsx|ts|js|jsx)|\.spec\.', content))
        e2e_tests = len(re.findall(r'detox|maestro|e2e|spec\.e2e', content.lower()))

        if test_files > 0 and e2e_tests == 0:
            self.warnings.append(f"[Testing] {filename}: Unit tests found but no E2E tests. Mobile needs E2E on real devices for complete coverage.")

        # 13.3 Accessibility Label Check (Mobile-specific)
        if is_react_native:
            has_pressable = bool(re.search(r'Pressable|TouchableOpacity|TouchableHighlight', content))
            has_a11y_label = bool(re.search(r'accessibilityLabel|aria-label|testID', content))
            if has_pressable and not has_a11y_label:
                self.warnings.append(f"[A11y Mobile] {filename}: Touchable element without accessibilityLabel. Screen readers need labels for all interactive elements.")

        # --- 14. MOBILE DEBUGGING CHECKS ---

        # 14.1 Performance Profiling Check
        has_performance = bool(re.search(r'Performance|systrace|profile|Flipper', content))
        has_console_log = len(re.findall(r'console\.(log|warn|error|debug|info)', content))
        has_debugger = bool(re.search(r'debugger|__DEV__|React\.DevTools', content))

        if has_console_log > 10:
            self.warnings.append(f"[Debugging] {filename}: {has_console_log} console.log statements. Remove before production; they block JS thread.")

        if has_performance:
            self.passed_count += 1  # Good performance monitoring

        # 14.2 Error Boundary Check
        has_error_boundary = bool(re.search(r'ErrorBoundary|componentDidCatch|getDerivedStateFromError', content))
        if not has_error_boundary and is_react_native:
            self.warnings.append(f"[Debugging] {filename}: No ErrorBoundary detected. Consider adding ErrorBoundary to prevent app crashes.")

        # 14.3 Hermes Check (React Native specific)
        if is_react_native:
            # Check if using Hermes engine (should be default in modern RN)
            # This is more of a configuration check, not code pattern
            self.passed_count += 1  # Hermes is default in RN 0.70+

    def audit_directory(self, directory: str) -> None:
        extensions = {'.tsx', '.ts', '.jsx', '.js', '.dart'}
        for root, dirs, files in os.walk(directory):
            dirs[:] = [d for d in dirs if d not in {'node_modules', '.git', 'dist', 'build', '.next', 'ios', 'android', 'build', '.idea'}]
            for file in files:
                if Path(file).suffix in extensions:
                    self.audit_file(os.path.join(root, file))

    def get_report(self):
        return {
            "files_checked": self.files_checked,
            "issues": self.issues,
            "warnings": self.warnings,
            "passed_checks": self.passed_count,
            "compliant": len(self.issues) == 0
        }


def main():
    if len(sys.argv) < 2:
        print("Usage: python mobile_audit.py <directory>")
        sys.exit(1)

    path = sys.argv[1]
    is_json = "--json" in sys.argv

    auditor = MobileAuditor()
    if os.path.isfile(path):
        auditor.audit_file(path)
    else:
        auditor.audit_directory(path)

    report = auditor.get_report()

    if is_json:
        print(json.dumps(report, indent=2))
    else:
        print(f"\n[MOBILE AUDIT] {report['files_checked']} mobile files checked")
        print("-" * 50)
        if report['issues']:
            print(f"[!] ISSUES ({len(report['issues'])}):")
            for i in report['issues'][:10]:
                print(f"  - {i}")
        if report['warnings']:
            print(f"[*] WARNINGS ({len(report['warnings'])}):")
            for w in report['warnings'][:15]:
                print(f"  - {w}")
        print(f"[+] PASSED CHECKS: {report['passed_checks']}")
        status = "PASS" if report['compliant'] else "FAIL"
        print(f"STATUS: {status}")

    sys.exit(0 if report['compliant'] else 1)


if __name__ == "__main__":
    # Fix missing import
    import re
    main()


################################################################################
# PASTA: .agent/skills/performance-profiling/scripts
################################################################################

--- ARQUIVO: .agent/skills/performance-profiling/scripts/lighthouse_audit.py ---
#!/usr/bin/env python3
"""
Skill: performance-profiling
Script: lighthouse_audit.py
Purpose: Run Lighthouse performance audit on a URL
Usage: python lighthouse_audit.py https://example.com
Output: JSON with performance scores
Note: Requires lighthouse CLI (npm install -g lighthouse)
"""
import subprocess
import json
import sys
import os
import tempfile

def run_lighthouse(url: str) -> dict:
    """Run Lighthouse audit on URL."""
    try:
        with tempfile.NamedTemporaryFile(suffix='.json', delete=False) as f:
            output_path = f.name
        
        result = subprocess.run(
            [
                "lighthouse",
                url,
                "--output=json",
                f"--output-path={output_path}",
                "--chrome-flags=--headless",
                "--only-categories=performance,accessibility,best-practices,seo"
            ],
            capture_output=True,
            text=True,
            timeout=120
        )
        
        if os.path.exists(output_path):
            with open(output_path, 'r') as f:
                report = json.load(f)
            os.unlink(output_path)
            
            categories = report.get("categories", {})
            return {
                "url": url,
                "scores": {
                    "performance": int(categories.get("performance", {}).get("score", 0) * 100),
                    "accessibility": int(categories.get("accessibility", {}).get("score", 0) * 100),
                    "best_practices": int(categories.get("best-practices", {}).get("score", 0) * 100),
                    "seo": int(categories.get("seo", {}).get("score", 0) * 100)
                },
                "summary": get_summary(categories)
            }
        else:
            return {"error": "Lighthouse failed to generate report", "stderr": result.stderr[:500]}
            
    except subprocess.TimeoutExpired:
        return {"error": "Lighthouse audit timed out"}
    except FileNotFoundError:
        return {"error": "Lighthouse CLI not found. Install with: npm install -g lighthouse"}

def get_summary(categories: dict) -> str:
    """Generate summary based on scores."""
    perf = categories.get("performance", {}).get("score", 0) * 100
    if perf >= 90:
        return "[OK] Excellent performance"
    elif perf >= 50:
        return "[!] Needs improvement"
    else:
        return "[X] Poor performance"

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print(json.dumps({"error": "Usage: python lighthouse_audit.py <url>"}))
        sys.exit(1)
    
    result = run_lighthouse(sys.argv[1])
    print(json.dumps(result, indent=2))


################################################################################
# PASTA: .agent/skills/seo-fundamentals/scripts
################################################################################

--- ARQUIVO: .agent/skills/seo-fundamentals/scripts/seo_checker.py ---
#!/usr/bin/env python3
"""
SEO Checker - Search Engine Optimization Audit
Checks HTML/JSX/TSX pages for SEO best practices.

PURPOSE:
    - Verify meta tags, titles, descriptions
    - Check Open Graph tags for social sharing
    - Validate heading hierarchy
    - Check image accessibility (alt attributes)

WHAT IT CHECKS:
    - HTML files (actual web pages)
    - JSX/TSX files (React page components)
    - Only files that are likely PUBLIC pages

Usage:
    python seo_checker.py <project_path>
"""
import sys
import json
import re
from pathlib import Path
from datetime import datetime

# Fix Windows console encoding
try:
    sys.stdout.reconfigure(encoding='utf-8', errors='replace')
except:
    pass


# Directories to skip
SKIP_DIRS = {
    'node_modules', '.next', 'dist', 'build', '.git', '.github',
    '__pycache__', '.vscode', '.idea', 'coverage', 'test', 'tests',
    '__tests__', 'spec', 'docs', 'documentation', 'examples'
}

# Files to skip (not pages)
SKIP_PATTERNS = [
    'config', 'setup', 'util', 'helper', 'hook', 'context', 'store',
    'service', 'api', 'lib', 'constant', 'type', 'interface', 'mock',
    '.test.', '.spec.', '_test.', '_spec.'
]


def is_page_file(file_path: Path) -> bool:
    """Check if this file is likely a public-facing page."""
    name = file_path.name.lower()
    stem = file_path.stem.lower()
    
    # Skip utility/config files
    if any(skip in name for skip in SKIP_PATTERNS):
        return False
    
    # Check path - pages in specific directories are likely pages
    parts = [p.lower() for p in file_path.parts]
    page_dirs = ['pages', 'app', 'routes', 'views', 'screens']
    
    if any(d in parts for d in page_dirs):
        return True
    
    # Filename indicators for pages
    page_names = ['page', 'index', 'home', 'about', 'contact', 'blog', 
                  'post', 'article', 'product', 'landing', 'layout']
    
    if any(p in stem for p in page_names):
        return True
    
    # HTML files are usually pages
    if file_path.suffix.lower() in ['.html', '.htm']:
        return True
    
    return False


def find_pages(project_path: Path) -> list:
    """Find page files to check."""
    patterns = ['**/*.html', '**/*.htm', '**/*.jsx', '**/*.tsx']
    
    files = []
    for pattern in patterns:
        for f in project_path.glob(pattern):
            # Skip excluded directories
            if any(skip in f.parts for skip in SKIP_DIRS):
                continue
            
            # Check if it's likely a page
            if is_page_file(f):
                files.append(f)
    
    return files[:50]  # Limit to 50 files


def check_page(file_path: Path) -> dict:
    """Check a single page for SEO issues."""
    issues = []
    
    try:
        content = file_path.read_text(encoding='utf-8', errors='ignore')
    except Exception as e:
        return {"file": str(file_path.name), "issues": [f"Error: {e}"]}
    
    # Detect if this is a layout/template file (has Head component)
    is_layout = 'Head>' in content or '<head' in content.lower()
    
    # 1. Title tag
    has_title = '<title' in content.lower() or 'title=' in content or 'Head>' in content
    if not has_title and is_layout:
        issues.append("Missing <title> tag")
    
    # 2. Meta description
    has_description = 'name="description"' in content.lower() or 'name=\'description\'' in content.lower()
    if not has_description and is_layout:
        issues.append("Missing meta description")
    
    # 3. Open Graph tags
    has_og = 'og:' in content or 'property="og:' in content.lower()
    if not has_og and is_layout:
        issues.append("Missing Open Graph tags")
    
    # 4. Heading hierarchy - multiple H1s
    h1_matches = re.findall(r'<h1[^>]*>', content, re.I)
    if len(h1_matches) > 1:
        issues.append(f"Multiple H1 tags ({len(h1_matches)})")
    
    # 5. Images without alt
    img_pattern = r'<img[^>]+>'
    imgs = re.findall(img_pattern, content, re.I)
    for img in imgs:
        if 'alt=' not in img.lower():
            issues.append("Image missing alt attribute")
            break
        if 'alt=""' in img or "alt=''" in img:
            issues.append("Image has empty alt attribute")
            break
    
    # 6. Check for canonical link (nice to have)
    # has_canonical = 'rel="canonical"' in content.lower()
    
    return {
        "file": str(file_path.name),
        "issues": issues
    }


def main():
    project_path = Path(sys.argv[1] if len(sys.argv) > 1 else ".").resolve()
    
    print(f"\n{'='*60}")
    print(f"  SEO CHECKER - Search Engine Optimization Audit")
    print(f"{'='*60}")
    print(f"Project: {project_path}")
    print(f"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("-"*60)
    
    # Find pages
    pages = find_pages(project_path)
    
    if not pages:
        print("\n[!] No page files found.")
        print("    Looking for: HTML, JSX, TSX in pages/app/routes directories")
        output = {"script": "seo_checker", "files_checked": 0, "passed": True}
        print("\n" + json.dumps(output, indent=2))
        sys.exit(0)
    
    print(f"Found {len(pages)} page files to analyze\n")
    
    # Check each page
    all_issues = []
    for f in pages:
        result = check_page(f)
        if result["issues"]:
            all_issues.append(result)
    
    # Summary
    print("=" * 60)
    print("SEO ANALYSIS RESULTS")
    print("=" * 60)
    
    if all_issues:
        # Group by issue type
        issue_counts = {}
        for item in all_issues:
            for issue in item["issues"]:
                issue_counts[issue] = issue_counts.get(issue, 0) + 1
        
        print("\nIssue Summary:")
        for issue, count in sorted(issue_counts.items(), key=lambda x: -x[1]):
            print(f"  [{count}] {issue}")
        
        print(f"\nAffected files ({len(all_issues)}):")
        for item in all_issues[:5]:
            print(f"  - {item['file']}")
        if len(all_issues) > 5:
            print(f"  ... and {len(all_issues) - 5} more")
    else:
        print("\n[OK] No SEO issues found!")
    
    total_issues = sum(len(item["issues"]) for item in all_issues)
    passed = total_issues == 0
    
    output = {
        "script": "seo_checker",
        "project": str(project_path),
        "files_checked": len(pages),
        "files_with_issues": len(all_issues),
        "issues_found": total_issues,
        "passed": passed
    }
    
    print("\n" + json.dumps(output, indent=2))
    
    sys.exit(0 if passed else 1)


if __name__ == "__main__":
    main()


################################################################################
# PASTA: .agent/skills/testing-patterns/scripts
################################################################################

--- ARQUIVO: .agent/skills/testing-patterns/scripts/test_runner.py ---
#!/usr/bin/env python3
"""
Test Runner - Unified test execution and coverage reporting
Runs tests and generates coverage report based on project type.

Usage:
    python test_runner.py <project_path> [--coverage]

Supports:
    - Node.js: npm test, jest, vitest
    - Python: pytest, unittest
"""

import subprocess
import sys
import json
from pathlib import Path
from datetime import datetime

# Fix Windows console encoding
try:
    sys.stdout.reconfigure(encoding='utf-8', errors='replace')
except:
    pass


def detect_test_framework(project_path: Path) -> dict:
    """Detect test framework and commands."""
    result = {
        "type": "unknown",
        "framework": None,
        "cmd": None,
        "coverage_cmd": None
    }
    
    # Node.js project
    package_json = project_path / "package.json"
    if package_json.exists():
        result["type"] = "node"
        try:
            pkg = json.loads(package_json.read_text(encoding='utf-8'))
            scripts = pkg.get("scripts", {})
            deps = {**pkg.get("dependencies", {}), **pkg.get("devDependencies", {})}
            
            # Check for test script
            if "test" in scripts:
                result["framework"] = "npm test"
                result["cmd"] = ["npm", "test"]
                
                # Try to detect specific framework for coverage
                if "vitest" in deps:
                    result["framework"] = "vitest"
                    result["coverage_cmd"] = ["npx", "vitest", "run", "--coverage"]
                elif "jest" in deps:
                    result["framework"] = "jest"
                    result["coverage_cmd"] = ["npx", "jest", "--coverage"]
            elif "vitest" in deps:
                result["framework"] = "vitest"
                result["cmd"] = ["npx", "vitest", "run"]
                result["coverage_cmd"] = ["npx", "vitest", "run", "--coverage"]
            elif "jest" in deps:
                result["framework"] = "jest"
                result["cmd"] = ["npx", "jest"]
                result["coverage_cmd"] = ["npx", "jest", "--coverage"]
                
        except:
            pass
    
    # Python project
    if (project_path / "pyproject.toml").exists() or (project_path / "requirements.txt").exists():
        result["type"] = "python"
        result["framework"] = "pytest"
        result["cmd"] = ["python", "-m", "pytest", "-v"]
        result["coverage_cmd"] = ["python", "-m", "pytest", "--cov", "--cov-report=term-missing"]
    
    return result


def run_tests(cmd: list, cwd: Path) -> dict:
    """Run tests and return results."""
    result = {
        "passed": False,
        "output": "",
        "error": "",
        "tests_run": 0,
        "tests_passed": 0,
        "tests_failed": 0
    }
    
    try:
        proc = subprocess.run(
            cmd,
            cwd=str(cwd),
            capture_output=True,
            text=True,
            encoding='utf-8',
            errors='replace',
            timeout=300  # 5 min timeout for tests
        )
        
        result["output"] = proc.stdout[:3000] if proc.stdout else ""
        result["error"] = proc.stderr[:500] if proc.stderr else ""
        result["passed"] = proc.returncode == 0
        
        # Try to parse test counts from output
        output = proc.stdout or ""
        
        # Jest/Vitest pattern: "Tests: X passed, Y failed, Z total"
        if "passed" in output.lower() and "failed" in output.lower():
            import re
            match = re.search(r'(\d+)\s+passed', output, re.IGNORECASE)
            if match:
                result["tests_passed"] = int(match.group(1))
            match = re.search(r'(\d+)\s+failed', output, re.IGNORECASE)
            if match:
                result["tests_failed"] = int(match.group(1))
            result["tests_run"] = result["tests_passed"] + result["tests_failed"]
        
        # Pytest pattern: "X passed, Y failed"
        if "pytest" in str(cmd):
            import re
            match = re.search(r'(\d+)\s+passed', output)
            if match:
                result["tests_passed"] = int(match.group(1))
            match = re.search(r'(\d+)\s+failed', output)
            if match:
                result["tests_failed"] = int(match.group(1))
            result["tests_run"] = result["tests_passed"] + result["tests_failed"]
        
    except FileNotFoundError:
        result["error"] = f"Command not found: {cmd[0]}"
    except subprocess.TimeoutExpired:
        result["error"] = "Timeout after 300s"
    except Exception as e:
        result["error"] = str(e)
    
    return result


def main():
    project_path = Path(sys.argv[1] if len(sys.argv) > 1 else ".").resolve()
    with_coverage = "--coverage" in sys.argv
    
    print(f"\n{'='*60}")
    print(f"[TEST RUNNER] Unified Test Execution")
    print(f"{'='*60}")
    print(f"Project: {project_path}")
    print(f"Coverage: {'enabled' if with_coverage else 'disabled'}")
    print(f"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Detect test framework
    test_info = detect_test_framework(project_path)
    print(f"Type: {test_info['type']}")
    print(f"Framework: {test_info['framework']}")
    print("-"*60)
    
    if not test_info["cmd"]:
        print("No test framework found for this project.")
        output = {
            "script": "test_runner",
            "project": str(project_path),
            "type": test_info["type"],
            "framework": None,
            "passed": True,
            "message": "No tests configured"
        }
        print(json.dumps(output, indent=2))
        sys.exit(0)
    
    # Choose command
    cmd = test_info["coverage_cmd"] if with_coverage and test_info["coverage_cmd"] else test_info["cmd"]
    
    print(f"Running: {' '.join(cmd)}")
    print("-"*60)
    
    # Run tests
    result = run_tests(cmd, project_path)
    
    # Print output (truncated)
    if result["output"]:
        lines = result["output"].split("\n")
        for line in lines[:30]:
            print(line)
        if len(lines) > 30:
            print(f"... ({len(lines) - 30} more lines)")
    
    # Summary
    print("\n" + "="*60)
    print("SUMMARY")
    print("="*60)
    
    if result["passed"]:
        print("[PASS] All tests passed")
    else:
        print("[FAIL] Some tests failed")
        if result["error"]:
            print(f"Error: {result['error'][:200]}")
    
    if result["tests_run"] > 0:
        print(f"Tests: {result['tests_run']} total, {result['tests_passed']} passed, {result['tests_failed']} failed")
    
    output = {
        "script": "test_runner",
        "project": str(project_path),
        "type": test_info["type"],
        "framework": test_info["framework"],
        "tests_run": result["tests_run"],
        "tests_passed": result["tests_passed"],
        "tests_failed": result["tests_failed"],
        "passed": result["passed"]
    }
    
    print("\n" + json.dumps(output, indent=2))
    
    sys.exit(0 if result["passed"] else 1)


if __name__ == "__main__":
    main()


################################################################################
# PASTA: .agent/skills/vulnerability-scanner/scripts
################################################################################

--- ARQUIVO: .agent/skills/vulnerability-scanner/scripts/security_scan.py ---
#!/usr/bin/env python3
"""
Skill: vulnerability-scanner
Script: security_scan.py
Purpose: Validate that security principles from SKILL.md are applied correctly
Usage: python security_scan.py <project_path> [--scan-type all|deps|secrets|patterns|config]
Output: JSON with validation findings

This script verifies:
1. Dependencies - Supply chain security (OWASP A03)
2. Secrets - No hardcoded credentials (OWASP A04)
3. Code Patterns - Dangerous patterns identified (OWASP A05)
4. Configuration - Security settings validated (OWASP A02)
"""
import subprocess
import json
import os
import sys
import re
import argparse
from pathlib import Path
from typing import Dict, List, Any
from datetime import datetime

# Fix Windows console encoding for Unicode output
try:
    sys.stdout.reconfigure(encoding='utf-8', errors='replace')
    sys.stderr.reconfigure(encoding='utf-8', errors='replace')
except AttributeError:
    pass  # Python < 3.7


# ============================================================================
#  CONFIGURATION
# ============================================================================

SECRET_PATTERNS = [
    # API Keys & Tokens
    (r'api[_-]?key\s*[=:]\s*["\'][^"\']{10,}["\']', "API Key", "high"),
    (r'token\s*[=:]\s*["\'][^"\']{10,}["\']', "Token", "high"),
    (r'bearer\s+[a-zA-Z0-9\-_.]+', "Bearer Token", "critical"),
    
    # Cloud Credentials
    (r'AKIA[0-9A-Z]{16}', "AWS Access Key", "critical"),
    (r'aws[_-]?secret[_-]?access[_-]?key\s*[=:]\s*["\'][^"\']+["\']', "AWS Secret", "critical"),
    (r'AZURE[_-]?[A-Z_]+\s*[=:]\s*["\'][^"\']+["\']', "Azure Credential", "critical"),
    (r'GOOGLE[_-]?[A-Z_]+\s*[=:]\s*["\'][^"\']+["\']', "GCP Credential", "critical"),
    
    # Database & Connections
    (r'password\s*[=:]\s*["\'][^"\']{4,}["\']', "Password", "high"),
    (r'(mongodb|postgres|mysql|redis):\/\/[^\s"\']+', "Database Connection String", "critical"),
    
    # Private Keys
    (r'-----BEGIN\s+(RSA|PRIVATE|EC)\s+KEY-----', "Private Key", "critical"),
    (r'ssh-rsa\s+[A-Za-z0-9+/]+', "SSH Key", "critical"),
    
    # JWT
    (r'eyJ[A-Za-z0-9-_]+\.eyJ[A-Za-z0-9-_]+\.[A-Za-z0-9-_]+', "JWT Token", "high"),
]

DANGEROUS_PATTERNS = [
    # Injection risks
    (r'eval\s*\(', "eval() usage", "critical", "Code Injection risk"),
    (r'exec\s*\(', "exec() usage", "critical", "Code Injection risk"),
    (r'new\s+Function\s*\(', "Function constructor", "high", "Code Injection risk"),
    (r'child_process\.exec\s*\(', "child_process.exec", "high", "Command Injection risk"),
    (r'subprocess\.call\s*\([^)]*shell\s*=\s*True', "subprocess with shell=True", "high", "Command Injection risk"),
    
    # XSS risks
    (r'dangerouslySetInnerHTML', "dangerouslySetInnerHTML", "high", "XSS risk"),
    (r'\.innerHTML\s*=', "innerHTML assignment", "medium", "XSS risk"),
    (r'document\.write\s*\(', "document.write", "medium", "XSS risk"),
    
    # SQL Injection indicators
    (r'["\'][^"\']*\+\s*[a-zA-Z_]+\s*\+\s*["\'].*(?:SELECT|INSERT|UPDATE|DELETE)', "SQL String Concat", "critical", "SQL Injection risk"),
    (r'f"[^"]*(?:SELECT|INSERT|UPDATE|DELETE)[^"]*\{', "SQL f-string", "critical", "SQL Injection risk"),
    
    # Insecure configurations
    (r'verify\s*=\s*False', "SSL Verify Disabled", "high", "MITM risk"),
    (r'--insecure', "Insecure flag", "medium", "Security disabled"),
    (r'disable[_-]?ssl', "SSL Disabled", "high", "MITM risk"),
    
    # Unsafe deserialization
    (r'pickle\.loads?\s*\(', "pickle usage", "high", "Deserialization risk"),
    (r'yaml\.load\s*\([^)]*\)(?!\s*,\s*Loader)', "Unsafe YAML load", "high", "Deserialization risk"),
]

SKIP_DIRS = {'node_modules', '.git', 'dist', 'build', '__pycache__', '.venv', 'venv', '.next'}
CODE_EXTENSIONS = {'.js', '.ts', '.jsx', '.tsx', '.py', '.go', '.java', '.rb', '.php'}
CONFIG_EXTENSIONS = {'.json', '.yaml', '.yml', '.toml', '.env', '.env.local', '.env.development'}


# ============================================================================
#  SCANNING FUNCTIONS
# ============================================================================

def scan_dependencies(project_path: str) -> Dict[str, Any]:
    """
    Validate supply chain security (OWASP A03).
    Checks: npm audit, lock file presence, dependency age.
    """
    results = {"tool": "dependency_scanner", "findings": [], "status": "[OK] Secure"}
    
    # Check for lock files
    lock_files = {
        "npm": ["package-lock.json", "npm-shrinkwrap.json"],
        "yarn": ["yarn.lock"],
        "pnpm": ["pnpm-lock.yaml"],
        "pip": ["requirements.txt", "Pipfile.lock", "poetry.lock"],
    }
    
    found_locks = []
    missing_locks = []
    
    for manager, files in lock_files.items():
        pkg_file = "package.json" if manager in ["npm", "yarn", "pnpm"] else "setup.py"
        pkg_path = Path(project_path) / pkg_file
        
        if pkg_path.exists() or (manager == "pip" and (Path(project_path) / "requirements.txt").exists()):
            has_lock = any((Path(project_path) / f).exists() for f in files)
            if has_lock:
                found_locks.append(manager)
            else:
                missing_locks.append(manager)
                results["findings"].append({
                    "type": "Missing Lock File",
                    "severity": "high",
                    "message": f"{manager}: No lock file found. Supply chain integrity at risk."
                })
    
    # Run npm audit if applicable
    if (Path(project_path) / "package.json").exists():
        try:
            result = subprocess.run(
                ["npm", "audit", "--json"],
                cwd=project_path,
                capture_output=True,
                text=True,
                timeout=60
            )
            
            try:
                audit_data = json.loads(result.stdout)
                vulnerabilities = audit_data.get("vulnerabilities", {})
                
                severity_count = {"critical": 0, "high": 0, "moderate": 0, "low": 0}
                for vuln in vulnerabilities.values():
                    sev = vuln.get("severity", "low").lower()
                    if sev in severity_count:
                        severity_count[sev] += 1
                
                if severity_count["critical"] > 0:
                    results["status"] = "[!!] Critical vulnerabilities"
                    results["findings"].append({
                        "type": "npm audit",
                        "severity": "critical",
                        "message": f"{severity_count['critical']} critical vulnerabilities in dependencies"
                    })
                elif severity_count["high"] > 0:
                    results["status"] = "[!] High vulnerabilities"
                    results["findings"].append({
                        "type": "npm audit",
                        "severity": "high",
                        "message": f"{severity_count['high']} high severity vulnerabilities"
                    })
                
                results["npm_audit"] = severity_count
                
            except json.JSONDecodeError:
                pass
                
        except (FileNotFoundError, subprocess.TimeoutExpired):
            pass
    
    if not results["findings"]:
        results["status"] = "[OK] Supply chain checks passed"
    
    return results


def scan_secrets(project_path: str) -> Dict[str, Any]:
    """
    Validate no hardcoded secrets (OWASP A04).
    Checks: API keys, tokens, passwords, cloud credentials.
    """
    results = {
        "tool": "secret_scanner",
        "findings": [],
        "status": "[OK] No secrets detected",
        "scanned_files": 0,
        "by_severity": {"critical": 0, "high": 0, "medium": 0}
    }
    
    for root, dirs, files in os.walk(project_path):
        dirs[:] = [d for d in dirs if d not in SKIP_DIRS]
        
        for file in files:
            ext = Path(file).suffix.lower()
            if ext not in CODE_EXTENSIONS and ext not in CONFIG_EXTENSIONS:
                continue
                
            filepath = Path(root) / file
            results["scanned_files"] += 1
            
            try:
                with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                    
                    for pattern, secret_type, severity in SECRET_PATTERNS:
                        matches = re.findall(pattern, content, re.IGNORECASE)
                        if matches:
                            results["findings"].append({
                                "file": str(filepath.relative_to(project_path)),
                                "type": secret_type,
                                "severity": severity,
                                "count": len(matches)
                            })
                            results["by_severity"][severity] += len(matches)
                            
            except Exception:
                pass
    
    if results["by_severity"]["critical"] > 0:
        results["status"] = "[!!] CRITICAL: Secrets exposed!"
    elif results["by_severity"]["high"] > 0:
        results["status"] = "[!] HIGH: Secrets found"
    elif sum(results["by_severity"].values()) > 0:
        results["status"] = "[?] Potential secrets detected"
    
    # Limit findings for output
    results["findings"] = results["findings"][:15]
    
    return results


def scan_code_patterns(project_path: str) -> Dict[str, Any]:
    """
    Validate dangerous code patterns (OWASP A05).
    Checks: Injection risks, XSS, unsafe deserialization.
    """
    results = {
        "tool": "pattern_scanner",
        "findings": [],
        "status": "[OK] No dangerous patterns",
        "scanned_files": 0,
        "by_category": {}
    }
    
    for root, dirs, files in os.walk(project_path):
        dirs[:] = [d for d in dirs if d not in SKIP_DIRS]
        
        for file in files:
            ext = Path(file).suffix.lower()
            if ext not in CODE_EXTENSIONS:
                continue
                
            filepath = Path(root) / file
            results["scanned_files"] += 1
            
            try:
                with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                    lines = f.readlines()
                    
                    for line_num, line in enumerate(lines, 1):
                        for pattern, name, severity, category in DANGEROUS_PATTERNS:
                            if re.search(pattern, line, re.IGNORECASE):
                                results["findings"].append({
                                    "file": str(filepath.relative_to(project_path)),
                                    "line": line_num,
                                    "pattern": name,
                                    "severity": severity,
                                    "category": category,
                                    "snippet": line.strip()[:80]
                                })
                                results["by_category"][category] = results["by_category"].get(category, 0) + 1
                                
            except Exception:
                pass
    
    critical_count = sum(1 for f in results["findings"] if f["severity"] == "critical")
    high_count = sum(1 for f in results["findings"] if f["severity"] == "high")
    
    if critical_count > 0:
        results["status"] = f"[!!] CRITICAL: {critical_count} dangerous patterns"
    elif high_count > 0:
        results["status"] = f"[!] HIGH: {high_count} risky patterns"
    elif results["findings"]:
        results["status"] = "[?] Some patterns need review"
    
    # Limit findings
    results["findings"] = results["findings"][:20]
    
    return results


def scan_configuration(project_path: str) -> Dict[str, Any]:
    """
    Validate security configuration (OWASP A02).
    Checks: Security headers, CORS, debug modes.
    """
    results = {
        "tool": "config_scanner",
        "findings": [],
        "status": "[OK] Configuration secure",
        "checks": {}
    }
    
    # Check common config files for issues
    config_issues = [
        (r'"DEBUG"\s*:\s*true', "Debug mode enabled", "high"),
        (r'debug\s*=\s*True', "Debug mode enabled", "high"),
        (r'NODE_ENV.*development', "Development mode in config", "medium"),
        (r'"CORS_ALLOW_ALL".*true', "CORS allow all origins", "high"),
        (r'"Access-Control-Allow-Origin".*\*', "CORS wildcard", "high"),
        (r'allowCredentials.*true.*origin.*\*', "Dangerous CORS combo", "critical"),
    ]
    
    for root, dirs, files in os.walk(project_path):
        dirs[:] = [d for d in dirs if d not in SKIP_DIRS]
        
        for file in files:
            ext = Path(file).suffix.lower()
            if ext not in CONFIG_EXTENSIONS and file not in ['next.config.js', 'webpack.config.js', '.eslintrc.js']:
                continue
                
            filepath = Path(root) / file
            
            try:
                with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                    
                    for pattern, issue, severity in config_issues:
                        if re.search(pattern, content, re.IGNORECASE):
                            results["findings"].append({
                                "file": str(filepath.relative_to(project_path)),
                                "issue": issue,
                                "severity": severity
                            })
                            
            except Exception:
                pass
    
    # Check for security header configurations
    header_files = ["next.config.js", "next.config.mjs", "middleware.ts", "nginx.conf"]
    for hf in header_files:
        hf_path = Path(project_path) / hf
        if hf_path.exists():
            results["checks"]["security_headers_config"] = True
            break
    else:
        results["checks"]["security_headers_config"] = False
        results["findings"].append({
            "issue": "No security headers configuration found",
            "severity": "medium",
            "recommendation": "Configure CSP, HSTS, X-Frame-Options headers"
        })
    
    if any(f["severity"] == "critical" for f in results["findings"]):
        results["status"] = "[!!] CRITICAL: Configuration issues"
    elif any(f["severity"] == "high" for f in results["findings"]):
        results["status"] = "[!] HIGH: Configuration review needed"
    elif results["findings"]:
        results["status"] = "[?] Minor configuration issues"
    
    return results


# ============================================================================
#  MAIN
# ============================================================================

def run_full_scan(project_path: str, scan_type: str = "all") -> Dict[str, Any]:
    """Execute security validation scans."""
    
    report = {
        "project": project_path,
        "timestamp": datetime.now().isoformat(),
        "scan_type": scan_type,
        "scans": {},
        "summary": {
            "total_findings": 0,
            "critical": 0,
            "high": 0,
            "overall_status": "[OK] SECURE"
        }
    }
    
    scanners = {
        "deps": ("dependencies", scan_dependencies),
        "secrets": ("secrets", scan_secrets),
        "patterns": ("code_patterns", scan_code_patterns),
        "config": ("configuration", scan_configuration),
    }
    
    for key, (name, scanner) in scanners.items():
        if scan_type == "all" or scan_type == key:
            result = scanner(project_path)
            report["scans"][name] = result
            
            findings_count = len(result.get("findings", []))
            report["summary"]["total_findings"] += findings_count
            
            for finding in result.get("findings", []):
                sev = finding.get("severity", "low")
                if sev == "critical":
                    report["summary"]["critical"] += 1
                elif sev == "high":
                    report["summary"]["high"] += 1
    
    # Determine overall status
    if report["summary"]["critical"] > 0:
        report["summary"]["overall_status"] = "[!!] CRITICAL ISSUES FOUND"
    elif report["summary"]["high"] > 0:
        report["summary"]["overall_status"] = "[!] HIGH RISK ISSUES"
    elif report["summary"]["total_findings"] > 0:
        report["summary"]["overall_status"] = "[?] REVIEW RECOMMENDED"
    
    return report


def main():
    parser = argparse.ArgumentParser(
        description="Validate security principles from vulnerability-scanner skill"
    )
    parser.add_argument("project_path", nargs="?", default=".", help="Project directory to scan")
    parser.add_argument("--scan-type", choices=["all", "deps", "secrets", "patterns", "config"],
                        default="all", help="Type of scan to run")
    parser.add_argument("--output", choices=["json", "summary"], default="json",
                        help="Output format")
    
    args = parser.parse_args()
    
    if not os.path.isdir(args.project_path):
        print(json.dumps({"error": f"Directory not found: {args.project_path}"}))
        sys.exit(1)
    
    result = run_full_scan(args.project_path, args.scan_type)
    
    if args.output == "summary":
        print(f"\n{'='*60}")
        print(f"Security Scan: {result['project']}")
        print(f"{'='*60}")
        print(f"Status: {result['summary']['overall_status']}")
        print(f"Total Findings: {result['summary']['total_findings']}")
        print(f"  Critical: {result['summary']['critical']}")
        print(f"  High: {result['summary']['high']}")
        print(f"{'='*60}\n")
        
        for scan_name, scan_result in result['scans'].items():
            print(f"\n{scan_name.upper()}: {scan_result['status']}")
            for finding in scan_result.get('findings', [])[:5]:
                print(f"  - {finding}")
    else:
        print(json.dumps(result, indent=2))


if __name__ == "__main__":
    main()


################################################################################
# PASTA: .agent/skills/webapp-testing/scripts
################################################################################

--- ARQUIVO: .agent/skills/webapp-testing/scripts/playwright_runner.py ---
#!/usr/bin/env python3
"""
Skill: webapp-testing
Script: playwright_runner.py
Purpose: Run basic Playwright browser tests
Usage: python playwright_runner.py <url> [--screenshot]
Output: JSON with page info, health status, and optional screenshot path
Note: Requires playwright (pip install playwright && playwright install chromium)
Screenshots: Saved to system temp directory (auto-cleaned by OS)
"""
import sys
import json
import os
import tempfile
from datetime import datetime

# Fix Windows console encoding for Unicode output
try:
    sys.stdout.reconfigure(encoding='utf-8', errors='replace')
    sys.stderr.reconfigure(encoding='utf-8', errors='replace')
except AttributeError:
    pass  # Python < 3.7

try:
    from playwright.sync_api import sync_playwright
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False


def run_basic_test(url: str, take_screenshot: bool = False) -> dict:
    """Run basic browser test on URL."""
    if not PLAYWRIGHT_AVAILABLE:
        return {
            "error": "Playwright not installed",
            "fix": "pip install playwright && playwright install chromium"
        }
    
    result = {
        "url": url,
        "timestamp": datetime.now().isoformat(),
        "status": "pending"
    }
    
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            context = browser.new_context(
                viewport={"width": 1280, "height": 720},
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            )
            page = context.new_page()
            
            # Navigate
            response = page.goto(url, wait_until="networkidle", timeout=30000)
            
            # Basic info
            result["page"] = {
                "title": page.title(),
                "url": page.url,
                "status_code": response.status if response else None
            }
            
            # Health checks
            result["health"] = {
                "loaded": response.ok if response else False,
                "has_title": bool(page.title()),
                "has_h1": page.locator("h1").count() > 0,
                "has_links": page.locator("a").count() > 0,
                "has_images": page.locator("img").count() > 0
            }
            
            # Console errors
            console_errors = []
            page.on("console", lambda msg: console_errors.append(msg.text) if msg.type == "error" else None)
            
            # Performance metrics
            result["performance"] = {
                "dom_content_loaded": page.evaluate("window.performance.timing.domContentLoadedEventEnd - window.performance.timing.navigationStart"),
                "load_complete": page.evaluate("window.performance.timing.loadEventEnd - window.performance.timing.navigationStart")
            }
            
            # Screenshot - uses system temp directory (cross-platform, auto-cleaned)
            if take_screenshot:
                # Cross-platform: Windows=%TEMP%, Linux/macOS=/tmp
                screenshot_dir = os.path.join(tempfile.gettempdir(), "maestro_screenshots")
                os.makedirs(screenshot_dir, exist_ok=True)
                screenshot_path = os.path.join(screenshot_dir, f"screenshot_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png")
                page.screenshot(path=screenshot_path, full_page=True)
                result["screenshot"] = screenshot_path
                result["screenshot_note"] = "Saved to temp directory (auto-cleaned by OS)"
            
            # Element counts
            result["elements"] = {
                "links": page.locator("a").count(),
                "buttons": page.locator("button").count(),
                "inputs": page.locator("input").count(),
                "images": page.locator("img").count(),
                "forms": page.locator("form").count()
            }
            
            browser.close()
            
            result["status"] = "success" if result["health"]["loaded"] else "failed"
            result["summary"] = "[OK] Page loaded successfully" if result["status"] == "success" else "[X] Page failed to load"
            
    except Exception as e:
        result["status"] = "error"
        result["error"] = str(e)
        result["summary"] = f"[X] Error: {str(e)[:100]}"
    
    return result


def run_accessibility_check(url: str) -> dict:
    """Run basic accessibility check."""
    if not PLAYWRIGHT_AVAILABLE:
        return {"error": "Playwright not installed"}
    
    result = {"url": url, "accessibility": {}}
    
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page()
            page.goto(url, wait_until="networkidle", timeout=30000)
            
            # Basic a11y checks
            result["accessibility"] = {
                "images_with_alt": page.locator("img[alt]").count(),
                "images_without_alt": page.locator("img:not([alt])").count(),
                "buttons_with_label": page.locator("button[aria-label], button:has-text('')").count(),
                "links_with_text": page.locator("a:has-text('')").count(),
                "form_labels": page.locator("label").count(),
                "headings": {
                    "h1": page.locator("h1").count(),
                    "h2": page.locator("h2").count(),
                    "h3": page.locator("h3").count()
                }
            }
            
            browser.close()
            result["status"] = "success"
            
    except Exception as e:
        result["status"] = "error"
        result["error"] = str(e)
    
    return result


if __name__ == "__main__":
    if len(sys.argv) < 2:
        print(json.dumps({
            "error": "Usage: python playwright_runner.py <url> [--screenshot] [--a11y]",
            "examples": [
                "python playwright_runner.py https://example.com",
                "python playwright_runner.py https://example.com --screenshot",
                "python playwright_runner.py https://example.com --a11y"
            ]
        }, indent=2))
        sys.exit(1)
    
    url = sys.argv[1]
    take_screenshot = "--screenshot" in sys.argv
    check_a11y = "--a11y" in sys.argv
    
    if check_a11y:
        result = run_accessibility_check(url)
    else:
        result = run_basic_test(url, take_screenshot)
    
    print(json.dumps(result, indent=2))


################################################################################
# PASTA: backend
################################################################################

--- ARQUIVO: backend/app.py ---
import os
import sys
import socket
import threading
import webview
from flask import Flask, jsonify
from flask_cors import CORS
from api.routes import api_blueprint

# --- CONFIGURATION ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
STATIC_DIR = os.path.join(BASE_DIR, "static")

def get_free_port():
    """Find a free port on localhost (strictly)."""
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.bind(('127.0.0.1', 0))
        _, port = s.getsockname()
        return port


def create_app():
    """Initialize Flask App."""
    app = Flask(__name__, static_folder=STATIC_DIR, static_url_path='/')
    CORS(app) # Relaxed for dev, strict in prod if needed

    # Register Blueprints
    app.register_blueprint(api_blueprint, url_prefix='/api')

    @app.route('/')
    def index():
        return app.send_static_file('index.html')

    return app




def start_server(port, app):
    """Run Flask server."""
    app.run(host='127.0.0.1', port=port, threaded=True)

if __name__ == '__main__':
    port = get_free_port()
    app = create_app()
    
    # Start Flask in a separate thread
    t = threading.Thread(target=start_server, args=(port, app))
    t.daemon = True
    t.start()

    # Launch PyWebView
    # "Interface Only" - clean, beautiful window
    webview.create_window(
        'ProSolve Professional', 
        f'http://127.0.0.1:{port}',
        width=1280, 
        height=800,
        resizable=True,
        min_size=(1024, 600),
        background_color='#0f172a' # Match Slate-950 theme
    )
    
    webview.start(debug=True)


################################################################################
# PASTA: backend/api
################################################################################

--- ARQUIVO: backend/api/routes.py ---
import os
import sys
import subprocess
import json
from flask import Blueprint, jsonify, request, current_app
import webview
from jinja2 import Environment, FileSystemLoader
import threading

from services.vtk_converter import call_med_extractor

api_blueprint = Blueprint('api', __name__)

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

@api_blueprint.route('/health', methods=['GET'])
def health():
    return jsonify({"status": "ok", "message": "ProSolve Professional API"})

@api_blueprint.route('/verification', methods=['GET'])
def get_verification_data():
    """
    Reads mass properties and reaction forces from CSV files generated by Code_Aster.
    """
    try:
        project_path = request.args.get('project_path')
        if not project_path:
            return jsonify({"status": "error", "message": "No project path provided"}), 400

        sim_dir = os.path.join(project_path, "simulation_files")
        mass_file = os.path.join(sim_dir, "mass_properties.csv")
        reac_file = os.path.join(sim_dir, "reactions.csv")

        # 1. Parse Mass Properties
        mass_data = None
        if os.path.exists(mass_file):
            try:
                # Code_Aster CSV format is specific: Skip header comments (#)
                # find the header line starting with LIEU,ENTITE,MASSE...
                with open(mass_file, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                
                header_idx = -1
                for i, line in enumerate(lines):
                    if line.strip().startswith("LIEU"):
                        header_idx = i
                        break
                
                if header_idx != -1 and header_idx + 1 < len(lines):
                    # Data is usually on the next line
                    data_line = lines[header_idx + 1]
                    parts = [p.strip() for p in data_line.split(',')]
                    # Filter out empty strings result of trailing commas
                    vals = [x for x in parts if x]
                    
                    if len(vals) >= 15: # Ensure we have enough columns
                        # Mapping based on standard Code_Aster header
                        # LIEU, ENTITE, MASSE, CDG_X, CDG_Y, CDG_Z, IX_G, IY_G, IZ_G...
                        mass_data = {
                            "mass": float(vals[2]),
                            "cdg_x": float(vals[3]),
                            "cdg_y": float(vals[4]),
                            "cdg_z": float(vals[5]),
                            "ix_g": float(vals[6]),
                            "iy_g": float(vals[7]),
                            "iz_g": float(vals[8])
                        }
            except Exception as e:
                print(f"Error parsing mass CSV: {e}")

        # 2. Parse Reactions
        reactions_data = []
        if os.path.exists(reac_file):
            try:
                with open(reac_file, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                
                # REAC_NODA format often has multiple tables in one file or multiple files
                # Simplified parser: look for data lines after headers
                # We expect headers like: NOEUD, DX, DY, DZ, DRX, DRY, DRZ...
                # BUT IMPR_TABLE output for RESULTANTE is simpler: Parameter names and values
                
                # Strategy: Detect case name from TITRE comments
                current_case = "Unknown"
                for line in lines:
                    line = line.strip()
                    
                    # Code_Aster IMPR_TABLE with TITRE prints: # TITRE : REACTIONS_Case 1
                    if "TITRE" in line and "REACTIONS_" in line:
                        current_case = line.split("REACTIONS_")[-1].strip()
                        continue
                        
                    if not line or line.startswith('#') or "DX" in line or "NOEUD" in line: # Skip header/comments
                        continue
                        
                    parts = [p.strip() for p in line.split(',')]
                    vals = [v for v in parts if v]
                    
                    # Heuristic: line with numbers corresponding to forces
                    if len(vals) >= 6: 
                        try:
                            # Try to parse at least 6 floats from the end
                            floats = [float(v) for v in vals[-6:]]
                            reactions_data.append({
                                "case_name": current_case,
                                "fx": floats[0],
                                "fy": floats[1],
                                "fz": floats[2],
                                "mx": floats[3],
                                "my": floats[4],
                                "mz": floats[5]
                            })
                        except:
                            continue

            except Exception as e:
                print(f"Error parsing reaction CSV: {e}")

        return jsonify({
            "status": "success",
            "mass_properties": mass_data,
            "reactions": reactions_data
        })

    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500

@api_blueprint.route('/open_folder_dialog', methods=['GET'])
def open_folder_dialog():
    """Opens native Windows Folder Picker using PyWebView."""
    try:
        if len(webview.windows) > 0:
            window = webview.windows[0]
            folder_path = window.create_file_dialog(webview.FOLDER_DIALOG)
            
            if folder_path and len(folder_path) > 0:
                return jsonify({"status": "success", "path": folder_path[0]})
        return jsonify({"status": "cancelled"})
    except Exception as e:
        print(f"Dialog Error: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500

@api_blueprint.route('/scan_workspace', methods=['POST'])
def scan_workspace():
    """Scans a folder for geometry and mesh files."""
    data = request.get_json()
    folder_path = data.get('folder_path')
    
    if not folder_path or not os.path.exists(folder_path):
        return jsonify({"status": "error", "message": "Invalid Path"}), 400

    try:
        files = os.listdir(folder_path)
        geo_files = [f for f in files if f.lower().endswith(('.step', '.stp'))]
        mesh_files = [f for f in files if f.lower().endswith('.med') and f.lower() != 'resu.med']
        config_files = [f for f in files if f.lower().endswith('.comm')]
        
        result = {
            "status": "success",
            "geometry": len(geo_files) > 0,
            "mesh": len(mesh_files) > 0,
            "config": len(config_files) > 0,
            "files": {
                "geometry": geo_files,
                "mesh": mesh_files,
                "config": config_files,
            }
        }
        
        # Professional Refactor: Don't trigger Aster init automatically (too slow)
        # The groups will be read on-the-fly via MEDCOUPLING when the component mounts.
        # if mesh_files:
        #     init_result = init_aster_files(folder_path, mesh_files)
        #     result["aster_init"] = init_result
        
        return jsonify(result)
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500

def init_aster_files(folder_path, mesh_files):
    """
    Initialize Code_Aster files:
    1. Create simulation_files and temp directories
    2. Generate mesh.json
    3. Generate export.export
    4. Call inspect_mesh.py
    """
    try:
        sim_files_dir = os.path.join(folder_path, "simulation_files")
        temp_working_dir = os.path.join(sim_files_dir, "temp")
        
        # Create directories
        os.makedirs(sim_files_dir, exist_ok=True)
        os.makedirs(temp_working_dir, exist_ok=True)
        
        # Generate mesh.json
        mesh_data_list = []
        for i, med_file in enumerate(mesh_files):
            name = os.path.splitext(med_file)[0].replace("-", "_").replace(" ", "_")
            mesh_data_list.append({
                "name": name,
                "filename": med_file,
                "format": "MED"
            })
        
        mesh_json_path = os.path.join(sim_files_dir, "mesh.json")
        with open(mesh_json_path, 'w', encoding='utf-8') as f:
            json.dump({"unit_start": 80, "meshes": mesh_data_list}, f, indent=4)
        
        # Generate export.export using Jinja2
        jinja_dir = os.path.join(BASE_DIR, "services", "jinja", "templates")
        env = Environment(loader=FileSystemLoader(jinja_dir), trim_blocks=True, lstrip_blocks=True)
        tpl_export = env.get_template("export.j2")
        
        export_meshes = []
        for i, m in enumerate(mesh_data_list):
            export_meshes.append({
                "path": os.path.abspath(os.path.join(folder_path, m["filename"])),
                "unit": 80 + i
            })
        
        export_content = tpl_export.render(
            temp_path=os.path.abspath(temp_working_dir),
            comm_path=os.path.abspath(os.path.join(sim_files_dir, "med.comm")),
            meshes=export_meshes,
            message_path=os.path.abspath(os.path.join(sim_files_dir, "message")),
            base_path=os.path.abspath(os.path.join(sim_files_dir, "base")),
            csv_path=None
        )
        
        export_path = os.path.join(sim_files_dir, "export.export")
        with open(export_path, "w", encoding="utf-8") as f:
            f.write(export_content)
        
        # Call inspect_mesh.py (Generates med.comm)
        script_path = os.path.join(BASE_DIR, "services", "jinja", "inspect_mesh.py")
        if os.path.exists(script_path):
            result = subprocess.run(
                [sys.executable, script_path, folder_path],
                capture_output=True,
                text=True,
                encoding='utf-8',
                errors='replace'
            )
            if result.returncode != 0:
                print(f"[ASTER] inspect_mesh.py failed: {result.stderr}")
                return False
                
        # EXECUTE CODE_ASTER (as_run)
        # 1. READ CONFIG
        config = get_prosolve_config()
        run_aster_bin = config.get("ASTER_BIN")
        
        # Fallbacks
        if not run_aster_bin or not os.path.exists(run_aster_bin):
            # Fallback 1: Local Jinja bat
            local_bat = os.path.join(BASE_DIR, "services", "jinja", "run_aster.bat")
            if os.path.exists(local_bat):
                run_aster_bin = local_bat
                print(f"[ASTER] Configured path not found. Using local fallback: {local_bat}")
            else:
                print(f"[ASTER] CRITICAL: Code_Aster executable not found in config or fallback.")
                return True # We return true to not block UI load, but scan incomplete
        
        print(f"[ASTER] Executing Inspection via: {run_aster_bin}")
        print(f"[ASTER] Target Export: {export_path}")

        exec_result = subprocess.run(
            [run_aster_bin, export_path],
            capture_output=True,
            text=True,
            encoding='utf-8',
            errors='ignore'
        )
        
        # DUMP OUTPUT TO TERMINAL
        print(f"--- ASTER STDOUT ---\n{exec_result.stdout}\n--------------------")
        if exec_result.stderr:
            print(f"--- ASTER STDERR ---\n{exec_result.stderr}\n--------------------")
            
        if exec_result.returncode != 0:
            print(f"[ASTER] Execution failed (Code {exec_result.returncode})")
        else:
             print(f"[ASTER] Inspection Execution Complete.")
        
        return True
    except Exception as e:
        print(f"[ASTER] Init error: {e}")
        return False

@api_blueprint.route('/read_mesh_groups', methods=['POST'])
def read_mesh_groups():
    """Reads mesh groups directly from MED files using MEDCOUPLING extractor."""
    try:
        from services.vtk_converter import call_med_extractor
        
        data = request.get_json()
        folder_path = data.get('folder_path')
        
        if not folder_path or not os.path.exists(folder_path):
            return jsonify({"status": "error", "message": "Path not provided or invalid"}), 400
        
        # Find all .med files
        files = [f for f in os.listdir(folder_path) if f.lower().endswith('.med') and 'resu' not in f.lower()]
        
        if not files:
            return jsonify({"status": "error", "message": "No .med files found"}), 404
            
        combined_groups = {}
        
        for mesh_file in files:
            target_path = os.path.join(folder_path, mesh_file)
            print(f"[API] Extracting groups from: {mesh_file}")
            
            # Use our professional extractor
            result = call_med_extractor(target_path)
            
            if result and result.get("status") == "success":
                # Merge into a format matches mesh_groups.json structure
                # { "groups": { "NAME": { "count": X, "types": { "T1": Y } } } }
                for g_name, g_data in result.get("cells", {}).items():
                    # If multiple files have the same group, they "merge" or we prefix?
                    # The frontend usually expects groups to be unique mapped to geometry.
                    # We'll use the group name directly to support multi-mesh assembly.
                    combined_groups[g_name] = {
                        "count": g_data.get("count", 0),
                        "types": g_data.get("types", {}),
                        "type": g_data.get("type", "unknown"), # Pass the general VTK type (line, quad, ... or node)
                        "source": mesh_file # Metadata
                    }
            else:
                print(f"[API] Failed to extract from {mesh_file}")

        return jsonify({
            "status": "success", 
            "data": {
                "source_mesh": "MEDCOUPLING_Professional_Extractor",
                "groups": combined_groups
            }
        })
    except Exception as e:
        import traceback
        traceback.print_exc()
        print(f"[API] Error reading groups: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500

def get_prosolve_config():
    """Helper to read config.txt"""
    config_file = os.path.join(BASE_DIR, "..", "prosolve", "config.txt")
    config = {}
    if os.path.exists(config_file):
        with open(config_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if "=" in line and not line.startswith("#"):
                    key, val = line.split("=", 1)
                    config[key.strip()] = val.strip()
    return config

@api_blueprint.route('/get_settings', methods=['GET'])
def get_settings():
    """Returns content of config.txt"""
    try:
        config = get_prosolve_config()
        return jsonify({
            "status": "success",
            "settings": {
                "aster_path": config.get("ASTER_BIN", ""),
                "freecad_path": config.get("FREECAD_BIN", ""),
                "salome_path": config.get("SALOME_BIN", "")
            }
        })
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500

@api_blueprint.route('/save_settings', methods=['POST'])
def save_settings():
    """Updates config.txt"""
    try:
        data = request.get_json()
        aster = data.get('aster_path', '')
        freecad = data.get('freecad_path', '')
        salome = data.get('salome_path', '')
        
        config_file = os.path.join(BASE_DIR, "..", "prosolve", "config.txt")
        
        # We rewrite the file preserving comments? Hard with simple parsing.
        # Simple approach: Write new file with standard header
        with open(config_file, 'w', encoding='utf-8') as f:
            f.write("# Configurações de Caminhos do ProSolve\n")
            f.write("# Gerado Automaticamente\n\n")
            f.write(f"ASTER_BIN={aster}\n")
            f.write(f"FREECAD_BIN={freecad}\n")
            f.write(f"SALOME_BIN={salome}\n")
            
        return jsonify({"status": "success", "message": "Settings saved"})
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500

@api_blueprint.route('/launch_tool', methods=['POST'])
def launch_tool():
    """Launch external tools (FreeCAD, Salome) from config."""
    data = request.get_json()
    tool_name = data.get('tool_name')
    
    if not tool_name:
        return jsonify({"status": "error", "message": "Tool name required"}), 400
    
    config = get_prosolve_config()
    bin_path = None
    
    if tool_name.lower() == 'salome':
        bin_path = config.get("SALOME_BIN")
    elif tool_name.lower() == 'freecad':
        bin_path = config.get("FREECAD_BIN")
    elif tool_name.lower() == 'aster':
        bin_path = config.get("ASTER_BIN")
        
    if not bin_path or not os.path.exists(bin_path):
        return jsonify({"status": "error", "message": f"{tool_name} path not configured or not found: {bin_path}"}), 404
    
    try:
        subprocess.Popen([bin_path], shell=True if tool_name.lower() == 'salome' else False)
        return jsonify({"status": "success", "message": f"{tool_name} launched"})
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500

@api_blueprint.route('/calculate_section', methods=['POST'])
def calculate_section():
    """Calculate section properties using an external extractor and cache."""
    try:
        import hashlib
        import subprocess
        
        data = request.get_json()
        section_type = data.get('type')
        params = data.get('params', {})
        
        if not section_type:
            return jsonify({"status": "error", "message": "Section type required"}), 400
            
        # 1. CACHE MANAGEMENT
        param_str = json.dumps({"type": section_type, "params": params}, sort_keys=True)
        param_hash = hashlib.sha256(param_str.encode()).hexdigest()
        
        cache_dir = os.path.join(BASE_DIR, ".cache", "sections")
        os.makedirs(cache_dir, exist_ok=True)
        cache_file = os.path.join(cache_dir, f"{param_hash}.json")
        
        if os.path.exists(cache_file):
            with open(cache_file, "r", encoding="utf-8") as f:
                return jsonify(json.load(f))
        
        # 2. DYNAMIC EXTRACTION
        extractor_path = os.path.join(BASE_DIR, "services", "section_extractor.py")
        
        result = subprocess.run(
            [sys.executable, extractor_path],
            input=json.dumps(data),
            capture_output=True,
            text=True,
            encoding='utf-8'
        )
        
        if result.returncode != 0:
            return jsonify({"status": "error", "message": f"Extractor failed: {result.stderr}"}), 500
            
        try:
            output_json = json.loads(result.stdout)
        except:
            return jsonify({"status": "error", "message": f"Invalid extractor output: {result.stdout}"}), 500
            
        # 3. SAVE TO CACHE
        if output_json.get("status") == "success":
            with open(cache_file, "w", encoding="utf-8") as f:
                json.dump(output_json, f)
                
        return jsonify(output_json)
        
    except Exception as e:
        print(f"[API] Section calc error: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500

@api_blueprint.route('/save_project', methods=['POST'])
def save_project():
    """
    Saves the full project configuration to project.json.
    Also separates data into specific JSONs for the Jinja generation pipeline
    and runs generate_comm.py.
    """
    try:
        data = request.get_json()
        folder_path = data.get('folder_path')
        project_config = data.get('config')
        
        if not folder_path or not project_config:
            return jsonify({"status": "error", "message": "Missing path or config"}), 400
            
        study_dir = os.path.join(os.path.dirname(BASE_DIR), "backend", "services", "jinja", "study")
        project_file = os.path.join(folder_path, "project.json")
        
        # 1. Save master project.json
        with open(project_file, 'w', encoding='utf-8') as f:
            json.dump(project_config, f, indent=4)
            
        # 2. Simplification: No longer need to decompose into multiple JSONs
        # unless builders are updated to read main json (already done)
        # We only need project.json to exist in the folder_path root
        
        # 3. Run generate_comm.py
        script_path = os.path.join(os.path.dirname(BASE_DIR), "backend", "services", "jinja", "generate_comm.py")
        print(f"[SAVE] Project Path: {folder_path}")
        print(f"[SAVE] Script Path: {script_path}")
        
        result = subprocess.run(
            [sys.executable, script_path, "--project_path", folder_path],
            capture_output=True,
            text=True, 
            encoding='utf-8',
            errors='replace'
        )
        
        if result.stdout: print(f"--- GENERATOR STDOUT ---\n{result.stdout}")
        if result.stderr: print(f"--- GENERATOR STDERR ---\n{result.stderr}")
        
        if result.returncode != 0:
            return jsonify({"status": "warning", "message": f"Saved, but generation failed: {result.stderr}"})
            
        sim_dir = os.path.join(folder_path, "simulation_files")
        os.makedirs(sim_dir, exist_ok=True)
        dst_comm = os.path.abspath(os.path.join(sim_dir, "calcul.comm"))
            
        # 4. GENERATE EXPORT.EXPORT for SIMULATION
        jinja_dir = os.path.join(BASE_DIR, "services", "jinja", "templates")
        env = Environment(loader=FileSystemLoader(jinja_dir), trim_blocks=True, lstrip_blocks=True)
        tpl_export = env.get_template("export.j2")
        
        # Prepare mesh list for export from unified config
        mesh_data_list = project_config.get("meshes", [])
        
        export_mesh_objs = []
        for i, m in enumerate(mesh_data_list):
            # med file is strictly the filename in the same folder usually
            full_path = os.path.abspath(os.path.join(folder_path, m["filename"]))
            export_mesh_objs.append({
                "path": full_path,
                "unit": 80 + i 
            })
            
        temp_working_dir = os.path.join(sim_dir, "temp")
        os.makedirs(temp_working_dir, exist_ok=True)
        
        export_content = tpl_export.render(
            temp_path=os.path.abspath(temp_working_dir),
            comm_path=dst_comm, # Points to calcul.comm
            meshes=export_mesh_objs,
            message_path=os.path.abspath(os.path.join(sim_dir, "message")),
            base_path=os.path.abspath(os.path.join(sim_dir, "base")),
            resu_med_path=os.path.abspath(os.path.join(sim_dir, "resu.med")),
            mass_csv_path=os.path.abspath(os.path.join(sim_dir, "mass_properties.csv")),
            reactions_csv_path=os.path.abspath(os.path.join(sim_dir, "reactions.csv"))
        )
        export_file = os.path.join(sim_dir, "export.export")
        with open(export_file, "w", encoding="utf-8") as f:
            f.write(export_content)
            
        return jsonify({"status": "success", "message": "Project saved, generated, and ready for simulation"})
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        return jsonify({"status": "error", "message": str(e)}), 500

@api_blueprint.route('/run_simulation', methods=['POST'])
def run_simulation():
    """Executes the simulation using export.export."""
    try:
        data = request.get_json()
        folder_path = data.get('folder_path')
        
        if not folder_path:
             return jsonify({"status": "error", "message": "Path required"}), 400
             
        sim_dir = os.path.join(folder_path, "simulation_files")
        export_path = os.path.join(sim_dir, "export.export")
        
        if not os.path.exists(export_path):
            return jsonify({"status": "error", "message": "Export file not found. Save project first."}), 404
            
        # Get Aster Bin
        config = get_prosolve_config()
        aster_bin = config.get("ASTER_BIN")
        if not aster_bin:
             return jsonify({"status": "error", "message": "Code_Aster path not configured in Settings."}), 400
             
        cmd = [aster_bin, export_path]
        print(f"[SIMULATION] Starting: {cmd}")
        
        # Determine cwd (project folder or simulation_files?)
        # Usually where export is or where we want output
        
        # Launch in a NEW CONSOLE and use cmd /k to keep window open after finish
        cmd_open = ["cmd", "/k"] + cmd
        print(f"[SIM] Launching Aster: {cmd_open}")
        
        subprocess.Popen(
            cmd_open,
            cwd=sim_dir,
            creationflags=subprocess.CREATE_NEW_CONSOLE
        )
        
        return jsonify({
            "status": "success", 
            "message": "Simulation started. Console window will remain open for inspection."
        })
            
    except Exception as e:
        print(f"[SIMULATION] Error: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500




@api_blueprint.route('/open_project', methods=['POST'])
def open_project_config():
    """Reads project.json and returns it."""
    try:
        data = request.get_json()
        folder_path = data.get('folder_path')
        
        if not folder_path:
             return jsonify({"status": "error", "message": "Path required"}), 400
             
        project_file = os.path.join(folder_path, "project.json")
        if not os.path.exists(project_file):
            return jsonify({"status": "not_found", "message": "Project config not found"})
            
        with open(project_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
            
        return jsonify({"status": "success", "config": config})
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500


@api_blueprint.route('/get_mesh_data', methods=['POST'])
def get_mesh_data():
    """Reads mesh data for visualization."""
    try:
        from services.mesh_reader import read_mesh_file
        
        data = request.get_json()
        folder_path = data.get('folder_path')
        mesh_filename = data.get('mesh_filename') # Optional, else pick first
        
        if not folder_path:
             return jsonify({"status": "error", "message": "Path required"}), 400
             
        # Find mesh file
        target_path = ""
        if mesh_filename:
            target_path = os.path.join(folder_path, mesh_filename)
        else:
            # Auto find
            files = [f for f in os.listdir(folder_path) if f.lower().endswith('.med') and 'resu' not in f.lower()]
            if not files:
                return jsonify({"status": "error", "message": "No .med file found"}), 404
            target_path = os.path.join(folder_path, files[0])
            
        result = read_mesh_file(target_path)
        return jsonify(result)
        
    except Exception as e:
        print(f"[API] Mesh data error: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500




@api_blueprint.route('/get_mesh_vtk', methods=['POST'])
def get_mesh_vtk():
    """Reads ALL mesh files in VTK format for visualization."""
    try:
        from services.vtk_converter import med_to_vtk_json
        
        data = request.get_json()
        folder_path = data.get('folder_path')
        geometries = data.get('geometries', [])
        
        if not folder_path:
            return jsonify({"status": "error", "message": "Path required"}), 400
            
        # Find ALL mesh files
        files = [f for f in os.listdir(folder_path) if f.lower().endswith('.med') and 'resu' not in f.lower()]
        if not files:
            return jsonify({"status": "error", "message": "No .med file found"}), 404
        
        print(f"[API] Found {len(files)} mesh files: {files}")
        
        # Combine all meshes
        combined_points = []
        combined_cells = {}
        point_offset = 0
        
        for mesh_file in files:
            target_path = os.path.join(folder_path, mesh_file)
            print(f"[API] Processing: {mesh_file}")
            
            result = med_to_vtk_json(target_path, geometries=geometries)
            
            if result["status"] != "success":
                print(f"[API] Error loading {mesh_file}: {result.get('message')}")
                continue
            
            # Add points with offset
            combined_points.extend(result["points"])
            
            # Add cells with adjusted indices
            for group_name, group_data in result["cells"].items():
                # Prefix group name with file name to avoid conflicts
                prefixed_name = f"{mesh_file.replace('.med', '')}_{group_name}"
                
                # Adjust cell connectivity indices
                adjusted_connectivity = []
                for cell in group_data["connectivity"]:
                    adjusted_cell = [idx + point_offset for idx in cell]
                    adjusted_connectivity.append(adjusted_cell)
                
                combined_cells[prefixed_name] = {
                    "type": group_data["type"],
                    "connectivity": adjusted_connectivity
                }
            
            point_offset += len(result["points"])
        
        print(f"[API] Combined mesh: {len(combined_points)} points, {len(combined_cells)} groups")
        
        return jsonify({
            "status": "success",
            "points": combined_points,
            "cells": combined_cells,
            "num_points": len(combined_points),
            "num_groups": len(combined_cells)
        })
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        print(f"[API] VTK mesh error: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500




@api_blueprint.route('/get_hq_assembly', methods=['POST'])
def get_hq_assembly():
    """
    TRIGGER: Direct transcription of standalone success logic.
    Calls med_mesher.py via SALOME env for ALL project meshes.
    """
    try:
        data = request.get_json()
        folder_path = data.get('folder_path')
        
        if not folder_path or not os.path.exists(folder_path):
            return jsonify({"status": "error", "message": "Invalid project path"}), 400
            
        # 1. FIND ALL MESHES
        med_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.med') and 'resu' not in f.lower()]
        
        # 2. RUN MED_MESHER FOR EACH (HQ Extraction)
        # Paths for environment setup
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        med_env_dir = os.path.join(os.path.dirname(project_root), "MEDCOUPLING-9.15.0", "MEDCOUPLING-9.15.0")
        mesher_script = os.path.join(project_root, "services", "med", "med_mesher.py")
        
        for med_file in med_files:
            source_path = os.path.join(folder_path, med_file)
            # cmd /c "cd /d ... && call ... && python ... ..."
            cmd = f'cmd /c "cd /d {med_env_dir} && call env_launch.bat && python \"{mesher_script}\" \"{source_path}\""'
            print(f"[HQ-TRIGGER] Extracting: {med_file}...")
            subprocess.run(cmd, shell=True, capture_output=True)
            
        # 3. AGGREGATE RESULTS
        json_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.json') and f != 'project.json']
        
        combined_points = []
        combined_cells = {}
        point_offset = 0
        
        for j_file in json_files:
            j_path = os.path.join(folder_path, j_file)
            try:
                with open(j_path, 'r', encoding='utf-8') as f:
                    mesh_data = json.load(f)
            except: continue
                
            if mesh_data.get("status") != "success": continue
            
            pts = mesh_data.get("points", [])
            conn = mesh_data.get("connectivity", [])
            vtk_type = mesh_data.get("vtk_type", 9)
            
            combined_points.extend(pts)
            
            mesh_name = os.path.splitext(j_file)[0]
            num_pts_in_mesh = len(pts) // 3
            
            # Index offset correction (Standalone success logic)
            adjusted_conn = []
            for cell in conn:
                adjusted_conn.append([idx + point_offset for idx in cell])
                
            combined_cells[mesh_name] = {
                "type": {3: "line", 5: "triangle", 9: "quad", 10: "tetra", 12: "hexa"}.get(vtk_type, "poly"),
                "connectivity": adjusted_conn
            }
            
            point_offset += num_pts_in_mesh

        return jsonify({
            "status": "success",
            "points": combined_points,
            "cells": combined_cells,
            "num_points": len(combined_points) // 3
        })
    except Exception as e:
        import traceback
        traceback.print_exc()
        return jsonify({"status": "error", "message": str(e)}), 500






@api_blueprint.route('/mesh_dna', methods=['POST'])
def api_mesh_dna():
    """Proxy for legacy mesh_dna call, using modern extractor service."""
    try:
        data = request.get_json()
        file_path = data.get('file_path')

        if not file_path or not os.path.exists(file_path):
            return jsonify({"status": "error", "message": "Invalid file path"}), 400

        print(f"[API] Requesting DNA for: {os.path.basename(file_path)}")
        
        # Use our professional service
        result = call_med_extractor(file_path)
        
        if result:
            return jsonify(result)
        else:
            return jsonify({"status": "error", "message": "Failed to extract mesh DNA"}), 500

    except Exception as e:
        print(f"[API] Mesh DNA Error: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500





# No topo do routes.py
from services.med.call_med_mesher import run_batch_med_processing

@api_blueprint.route('/vtk', methods=['POST'])
def generate_vtk():
    data = request.json
    project_path = data.get('project_path')

    if not project_path or not os.path.exists(project_path):
        return jsonify({"status": "error", "message": "Caminho inválido"}), 400

    try:
        # --- MUDANÇA AQUI: TIRA A THREAD ---
        # Chamamos a função diretamente. O servidor vai ficar "travado" 
        # aqui até o script terminar. Isso garante que quando retornar,
        # os arquivos JSON já existem no disco.
        run_batch_med_processing(project_path)

        return jsonify({
            "status": "success", 
            "message": "Processamento concluído com sucesso."
        })

    except Exception as e:
        print(f"Erro no processamento VTK: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500





@api_blueprint.route('/get_vtk_geometry', methods=['POST'])
def get_vtk_geometry():
    """
    Lê TODOS os arquivos .json da pasta.
    SEM FILTROS. SEM VALIDAÇÃO DE CONTEÚDO.
    """
    data = request.get_json()
    folder_path = data.get('folder_path') or data.get('project_path')

    if not folder_path or not os.path.exists(folder_path):
        return jsonify({"status": "error", "message": "Caminho inválido"}), 400

    json_files = []
    
    try:
        # Pega tudo que termina com .json
        files = [f for f in os.listdir(folder_path) if f.lower().endswith('.json')]

        for filename in files:
            file_path = os.path.join(folder_path, filename)
            
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = json.load(f)

                # Adiciona à lista independente do que tem dentro
                json_files.append({
                    "id": filename,
                    "data": content
                })
                    
            except Exception as read_err:
                # Se o arquivo estiver corrompido e não for um JSON válido, avisa no log mas não trava
                print(f"[API] Falha ao ler JSON {filename}: {read_err}")
                continue

        print(f"[API] Entregando {len(json_files)} arquivos JSON.")
        
        return jsonify({
            "status": "success", 
            "data": json_files
        })

    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500


################################################################################
# PASTA: backend/services
################################################################################

--- ARQUIVO: backend/services/mesh_reader.py ---

import os
import meshio
import numpy as np

def read_mesh_file(file_path):
    """
    Reads a MED file and returns nodes and elements organized by group.
    
    Returns:
    {
        "status": "success",
        "nodes": [[x, y, z], ...],
        "groups": {
            "Group_Name": {
                "type": "line", # or 'triangle', 'quad'
                "elements": [[n1, n2], ...] # Indices into nodes array
            }
        }
    }
    """
    try:
        if not os.path.exists(file_path):
            return {"status": "error", "message": "File not found"}

        mesh = meshio.read(file_path)
        
        # Nodes coordinates
        nodes = mesh.points.tolist()
        
        groups_data = {}

        # meshio organizes data by cell blocks.
        # We need to map groups to cells.
        # In MED files via meshio, groups are often in cell_data_dict['med:group_name'] or similar tags.
        # However, meshio behavior for MED varies. Let's try a robust approach.
        
        # 1. Inspect Cell Data for Groups
        # Usually cell_data contains "med:group" or similar keys mapping to group names?
        # A common way meshio loads MED groups is putting them in mesh.cell_sets (dict of name -> list of cell arrays)
        # OR mesh.cell_data dictionary.
        
        # Let's try to extract logic based on typical meshio 5.x behavior
        
        # Structure to fill:
        # groups_data = { "GroupA": { "lines": [], "triangles": [] } }
        
        for cell_block in mesh.cells:
            cell_type = cell_block.type
            data = cell_block.data.tolist()
            
            # If no groups defined, we might just dump everything as "Default" (but user needs groups)
            # Let's check subsets/groups
            pass

        # Better approach for Code_Aster MED files specifically:
        # Usually meshio puts groups in mesh.cell_sets
        # mesh.cell_sets[group_name] = [ array_of_indices_for_block_0, array_of_indices_for_block_1, ... ]
        
        if hasattr(mesh, 'cell_sets') and mesh.cell_sets:
            for group_name, cell_indices_list in mesh.cell_sets.items():
                if group_name not in groups_data:
                    groups_data[group_name] = {"elements": [], "type": "unknown"}
                
                # Iterate through blocks (mesh.cells) and corresponding indices
                all_elements = []
                primary_type = "unknown"
                
                for i, indices in enumerate(cell_indices_list):
                    if len(indices) == 0: continue
                    
                    cell_block = mesh.cells[i]
                    c_type = cell_block.type
                    c_data = cell_block.data
                    
                    # Store type (simplification: last seen type wins, or prefer lines for beams)
                    if c_type == "line":
                        primary_type = "line"
                    elif c_type in ["triangle", "quad"] and primary_type != "line":
                        primary_type = "surface"
                    
                    # Extract elements belonging to this group
                    # indices is a list/array of indices *valid for this block* (?) 
                    # Actually mesh.cell_sets usually maps to the global index or block index?
                    # In meshio < 5 it was dict of group -> list of arrays (one per cell block).
                    # In meshio >= 5 it is... let's assume it matches the `mesh.cells` list order.
                    
                    # Filter elements
                    # meshio 5.x: cell_sets[name] is a list of numpy arrays. 
                    # The i-th array corresponds to the i-th block in mesh.cells.
                    # The values in the array are boolean (mask) or indices?
                    # Usually it's boolean mask if using latest standards, OR list of indices.
                    # Let's assume generic logic:
                    
                    subset = c_data[indices] # If indices are integer indices
                    # If indices is boolean:
                    # subset = c_data[indices]
                    
                    # We simply convert to list and extend
                    if len(subset) > 0:
                        all_elements.extend(subset.tolist())
                
                if len(all_elements) > 0:
                    groups_data[group_name] = {
                        "type": primary_type,
                        "elements": all_elements
                    }
        else:
            # No groups found, valid fallback?
            # Return everything as "All"
            all_lines = []
            for cell_block in mesh.cells:
                if cell_block.type == "line":
                   all_lines.extend(cell_block.data.tolist())
            
            if all_lines:
                groups_data["All_Lines"] = { "type": "line", "elements": all_lines }

        return {
            "status": "success",
            "nodes": nodes,
            "groups": groups_data
        }

    except Exception as e:
        print(f"Mesh Read Error: {e}")
        return {"status": "error", "message": str(e)}

--- ARQUIVO: backend/services/section_calculator - Copia.py ---
"""
Section Calculator Service - sectionproperties Integration
Migrated from main.pyw lines 389-589
CRITICAL: Preserves exact offset/rotation logic
"""
import io
import base64
from sectionproperties.pre.library import (
    rectangular_section,
    rectangular_hollow_section,
    circular_section,
    circular_hollow_section,
    mono_i_section
)
from sectionproperties.analysis import Section
import matplotlib
matplotlib.use('Agg')  # Non-interactive backend
import matplotlib.pyplot as plt


def calculate_section_properties(section_type: str, params: dict) -> dict:
    """
    Calculate section properties using sectionproperties library.
    
    CRITICAL 3-STEP TRANSFORM:
    1. RESET: align_center(0,0) - Normalize centroid
    2. ROTATE: rotate_section(angle, rot_point=(0,0)) - Rotate around centroid
    3. SHIFT: shift_section(x_offset=off_z, y_offset=off_y) - Move to final position
    
    Args:
        section_type: Type of section (RECTANGLE, BOX, CIRCLE, TUBE, I_SECTION)
        params: Dictionary of section parameters
        
    Returns:
        Dictionary with 'properties' and 'image' (base64)
    """
    try:
        # 1. PARSE PARAMETERS
        p = {k: float(v) for k, v in params.items() if v}
        
        # Position parameters (CRITICAL!)
        off_y = p.get('offset_y', 0.0)
        off_z = p.get('offset_z', 0.0)
        rotation = p.get('rotation', 0.0)  # Degrees
        
        geometry = None
        mesh_size = 10.0
        
        # 2. CREATE BASE GEOMETRY
        if section_type == 'RECTANGLE':
            d, b = p.get('hy', 100), p.get('hz', 50)
            geometry = rectangular_section(d=d, b=b)
            mesh_size = min(d, b) / 5.0
            
        elif section_type == 'BOX':
            d, b, t = p.get('hy', 100), p.get('hz', 50), p.get('t', 5)
            if t*2 >= d or t*2 >= b:
                t = min(d, b)/2 - 0.1
            geometry = rectangular_hollow_section(d=d, b=b, t=t, r_out=0, n_r=1)
            mesh_size = t / 1.5
            
        elif section_type == 'CIRCLE':
            d = 2 * p.get('r', 50)
            geometry = circular_section(d=d, n=64)
            mesh_size = d / 10.0
            
        elif section_type == 'TUBE':
            d, t = 2 * p.get('r', 50), p.get('t', 5)
            if t*2 >= d:
                t = d/2 - 0.1
            geometry = circular_hollow_section(d=d, t=t, n=64)
            mesh_size = t / 1.5
            
        elif section_type == 'I_SECTION':
            h = p.get('h', 200)
            bf_t, bf_b = p.get('bf_top', 100), p.get('bf_bot', 100)
            tf_t, tf_b, tw = p.get('tf_top', 10), p.get('tf_bot', 10), p.get('tw', 6)
            if tw >= bf_t:
                tw = bf_t - 2
            if (tf_t + tf_b) >= h:
                h = tf_t + tf_b + 10
            geometry = mono_i_section(
                d=h, b_t=bf_t, b_b=bf_b, 
                t_ft=tf_t, t_fb=tf_b, t_w=tw, 
                r=p.get('r', 0), n_r=8
            )
            mesh_size = min(tw, tf_t, tf_b) / 1.5
        
        if not geometry:
            raise ValueError(f"Unknown section type: {section_type}")
        
        # 3. GEOMETRIC MANIPULATION (3-STEP CRITICAL FLOW)
        
        # STEP A: RESET (Normalization)
        # Centroid goes to (0,0)
        geometry = geometry.align_center(align_to=(0, 0))
        
        # STEP B: ROTATION (Local)
        # Rotate around centroid (now at 0,0)
        if abs(rotation) > 1e-9:
            # rot_point=(0,0) ensures rotation around own axis
            geometry = geometry.rotate_section(angle=rotation, rot_point=(0, 0))
        
        # STEP C: SHIFT (Global Positioning)
        # Move from (0,0) to final offset
        if abs(off_y) > 1e-9 or abs(off_z) > 1e-9:
            geometry = geometry.shift_section(x_offset=off_z, y_offset=off_y)
        
        # 4. MESH AND CALCULATION
        # Mesh is created at final position (rotated and shifted)
        mesh_size = max(mesh_size, 2.0)
        geometry.create_mesh(mesh_sizes=[mesh_size])
        
        sec = Section(geometry)
        
        # Execute FEM integrals
        sec.calculate_geometric_properties()
        sec.calculate_warping_properties()
        sec.calculate_plastic_properties()
        
        # 5. DIRECT EXTRACTION (GLOBAL ATTRIBUTES)
        # Since mesh is at final position, global integral IS nodal property (Steiner included)
        
        area = sec.section_props.area
        (cx, cy) = sec.get_c()  # Final centroid
        
        # Local inertias (Centroidal - already reflect piece rotation!)
        (ixx_c, iyy_c, ixy_c) = sec.get_ic()
        
        # Nodal inertias (Global at 0,0)
        ixx_frame = sec.section_props.ixx_g
        iyy_frame = sec.section_props.iyy_g
        ixy_frame = sec.section_props.ixy_g
        
        qx_frame = sec.section_props.qx
        qy_frame = sec.section_props.qy
        
        # Others
        try:
            (i1, i2) = sec.get_ip()
            theta = sec.get_phi()
        except:
            (i1, i2, theta) = sec.get_ip()
            
        (rx, ry) = sec.get_rc()
        j = sec.get_j()
        gamma = sec.get_gamma()
        (asx, asy) = sec.get_as()
        
        z_vals = sec.get_z()
        if len(z_vals) == 4:
            zxx_eff = min(z_vals[0], z_vals[1])
            zyy_eff = min(z_vals[2], z_vals[3])
        else:
            zxx_eff, zyy_eff = z_vals[:2]
        
        s_vals = sec.get_s()
        sxx, syy = s_vals[:2]
        
        # 6. RETURN PROPERTIES
        props = {
            "Area (A)": area,
            "Centroid Y (cy)": cy,
            "Centroid Z (cx)": cx,
            "Static Moment Qy (at 0,0)": qx_frame,
            "Static Moment Qz (at 0,0)": qy_frame,
            
            # Local (Relative to rotated piece centroid)
            "Iyy (Local)": iyy_c,
            "Izz (Local)": ixx_c,
            "Iyz (Local)": ixy_c,
            "I1 (Principal)": i1,
            "I2 (Principal)": i2,
            "Angle (deg)": theta,
            
            # Nodal (Absolute reference 0,0)
            "Iyy (Node 0,0)": iyy_frame,
            "Izz (Node 0,0)": ixx_frame,
            "Iyz (Node 0,0)": ixy_frame,
            
            "Torsion J": j,
            "Warping Iw": gamma,
            "Shear Area Ay": asx,
            "Shear Area Az": asy,
            
            "Elastic Mod. Wy (Zxx)": zxx_eff,
            "Elastic Mod. Wz (Zyy)": zyy_eff,
            "Plastic Mod. Zy (Sxx)": sxx,
            "Plastic Mod. Zz (Syy)": syy,
            
            "Radius Gyration ry": ry,
            "Radius Gyration rz": rx,
        }
        
        # 7. GENERATE IMAGE
        plt.style.use('default')
        fig, ax = plt.subplots(figsize=(6, 6))
        
        geometry.plot_geometry(ax=ax, cp=False, legend=False, title='')
        
        fig.patch.set_facecolor('white')
        ax.set_facecolor('white')
        ax.axis('on')
        ax.grid(True, color='#e2e8f0', linestyle='--', linewidth=0.5)
        ax.set_aspect('equal', adjustable='box')
        
        ax.plot(cx, cy, 'r+', markersize=15, markeredgewidth=2, label='Centroid')
        ax.plot(0, 0, 'bx', markersize=12, markeredgewidth=2, label='Node (0,0)')
        
        if abs(cx) > 1e-4 or abs(cy) > 1e-4:
            ax.plot([0, cx], [0, cy], color='red', linestyle=':', linewidth=1.5, label='Offset')
        
        ax.axhline(y=cy, color='#94a3b8', linestyle='-.', linewidth=1)
        ax.axvline(x=cx, color='#94a3b8', linestyle='-.', linewidth=1)
        ax.axhline(y=0, color='black', linestyle='-', linewidth=0.8, alpha=0.3)
        ax.axvline(x=0, color='black', linestyle='-', linewidth=0.8, alpha=0.3)
        
        x_data = [0, cx]
        y_data = [0, cy]
        (xmin, xmax, ymin, ymax) = geometry.calculate_extents()
        x_data.extend([xmin, xmax])
        y_data.extend([ymin, ymax])
        margin = max(xmax-xmin, ymax-ymin) * 0.2
        if margin == 0:
            margin = 10
        ax.set_xlim(min(x_data)-margin, max(x_data)+margin)
        ax.set_ylim(min(y_data)-margin, max(y_data)+margin)
        
        buf = io.BytesIO()
        plt.savefig(buf, format='png', bbox_inches='tight', pad_inches=0.05, dpi=120)
        buf.seek(0)
        img_str = base64.b64encode(buf.read()).decode('utf-8')
        plt.close(fig)
        
        # 8. EXTRACT MESH DATA
        try:
            sp_mesh = geometry.mesh
            vertices = sp_mesh.get('vertices')
            triangles = sp_mesh.get('triangles')
            # Handle quadratic triangles (6 nodes) -> Linear (3 nodes)
            if triangles is not None and len(triangles) > 0 and len(triangles[0]) == 6:
                triangles = triangles[:, :3]
            
            mesh_data = {
                "vertices": vertices.tolist() if vertices is not None else [],
                "triangles": triangles.tolist() if triangles is not None else []
            }
        except:
            mesh_data = None
        
        return {
            "status": "success",
            "properties": props,
            "mesh": mesh_data,
            "image": f"data:image/png;base64,{img_str}"
        }
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise Exception(f"Section calculation error: {str(e)}")

--- ARQUIVO: backend/services/section_calculator.py ---
"""
Section Calculator Service - sectionproperties Integration
Migrated from main.pyw lines 389-589
CRITICAL: Preserves exact offset/rotation logic
"""
import io
import base64
from sectionproperties.pre.library import (
    rectangular_section,
    rectangular_hollow_section,
    circular_section,
    circular_hollow_section,
    mono_i_section
)
from sectionproperties.analysis import Section
import matplotlib
matplotlib.use('Agg')  # Non-interactive backend
import matplotlib.pyplot as plt


def calculate_section_properties(section_type: str, params: dict) -> dict:
    """
    Calculate section properties using sectionproperties library.
    
    CRITICAL 3-STEP TRANSFORM:
    1. RESET: align_center(0,0) - Normalize centroid
    2. ROTATE: rotate_section(angle, rot_point=(0,0)) - Rotate around centroid
    3. SHIFT: shift_section(x_offset=off_z, y_offset=off_y) - Move to final position
    
    Args:
        section_type: Type of section (RECTANGLE, BOX, CIRCLE, TUBE, I_SECTION)
        params: Dictionary of section parameters
        
    Returns:
        Dictionary with 'properties' and 'image' (base64)
    """
    try:
        # 1. PARSE PARAMETERS
        p = {k: float(v) for k, v in params.items() if v}
        
        # Position parameters (CRITICAL!)
        off_y = p.get('offset_y', 0.0)
        off_z = p.get('offset_z', 0.0)
        rotation = p.get('rotation', 0.0)  # Degrees
        
        geometry = None
        mesh_size = 10.0
        
        # 2. CREATE BASE GEOMETRY
        if section_type == 'RECTANGLE':
            d, b = p.get('hy', 100), p.get('hz', 50)
            geometry = rectangular_section(d=d, b=b)
            mesh_size = min(d, b) / 5.0
            
        elif section_type == 'BOX':
            d, b, t = p.get('hy', 100), p.get('hz', 50), p.get('t', 5)
            if t*2 >= d or t*2 >= b:
                t = min(d, b)/2 - 0.1
            geometry = rectangular_hollow_section(d=d, b=b, t=t, r_out=0, n_r=1)
            mesh_size = t / 1.5
            
        elif section_type == 'CIRCLE':
            d = 2 * p.get('r', 50)
            geometry = circular_section(d=d, n=64)
            mesh_size = d / 10.0
            
        elif section_type == 'TUBE':
            d, t = 2 * p.get('r', 50), p.get('t', 5)
            if t*2 >= d:
                t = d/2 - 0.1
            geometry = circular_hollow_section(d=d, t=t, n=64)
            mesh_size = t / 1.5
            
        elif section_type == 'I_SECTION':
            h = p.get('h', 200)
            bf_t, bf_b = p.get('bf_top', 100), p.get('bf_bot', 100)
            tf_t, tf_b, tw = p.get('tf_top', 10), p.get('tf_bot', 10), p.get('tw', 6)
            if tw >= bf_t:
                tw = bf_t - 2
            if (tf_t + tf_b) >= h:
                h = tf_t + tf_b + 10
            geometry = mono_i_section(
                d=h, b_t=bf_t, b_b=bf_b, 
                t_ft=tf_t, t_fb=tf_b, t_w=tw, 
                r=p.get('r', 0), n_r=8
            )
            mesh_size = min(tw, tf_t, tf_b) / 1.5
        
        if not geometry:
            raise ValueError(f"Unknown section type: {section_type}")
        
        # 3. GEOMETRIC MANIPULATION (3-STEP CRITICAL FLOW)
        
        # STEP A: RESET (Normalization)
        # Centroid goes to (0,0)
        geometry = geometry.align_center(align_to=(0, 0))
        
        # STEP B: ROTATION (Local)
        # Rotate around centroid (now at 0,0)
        if abs(rotation) > 1e-9:
            # rot_point=(0,0) ensures rotation around own axis
            geometry = geometry.rotate_section(angle=rotation, rot_point=(0, 0))
        
        # STEP C: SHIFT (Global Positioning)
        # Move from (0,0) to final offset
        if abs(off_y) > 1e-9 or abs(off_z) > 1e-9:
            geometry = geometry.shift_section(x_offset=off_z, y_offset=off_y)
        
        # 4. MESH AND CALCULATION
        # Mesh is created at final position (rotated and shifted)
        mesh_size = max(mesh_size, 2.0)
        geometry.create_mesh(mesh_sizes=[mesh_size])
        
        sec = Section(geometry)
        
        # Execute FEM integrals
        sec.calculate_geometric_properties()
        sec.calculate_warping_properties()
        sec.calculate_plastic_properties()
        
        # 5. DIRECT EXTRACTION (GLOBAL ATTRIBUTES)
        # Since mesh is at final position, global integral IS nodal property (Steiner included)
        
        area = sec.section_props.area
        (cx, cy) = sec.get_c()  # Final centroid
        
        # Local inertias (Centroidal - already reflect piece rotation!)
        (ixx_c, iyy_c, ixy_c) = sec.get_ic()
        
        # Nodal inertias (Global at 0,0)
        ixx_frame = sec.section_props.ixx_g
        iyy_frame = sec.section_props.iyy_g
        ixy_frame = sec.section_props.ixy_g
        
        qx_frame = sec.section_props.qx
        qy_frame = sec.section_props.qy
        
        # Others
        try:
            (i1, i2) = sec.get_ip()
            theta = sec.get_phi()
        except:
            (i1, i2, theta) = sec.get_ip()
            
        (rx, ry) = sec.get_rc()
        j = sec.get_j()
        gamma = sec.get_gamma()
        (asx, asy) = sec.get_as()
        
        z_vals = sec.get_z()
        if len(z_vals) == 4:
            zxx_eff = min(z_vals[0], z_vals[1])
            zyy_eff = min(z_vals[2], z_vals[3])
        else:
            zxx_eff, zyy_eff = z_vals[:2]
        
        s_vals = sec.get_s()
        sxx, syy = s_vals[:2]
        
        # 6. CALCULATE EXTENTS AND PREPARE PROPERTIES
        (xmin, xmax, ymin, ymax) = geometry.calculate_extents()
        
        props = {
            "Area (A)": area,
            "Centroid Y (cy)": cy,
            "Centroid Z (cx)": cx,
            "Static Moment Qy (at 0,0)": qx_frame,
            "Static Moment Qz (at 0,0)": qy_frame,
            
            # Local (Relative to rotated piece centroid)
            "Iyy (Local)": iyy_c,
            "Izz (Local)": ixx_c,
            "Iyz (Local)": ixy_c,
            "I1 (Principal)": i1,
            "I2 (Principal)": i2,
            "Angle (deg)": theta,
            
            # Nodal (Absolute reference 0,0)
            "Iyy (Node 0,0)": iyy_frame,
            "Izz (Node 0,0)": ixx_frame,
            "Iyz (Node 0,0)": ixy_frame,
            
            "Torsion J": j,
            "Warping Iw": gamma,
            "Shear Area Ay": asx,
            "Shear Area Az": asy,
            
            "Elastic Mod. Wy (Zxx)": zxx_eff,
            "Elastic Mod. Wz (Zyy)": zyy_eff,
            "Plastic Mod. Zy (Sxx)": sxx,
            "Plastic Mod. Zz (Syy)": syy,
            
            "Radius Gyration ry": ry,
            "Radius Gyration rz": rx,
 
            # Extents (Relative to Node 0,0)
            "Min Y": ymin,
            "Max Y": ymax,
            "Min X": xmin,
            "Max X": xmax,
        }
        
        # 7. GENERATE IMAGE
        plt.style.use('default')
        fig, ax = plt.subplots(figsize=(6, 6))
        
        geometry.plot_geometry(ax=ax, cp=False, legend=False, title='')
        
        fig.patch.set_facecolor('white')
        ax.set_facecolor('white')
        ax.axis('on')
        ax.grid(True, color='#e2e8f0', linestyle='--', linewidth=0.5)
        ax.set_aspect('equal', adjustable='box')
        
        ax.plot(cx, cy, 'r+', markersize=15, markeredgewidth=2, label='Centroid')
        ax.plot(0, 0, 'bx', markersize=12, markeredgewidth=2, label='Node (0,0)')
        
        if abs(cx) > 1e-4 or abs(cy) > 1e-4:
            ax.plot([0, cx], [0, cy], color='red', linestyle=':', linewidth=1.5, label='Offset')
        
        ax.axhline(y=cy, color='#94a3b8', linestyle='-.', linewidth=1)
        ax.axvline(x=cx, color='#94a3b8', linestyle='-.', linewidth=1)
        ax.axhline(y=0, color='black', linestyle='-', linewidth=0.8, alpha=0.3)
        ax.axvline(x=0, color='black', linestyle='-', linewidth=0.8, alpha=0.3)
        
        x_data = [0, cx, xmin, xmax]
        y_data = [0, cy, ymin, ymax]
        margin = max(xmax-xmin, ymax-ymin) * 0.2
        if margin == 0:
            margin = 10
        ax.set_xlim(min(x_data)-margin, max(x_data)+margin)
        ax.set_ylim(min(y_data)-margin, max(y_data)+margin)
        
        buf = io.BytesIO()
        plt.savefig(buf, format='png', bbox_inches='tight', pad_inches=0.05, dpi=120)
        buf.seek(0)
        img_str = base64.b64encode(buf.read()).decode('utf-8')
        plt.close(fig)
        
        # 8. EXTRACT MESH DATA
        try:
            sp_mesh = geometry.mesh
            vertices = sp_mesh.get('vertices')
            triangles = sp_mesh.get('triangles')
            # Handle quadratic triangles (6 nodes) -> Linear (3 nodes)
            if triangles is not None and len(triangles) > 0 and len(triangles[0]) == 6:
                triangles = triangles[:, :3]
            
            mesh_data = {
                "vertices": vertices.tolist() if vertices is not None else [],
                "triangles": triangles.tolist() if triangles is not None else []
            }
        except:
            mesh_data = None
        
        return {
            "status": "success",
            "properties": props,
            "mesh": mesh_data,
            "image": f"data:image/png;base64,{img_str}"
        }
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise Exception(f"Section calculation error: {str(e)}")

--- ARQUIVO: backend/services/section_extractor.py ---
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Section Extractor - Independent Process for SectionProperties
FIX: Added robust path resolution to find the 'services' package.
"""
import sys
import os
import json
import traceback

# 🛠️ BOOTSTRAP: Add backend root to sys.path to resolve 'services' module
# Get absolute path to the 'backend' directory
current_dir = os.path.dirname(os.path.abspath(__file__))
backend_root = os.path.dirname(current_dir)
if backend_root not in sys.path:
    sys.path.insert(0, backend_root)

def main():
    try:
        # 1. Read input from StdIn
        input_data = sys.stdin.read()
        if not input_data:
            print(json.dumps({"status": "error", "message": "No input received"}))
            return
            
        params_bundle = json.loads(input_data)
        section_type = params_bundle.get('type')
        params = params_bundle.get('params', {})
        
        # 2. Lazy Import (Now safe after path bootstrap)
        from services.section_calculator import calculate_section_properties
        
        # 3. Calculate
        result = calculate_section_properties(section_type, params)
        
        # 4. Output result
        print(json.dumps(result))
        
    except Exception as e:
        error_info = {
            "status": "error",
            "message": str(e),
            "traceback": traceback.format_exc()
        }
        print(json.dumps(error_info))

if __name__ == "__main__":
    main()

--- ARQUIVO: backend/services/section_mesh_service.py ---
"""
Section Mesh Service
Provides 2D mesh data from sectionproperties for 3D extrusion.
Separated from other services to maintain clean logic.
"""
from sectionproperties.pre.library import (
    rectangular_section,
    rectangular_hollow_section,
    circular_section,
    circular_hollow_section,
    mono_i_section
)
import numpy as np

def get_section_mesh(section_type: str, params: dict) -> dict:
    """
    Generate a 2D mesh of the section using sectionproperties.
    
    Args:
        section_type: RECTANGLE, BOX, CIRCLE, TUBE, I_SECTION
        params: Dictionary with hy, hz, thickness, rotation, offset_y, offset_z, etc.
        
    Returns:
        Dictionary with 'vertices' (Nx2) and 'triangles' (Mx3)
    """
    try:
        # Parse parameters
        p = {k: float(v) for k, v in params.items() if v}
        
        # Position parameters
        off_y = p.get('offset_y', 0.0)
        off_z = p.get('offset_z', 0.0)
        rotation = p.get('rotation', 0.0)
        
        geometry = None
        mesh_size = 10.0
        
        # Create base geometry
        if section_type == 'RECTANGLE':
            d, b = p.get('hy', 100), p.get('hz', 50)
            geometry = rectangular_section(d=d, b=b)
            mesh_size = min(d, b) / 5.0
            
        elif section_type == 'BOX':
            d, b, t = p.get('hy', 100), p.get('hz', 50), p.get('t', 5)
            geometry = rectangular_hollow_section(d=d, b=b, t=t, r_out=0, n_r=1)
            mesh_size = t / 1.5
            
        elif section_type == 'CIRCLE':
            d = 2 * p.get('r', 50)
            geometry = circular_section(d=d, n=32) # Lower n for visualization
            mesh_size = d / 8.0
            
        elif section_type == 'TUBE':
            d, t = 2 * p.get('r', 50), p.get('t', 5)
            geometry = circular_hollow_section(d=d, t=t, n=32)
            mesh_size = t / 1.5
            
        elif section_type == 'I_SECTION':
            h = p.get('h', 200)
            bf_t, bf_b = p.get('bf_top', 100), p.get('bf_bot', 100)
            tf_t, tf_b, tw = p.get('tf_top', 10), p.get('tf_bot', 10), p.get('tw', 6)
            geometry = mono_i_section(
                d=h, b_t=bf_t, b_b=bf_b, 
                t_ft=tf_t, t_fb=tf_b, t_w=tw, 
                r=p.get('r', 0), n_r=8
            )
            mesh_size = min(tw, tf_t, tf_b) / 1.5
            
        if not geometry:
            return {"status": "error", "message": f"Unsupported section type: {section_type}"}
            
        # Geometric Transformation (Normalization -> Rotation -> Shift)
        geometry = geometry.align_center(align_to=(0, 0))
        if abs(rotation) > 1e-9:
            geometry = geometry.rotate_section(angle=rotation, rot_point=(0, 0))
        if abs(off_y) > 1e-9 or abs(off_z) > 1e-9:
            geometry = geometry.shift_section(x_offset=off_z, y_offset=off_y)
            
        # Create mesh (Force linear order=1 to get T3 triangles for simpler extrusion)
        mesh_size = max(mesh_size, 2.0)
        geometry.create_mesh(mesh_sizes=[mesh_size])
        
        # Extract mesh data
        mesh_data = geometry.mesh
        vertices = mesh_data.get('vertices')
        triangles = mesh_data.get('triangles')
        
        # If triangles are quadratic (6 nodes), take only the first 3 (corners)
        if triangles is not None and len(triangles) > 0 and len(triangles[0]) == 6:
            triangles = triangles[:, :3]
            
        return {
            "status": "success",
            "vertices": vertices.tolist() if vertices is not None else [],
            "triangles": triangles.tolist() if triangles is not None else []
        }
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        return {"status": "error", "message": str(e)}

--- ARQUIVO: backend/services/vtk_converter.py ---
import os
import subprocess
import json
import vtk
import meshio
import numpy as np

def vtk_obj_to_json(vtk_obj):
    """Auxiliary to convert VTK result back to frontend JSON format"""
    points = []
    pts = vtk_obj.GetPoints()
    if not pts: return {"points": [], "connectivity": []}
    for i in range(pts.GetNumberOfPoints()):
        points.append(pts.GetPoint(i))
    cells = []
    for i in range(vtk_obj.GetNumberOfCells()):
        cell = vtk_obj.GetCell(i)
        p_ids = [cell.GetPointId(j) for j in range(cell.GetNumberOfPoints())]
        cells.append(p_ids)
    return {"points": points, "connectivity": cells}

def call_med_extractor(file_path):
    """
    STRICT STEP 1: Calls med_extractor.py to get groups and mesh info.
    """
    base_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    med_dir = os.path.join(base_dir, "MEDCOUPLING-9.15.0", "MEDCOUPLING-9.15.0")
    extractor_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "med", "med_extractor.py")

    if not os.path.exists(med_dir):
        print(f"[VTK] MEDCOUPLING directory not found at {med_dir}")
        return None
        
    try:
        cmd = f'cmd /c "cd /d {med_dir} && call env_launch.bat && python \"{extractor_path}\" \"{file_path}\""'
        print(f"[VTK] Extracting Groups via med_extractor.py...")
        
        result = subprocess.run(cmd, capture_output=True, text=True, shell=True)
        if result.returncode == 0:
            output = result.stdout.strip()
            lines = output.split('\n')
            for line in reversed(lines):
                line = line.strip()
                if line.startswith('{') and line.endswith('}'):
                    try:
                        data = json.loads(line)
                        if data.get("status") == "success": return data
                    except: continue
        else:
            print(f"[VTK] med_extractor failed: {result.stderr}")
    except Exception as e:
        print(f"[VTK] Error calling extractor: {e}")
    return None

def med_to_vtk_json(file_path, geometries=None):
    """
    Convert MED file to VTK JSON with NATIVE EXTRUSION.
    Follows: Load -> med_extractor -> VTK filters.
    """
    try:
        if not os.path.exists(file_path):
            return {"status": "error", "message": f"File not found: {file_path}"}

        # 1. GET SOURCE DATA (Strict Step 1)
        med_data = call_med_extractor(file_path)
        if not med_data:
            return {"status": "error", "message": "Failed to extract MED data"}

        points = med_data["points"]
        cells_by_group_source = med_data["cells"]
        
        geom_map = {}
        if geometries:
            for g in geometries:
                group = g.get('group')
                if group: geom_map[str(group).strip().upper()] = g

        final_points = list(points)
        final_cells = {}

        # 2. PROCESS NATIVE VTK EXTRUSION (Keep as requested)
        for g_name, g_data in cells_by_group_source.items():
            connectivity = g_data["connectivity"]
            cell_type = g_data["type"]
            
            geom = geom_map.get(str(g_name).strip().upper())
            category = geom.get('_category') if geom else None
            if not category:
                category = '1D' if cell_type == 'line' else ('2D' if cell_type in ('triangle', 'quad') else '3D')

            params = geom.get('section_params', {}) if geom else {}
            thickness = float(params.get('thickness', 0.0))
            offset = float(params.get('offset', 0.0))

            if category == '2D' and thickness > 0:
                print(f"[VTK] NATIVE: Extruding Shell {g_name}")
                vtk_pts = vtk.vtkPoints()
                for p in points: vtk_pts.InsertNextPoint(p)
                
                poly = vtk.vtkPolyData()
                poly.SetPoints(vtk_pts)
                polys = vtk.vtkCellArray()
                for c in connectivity:
                    polys.InsertNextCell(len(c))
                    for p_idx in c: polys.InsertCellPoint(p_idx)
                poly.SetPolys(polys)
                
                norm = vtk.vtkPolyDataNormals()
                norm.SetInputData(poly)
                norm.SetComputePointNormals(True)
                norm.SplittingOff()
                norm.Update()
                
                warp = vtk.vtkWarpVector()
                warp.SetInputConnection(norm.GetOutputPort())
                warp.SetScaleFactor(offset - thickness/2.0)
                warp.Update()
                
                ext = vtk.vtkLinearExtrusionFilter()
                ext.SetInputConnection(warp.GetOutputPort())
                ext.SetExtrusionTypeToNormalExtrusion()
                ext.SetScaleFactor(thickness)
                ext.Update()
                
                res = vtk_obj_to_json(ext.GetOutput())
                base_idx = len(final_points)
                final_points.extend(res["points"])
                translated_conn = [[i + base_idx for i in c] for c in res["connectivity"]]
                
                final_cells[g_name] = {"type": "quad", "connectivity": translated_conn, "is_extruded": True}
                final_cells[g_name + "_ORIGINAL"] = {"type": cell_type, "connectivity": connectivity, "is_extruded": False}

            elif category == '1D' and cell_type == 'line':
                print(f"[VTK] NATIVE: Extruding Beam {g_name}")
                radius = float(params.get('radius', 0.05))
                vtk_pts = vtk.vtkPoints()
                for p in points: vtk_pts.InsertNextPoint(p)
                
                poly = vtk.vtkPolyData()
                poly.SetPoints(vtk_pts)
                lines = vtk.vtkCellArray()
                for c in connectivity:
                    lines.InsertNextCell(len(c))
                    for p_idx in c: lines.InsertCellPoint(p_idx)
                poly.SetLines(lines)
                
                tuber = vtk.vtkTubeFilter()
                tuber.SetInputData(poly)
                tuber.SetRadius(radius)
                tuber.SetNumberOfSides(8)
                tuber.CappingOn()
                tuber.Update()
                
                res = vtk_obj_to_json(tuber.GetOutput())
                base_idx = len(final_points)
                final_points.extend(res["points"])
                translated_conn = [[i + base_idx for i in c] for c in res["connectivity"]]
                
                final_cells[g_name] = {"type": "quad", "connectivity": translated_conn, "is_extruded": True}
                final_cells[g_name + "_ORIGINAL"] = {"type": cell_type, "connectivity": connectivity, "is_extruded": False}
            else:
                final_cells[g_name] = g_data # Keep original metadata

        return {
            "status": "success",
            "points": final_points,
            "cells": final_cells,
            "num_points": len(final_points),
            "num_groups": len(final_cells)
        }
    except Exception as e:
        import traceback
        traceback.print_exc()
        return {"status": "error", "message": f"VTK Process Error: {str(e)}"}

--- ARQUIVO: backend/services/vtk_shell_extruder.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import sys
import os
import json
import vtk

# ==========================================================
# PARÂMETROS DE CONFIGURAÇÃO
# ==========================================================
# Se passar um diretório por argumento no terminal, ele usa. 
# Caso contrário, usa o caminho abaixo:
DIRETORIO_ALVO = r"C:\Users\jorge\OneDrive\ProSolveSimulation\testcases\hibrido"

ESPESSURA = 10.0  # Para as cascas
OFFSET    = -5   # Centralizado
# ==========================================================

def process_mesh(json_path, renderer):
    """Lê o JSON e decide se extruda ou apenas renderiza."""
    if not os.path.exists(json_path): return

    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    points_data = data.get("points", [])
    connectivity = data.get("connectivity", [])
    vtk_type = data.get("vtk_type", 5)
    name = os.path.basename(json_path)

    # 1. Criar Pontos e Grid
    vtk_pts = vtk.vtkPoints()
    for i in range(0, len(points_data), 3):
        vtk_pts.InsertNextPoint(points_data[i], points_data[i+1], points_data[i+2])

    grid = vtk.vtkUnstructuredGrid()
    grid.SetPoints(vtk_pts)
    for cell in connectivity:
        grid.InsertNextCell(vtk_type, len(cell), cell)

    # 2. DECISÃO DE RENDERIZAÇÃO
    # Cascas (TRI3=5, QUAD4=9) -> EXTRUDA
    if vtk_type in [5, 9]:
        print(f"[SHELL] Extrudando: {name}")
        
        # Actor da Face Original (Amarelo)
        m_mapper = vtk.vtkDataSetMapper()
        m_mapper.SetInputData(grid)
        m_actor = vtk.vtkActor()
        m_actor.SetMapper(m_mapper)
        m_actor.GetProperty().SetColor(1, 1, 0) # Amarelo
        m_actor.GetProperty().SetEdgeVisibility(True)
        m_actor.GetProperty().SetEdgeColor(0,0,0)
        m_mapper.SetRelativeCoincidentTopologyPolygonOffsetParameters(-1, -1)
        renderer.AddActor(m_actor)

        # Pipeline de Extrusão com Offset
        geom = vtk.vtkGeometryFilter()
        geom.SetInputData(grid)
        
        normals = vtk.vtkPolyDataNormals()
        normals.SetInputConnection(geom.GetOutputPort())
        normals.ComputePointNormalsOn()
        
        start_pos = OFFSET - (ESPESSURA / 2.0)
        warp = vtk.vtkWarpVector()
        warp.SetInputConnection(normals.GetOutputPort())
        warp.SetInputArrayToProcess(0, 0, 0, vtk.vtkDataObject.FIELD_ASSOCIATION_POINTS, vtk.vtkDataSetAttributes.NORMALS)
        warp.SetScaleFactor(start_pos)
        
        extruder = vtk.vtkLinearExtrusionFilter()
        extruder.SetInputConnection(warp.GetOutputPort())
        extruder.SetExtrusionTypeToNormalExtrusion()
        extruder.SetScaleFactor(ESPESSURA)
        
        ext_mapper = vtk.vtkPolyDataMapper()
        ext_mapper.SetInputConnection(extruder.GetOutputPort())
        ext_actor = vtk.vtkActor()
        ext_actor.SetMapper(ext_mapper)
        ext_actor.GetProperty().SetColor(0.2, 0.6, 1.0) # Azul
        ext_actor.GetProperty().SetOpacity(1.0)
        renderer.AddActor(ext_actor)

    # Linhas (SEG2=3) -> APENAS LINHAS
    elif vtk_type in [3, 4]:
        print(f"[LINE] Renderizando linhas: {name}")
        l_mapper = vtk.vtkDataSetMapper()
        l_mapper.SetInputData(grid)
        l_actor = vtk.vtkActor()
        l_actor.SetMapper(l_mapper)
        l_actor.GetProperty().SetColor(0, 1, 0) # Verde
        l_actor.GetProperty().SetLineWidth(2)
        renderer.AddActor(l_actor)

    # Sólidos (TETRA=10, HEXA=12) -> APENAS O VOLUME ORIGINAL
    else:
        print(f"[SOLID] Renderizando volume original: {name}")
        s_mapper = vtk.vtkDataSetMapper()
        s_mapper.SetInputData(grid)
        s_actor = vtk.vtkActor()
        s_actor.SetMapper(s_mapper)
        s_actor.GetProperty().SetColor(0.7, 0.2, 0.2) # Vermelho escuro
        s_actor.GetProperty().SetOpacity(0.5)
        s_actor.GetProperty().SetEdgeVisibility(True)
        renderer.AddActor(s_actor)

def run_scene_viewer(folder_path):
    if not os.path.isdir(folder_path):
        print(f"Erro: Pasta não encontrada: {folder_path}")
        return

    json_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.json') and f != 'project.json']
    
    if not json_files:
        print("Nenhum arquivo de malha JSON encontrado.")
        return

    # Setup do VTK
    renderer = vtk.vtkRenderer()
    renderWin = vtk.vtkRenderWindow()
    renderWin.AddRenderer(renderer)
    renderWin.SetWindowName(f"VTK Multi-Mesh Viewer - {os.path.basename(folder_path)}")
    renderWin.SetSize(1280, 720)

    interactor = vtk.vtkRenderWindowInteractor()
    interactor.SetRenderWindow(renderWin)

    print(f"\n--- Carregando arquivos de: {folder_path} ---")
    for jf in json_files:
        full_path = os.path.join(folder_path, jf)
        process_mesh(full_path, renderer)

    renderer.SetBackground(0.1, 0.1, 0.1)
    renderWin.Render()
    renderer.ResetCamera()
    renderWin.Render()
    
    print("\n[INFO] 'q' para sair | Mouse para rotacionar.")
    interactor.Start()

if __name__ == "__main__":
    path = DIRETORIO_ALVO
    if len(sys.argv) > 1:
        path = sys.argv[1]
    
    run_scene_viewer(path)


################################################################################
# PASTA: backend/services/jinja
################################################################################

--- ARQUIVO: backend/services/jinja/generate_comm.py ---
# =========================================================
# generate_comm.py
# Gera arquivo .comm do Code_Aster a partir de JSON + Jinja
# =========================================================

import json
import sys
import argparse
from pathlib import Path
from jinja2 import Environment, FileSystemLoader

# ---------------------------------------------------------
# 1. Configuração de Diretórios e Argumentos
# ---------------------------------------------------------
parser = argparse.ArgumentParser(description="Gera script .comm do Code_Aster")
parser.add_argument("--project_path", type=str, help="Caminho raiz do projeto")
args = parser.parse_args()

BASE_DIR = Path(__file__).resolve().parent
BUILDERS_DIR = BASE_DIR / "builders"
TEMPLATES_DIR = BASE_DIR / "templates"

if args.project_path:
    # Modo Produção: Usa a pasta simulation_files do projeto
    PROJECT_DIR = Path(args.project_path)
    SIM_DIR = PROJECT_DIR / "simulation_files"
    STUDY_DIR = SIM_DIR
    OUTPUT_DIR = SIM_DIR
else:
    # Modo Legado/Desenvolvimento: Usa as pastas locais
    STUDY_DIR = BASE_DIR / "study"
    OUTPUT_DIR = BASE_DIR / "output"

# ---------------------------------------------------------
# 2. Importação de Builders
# ---------------------------------------------------------
sys.path.insert(0, str(BUILDERS_DIR))

try:
    from asse_maillage import build_asse_maillage
    from affe_modele import build_affe_modele
    from defi_materiau import build_defi_materiau
    from affe_materiau import build_affe_materiau
    from affe_char_meca_ddl import build_affe_char_meca_ddl 
    from pesanteur import build_pesanteur
    from load_cases import build_load_cases
    from force_coque import build_force_coque
    from post_elem_mass import build_post_elem_mass
    from post_releve_t_reactions import build_post_releve_t_reactions
    from force_nodale import build_force_nodale
    from geometry import build_geometry
except ImportError as e:
    raise ImportError(f"Erro ao importar builders: {e}")

# 3. Leitura dos Arquivos de Configuração (JSON)
# ---------------------------------------------------------

def load_json(path, key=None):
    if not path.exists(): return [] if key else {}
    try:
        with open(path, encoding="utf-8", errors='replace') as f:
            data = json.load(f)
            return data.get(key, []) if key else data
    except Exception:
        return [] if key else {}

# A. Unified Project Config
project_file = PROJECT_DIR / "project.json"
project_config = load_json(project_file)

if not project_config:
    raise FileNotFoundError(f"Arquivo de projeto não encontrado em {project_file}")

# Mesh data now comes from project.json meshes key
mesh_config = project_config.get("meshes", [])
if not mesh_config:
    # Check if meshes are in a different key or if it's the old structure
    raise FileNotFoundError(f"Dados de malha ('meshes') não encontrados em {project_file}")

mesh_names = [m["name"] for m in mesh_config]

# B. Materials & Geometry (From project.json keys)
mat_props_list = project_config.get("materials", [])
# Material assignments are now expected inside each material object as 'assignedGroups'
# but we can also handle a separate list if present
assign_list = project_config.get("material_assignments", [])
if not assign_list and mat_props_list:
    # Synthesize internal assignments for builder compatibility
    assign_list = [{"material": m["name"], "groups": m.get("assignedGroups", [])} for m in mat_props_list]

geometry_list = project_config.get("geometries", [])

# F. Loadings (From project.json keys)
ddl_list = project_config.get("restrictions", []) # restrictions maps to DDL
meca_config = project_config.get("meca_statique", {})
pes_config = project_config.get("pesanteur", {})
lc_list = project_config.get("load_cases", [])
foc_config = project_config.get("force_coque", {})
mass_config = project_config.get("post_elem_mass", {})
reac_config = project_config.get("post_releve_t_reactions", {})

# Handle loads from unified list
all_loads = project_config.get("loads", [])
nod_list = [l for l in all_loads if l.get("type") == "FORCE_NODALE"]
pesanteur_loads = [l for l in all_loads if l.get("type") == "PESANTEUR"]

# If there's a legacy top-level pesanteur key, we could merge it, 
# but the current UI uses the unified 'loads' list.
if not pesanteur_loads and project_config.get("pesanteur"):
    lp = project_config.get("pesanteur")
    pesanteur_loads = lp if isinstance(lp, list) else [lp]

# ---------------------------------------------------------
# 4. Preparação dos Dados (Builders)
# ---------------------------------------------------------

FINAL_MESH = "MAIL"
FINAL_MODEL = "MODELE"
FINAL_CHMAT = "CHAM_MATER"
FINAL_CARA = "CARA_ELEM"
FINAL_DDL = "CHARGE_DDL"
FINAL_PES = "CHARGE_PES"
FINAL_NOD = "CHARGE_NOD"

# A. Assembly
asse_data = build_asse_maillage(mesh_names, result_name=FINAL_MESH)

# B & E. Geometry (Model + Properties)
geom_data = build_geometry(geometry_list, model_name=FINAL_MODEL, result_name=FINAL_CARA)
model_data = { "result_name": FINAL_MODEL, "mesh_name": FINAL_MESH, "items": geom_data["model_items"] }
for item in model_data["items"]: item["phenomene"] = "MECANIQUE"

# C & D. Materials
defi_mat_data = build_defi_materiau(mat_props_list)
affe_mat_data = build_affe_materiau(assign_list, model_name=FINAL_MODEL, result_name=FINAL_CHMAT)

# F. DDL & Forces
ddl_data = build_affe_char_meca_ddl(ddl_list, model_name=FINAL_MODEL, result_name=FINAL_DDL)
pes_data = build_pesanteur({"pesanteur": pesanteur_loads}, model_name=FINAL_MODEL, result_name=FINAL_PES)
foc_calc_data, foc_load_data = build_force_coque(foc_config, model_name=FINAL_MODEL)
reac_data = build_post_releve_t_reactions(reac_config, ddl_list)
nod_data = build_force_nodale(nod_list, model_name=FINAL_MODEL, result_name=FINAL_NOD)

# G. Load Cases Summary
lc_data = build_load_cases(
    lc_list, 
    meca_config, 
    pes_data=pes_data, 
    ddl_data=ddl_data, 
    model_name=FINAL_MODEL,
    reaction_extraction_data=reac_data,
    foc_data=foc_load_data,
    nod_data=nod_data
)

# H. Mass
mass_data = build_post_elem_mass(mass_config, model_name=FINAL_MODEL, field_mat_name=FINAL_CHMAT, cara_elem_name=FINAL_CARA)

# ---------------------------------------------------------
# 5. Configuração Jinja
# ---------------------------------------------------------
env = Environment(loader=FileSystemLoader(str(TEMPLATES_DIR)), trim_blocks=True, lstrip_blocks=True)

try:
    tpl_preamble = env.get_template("preamble.j2")
    tpl_lire = env.get_template("lire_maillage.j2")
    tpl_asse = env.get_template("asse_maillage.j2")
    tpl_inspect = env.get_template("inspect_mesh.j2") 
    tpl_model = env.get_template("affe_modele.j2")
    tpl_defi = env.get_template("defi_materiau.j2")
    tpl_affe = env.get_template("affe_materiau.j2")
    tpl_cara = env.get_template("affe_cara_elem.j2")
    tpl_ddl = env.get_template("affe_char_meca_ddl.j2") 
    tpl_pes = env.get_template("pesanteur.j2")
    tpl_lc = env.get_template("load_cases.j2")
    tpl_foc_calc = env.get_template("force_coque_calc.j2")
    tpl_foc_load = env.get_template("force_coque_load.j2")
    tpl_nod = env.get_template("force_nodale.j2")
    tpl_mass = env.get_template("post_elem_mass.j2")
    tpl_geom_check = env.get_template("geometric_check.j2")
    tpl_results = env.get_template("extract_results.j2")
except Exception as e:
    raise RuntimeError(f"Error loading Jinja templates: {e}")

# ---------------------------------------------------------
# 6. Escrita do Arquivo .comm
# ---------------------------------------------------------
OUTPUT_DIR.mkdir(exist_ok=True)
comm_path = OUTPUT_DIR / "calcul.comm"
import io
import re

import datetime

# Resolve absolute paths for debugging
print(f"DEBUG: Script starting. Path: {Path(__file__).resolve()}")
print(f"DEBUG: PROJECT_DIR: {PROJECT_DIR.resolve()}")
print(f"DEBUG: OUTPUT_DIR: {OUTPUT_DIR.resolve()}")

comm_path = (OUTPUT_DIR / "calcul.comm").resolve()
print(f"Generating auditable script in: {comm_path}")

output_buffer = io.StringIO()

with output_buffer as f:
    f.write(f"# Generated at: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    f.write("DEBUT(LANG='FR')\n\n")
    
    f.write(tpl_preamble.render() + "\n")
    for i, mesh in enumerate(mesh_config):
        unit = mesh.get("unit", 80+i)
        f.write(tpl_lire.render(mesh_name=mesh["name"], unit=unit, filename=mesh.get("filename", f"{mesh['name']}.med")))
        f.write("\n\n\n\n")

    if asse_data["mode"] == "ASSE":
        f.write(tpl_asse.render(**asse_data))
    elif asse_data["mode"] == "SINGLE":
        if asse_data['final_mesh'] != FINAL_MESH:
            f.write(f"{FINAL_MESH} = {asse_data['final_mesh']}\n")
    f.write("\n\n\n\n")

    if model_data["items"]:
        f.write(tpl_model.render(**model_data))
        f.write("\n\n\n\n")

    if defi_mat_data or affe_mat_data["items"]:
        if defi_mat_data:
            f.write(tpl_defi.render(definitions=defi_mat_data))
            f.write("\n\n\n\n")
        if affe_mat_data["items"]:
            f.write(tpl_affe.render(**affe_mat_data))
            f.write("\n\n\n\n")

    if geom_data["cara_items"]:
        f.write(tpl_cara.render(**geom_data))
        f.write("\n\n\n\n")

    if foc_calc_data:
        f.write(tpl_foc_calc.render(**foc_calc_data))
        f.write("\n\n\n\n")

    if ddl_data: f.write(tpl_ddl.render(commands=ddl_data) + "\n\n\n\n")
    if pes_data: f.write(tpl_pes.render(commands=pes_data) + "\n\n\n\n")
    if nod_data: f.write(tpl_nod.render(**nod_data) + "\n\n\n\n")
    if foc_load_data and foc_load_data.get("load_items"):
        f.write(tpl_foc_load.render(**foc_load_data) + "\n\n\n\n")

    if lc_data.get("runs"):
        for run in lc_data["runs"]:
            f.write(tpl_lc.render(**run))
            f.write("\n\n\n\n")

    f.write(tpl_geom_check.render(model_name=FINAL_MODEL, cara_elem_name=FINAL_CARA))
    f.write("\n\n\n\n")

    if lc_data.get("runs"):
        has_shells = any(item.get("type") == "COQUE" for item in geom_data.get("cara_items", []))
        f.write(tpl_results.render(has_shells=has_shells, cara_items=geom_data.get("cara_items", []), **lc_data))
        f.write("\n\n\n\n")

    f.write("FIN()\n")
    
    # Extração e Limpeza do conteúdo (dentro do bloco with)
    comm_content = output_buffer.getvalue()

# Limpeza e Escrita final
# Permite ate 4 quebras de linha para o respiro de 3 linhas vazias
comm_content = re.sub(r'\n{5,}', '\n\n\n\n', comm_content)

print(f"DEBUG: Writing {len(comm_content)} bytes to {comm_path}")
try:
    with open(comm_path, "w", encoding="utf-8") as f_out:
        f_out.write(comm_content)
    print(f"Success! .comm script generated at {comm_path}")
except Exception as e:
    print(f"CRITICAL ERROR writing script: {e}", file=sys.stderr)
    sys.exit(1)

--- ARQUIVO: backend/services/jinja/inspect_mesh.py ---
import sys
import json
import os
from pathlib import Path
from jinja2 import Environment, FileSystemLoader

# --- CONFIGURAÇÃO DE CAMINHOS ---
# Localização deste script: root/jinja/inspect_mesh.py
BASE_DIR = Path(__file__).resolve().parent
BUILDERS_DIR = BASE_DIR / "builders"
TEMPLATES_DIR = BASE_DIR / "templates"

# Validação de diretórios essenciais
if not BUILDERS_DIR.exists():
    print(f"[ERROR] Diretorio de builders nao encontrado: {BUILDERS_DIR}")
    sys.exit(1)

if not TEMPLATES_DIR.exists():
    print(f"[ERROR] Diretorio de templates nao encontrado: {TEMPLATES_DIR}")
    sys.exit(1)

# Adiciona builders ao path do sistema para importação
sys.path.insert(0, str(BUILDERS_DIR))

try:
    from asse_maillage import build_asse_maillage
    # lire_maillage é opcional importar se fizermos o loop manual, mas asse é vital
except ImportError as e:
    print(f"[ERROR] Falha ao importar builders do Code_Aster: {e}")
    sys.exit(1)

def generate_inspection_comm(project_folder):
    """
    Orquestrador que lê o mesh.json e gera o arquivo de comando med.comm
    dentro da pasta simulation_files.
    """
    project_path = Path(project_folder)
    sim_files_path = project_path / "simulation_files"
    
    # ARQUIVO DE ENTRADA (Gerado pelo Python Main)
    mesh_json_path = sim_files_path / "mesh.json"
    
    # ARQUIVO DE SAÍDA (Onde o .comm será salvo)
    output_comm = sim_files_path / "med.comm"

    print(f"[INSPECT] Gerando .comm para: {project_path}")
    print(f"[INSPECT] Arquivo de destino: {output_comm}")

    # 1. Ler o JSON de Input (mesh.json)
    if not mesh_json_path.exists():
        print(f"[ERROR] mesh.json nao encontrado em: {mesh_json_path}")
        return False

    try:
        with open(mesh_json_path, 'r', encoding='utf-8') as f:
            mesh_config = json.load(f)
    except Exception as e:
        print(f"[ERROR] Falha ao ler ou decodificar mesh.json: {e}")
        return False

    # 2. Configurar Jinja2
    env = Environment(loader=FileSystemLoader(str(TEMPLATES_DIR)), trim_blocks=True, lstrip_blocks=True)
    
    try:
        tpl_lire = env.get_template("lire_maillage.j2")
        tpl_asse = env.get_template("asse_maillage.j2")
        tpl_inspect = env.get_template("inspect_mesh.j2")
    except Exception as e:
        print(f"[ERROR] Erro ao carregar templates Jinja: {e}")
        return False

    # 3. Processar Dados
    unit_start = mesh_config.get("unit_start", 80)
    meshes_list = mesh_config.get("meshes", [])
    
    if not meshes_list:
        print("[ERROR] Nenhuma malha definida dentro de mesh.json")
        return False
    
    try:
        # Inicio do arquivo .comm
        comm_content = "DEBUT(LANG='FR')\n\n"
        comm_content += "# --- 1. Leitura das Malhas ---\n"

        mesh_names = []
        
        # Loop para gerar comandos LIRE_MAILLAGE
        for i, mesh in enumerate(meshes_list):
            unit = unit_start + i
            mesh_name = mesh["name"]
            mesh_names.append(mesh_name)
            
            # O nome do arquivo no .comm é menos relevante se usarmos UNITE no export,
            # mas mantemos por consistência.
            filename = mesh.get("filename", f"{mesh_name}.med")
            
            # Renderiza template LIRE
            comm_content += tpl_lire.render(
                mesh_name=mesh_name,
                unit=unit,
                filename=filename
            ) + "\n"

        # Builder ASSE_MAILLAGE (Assembly)
        comm_content += "\n# --- 2. Assembly ---\n"
        asse_data = build_asse_maillage(mesh_names, result_name="MAIL")
        
        final_mesh_variable = "MAIL"
        
        # Lógica de renderização do Assembly
        if asse_data["mode"] == "ASSE":
            # Se houver múltiplas malhas, usa o comando ASSE_MAILLAGE
            comm_content += tpl_asse.render(**asse_data)
            final_mesh_variable = "MAIL"
            
        elif asse_data["mode"] == "SINGLE":
            # Se for única, o nome final é o nome da malha lida
            original_name = asse_data['final_mesh']
            
            # Cria alias MAIL = Mesh_Nome para padronizar o script de inspeção
            if original_name != "MAIL":
                 comm_content += f"MAIL = {original_name}\n"
                 final_mesh_variable = "MAIL"
        
        comm_content += "\n"

        # 4. Injeção do Script de Inspeção (Gera mesh_groups.json)
        comm_content += "# --- 3. Inspecao e Geracao de JSON ---\n"
        
        # --- PREPARAÇÃO DO CAMINHO ABSOLUTO ---
        # Converte para string e substitui backslash por forward slash para evitar 
        # problemas de escape string no Python gerado (ex: \t, \n, \r)
        abs_output_folder = str(sim_files_path.absolute()).replace("\\", "/")
        
        # Passamos a variável output_folder para o template
        comm_content += tpl_inspect.render(
            mesh_variable=final_mesh_variable,
            output_folder=abs_output_folder
        )
        comm_content += "\n"

        comm_content += "FIN()\n"

        # 5. Salvar Arquivo med.comm final na pasta simulation_files
        # --- Encoding="latin-1" para compatibilidade com Code_Aster Windows ---
        with open(output_comm, "w", encoding="latin-1") as f:
            f.write(comm_content)
            
        print(f"[INSPECT] med.comm gerado com sucesso em: {output_comm}")
        return True

    except Exception as e:
        print(f"[ERROR] Falha na construcao do conteudo .comm: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Uso: python inspect_mesh.py <caminho_do_projeto>")
        sys.exit(1)
    
    folder_arg = sys.argv[1]
    success_status = generate_inspection_comm(folder_arg)
    
    # Retorna código de saída para o subprocesso pai (main.pyw) capturar
    sys.exit(0 if success_status else 1)

--- ARQUIVO: backend/services/jinja/server.py ---
import http.server
import subprocess
import webbrowser
import os
import json

PORT = 8000

class Handler(http.server.SimpleHTTPRequestHandler):
    def do_POST(self):
        if self.path == '/run-aster':
            # Read JSON body
            content_length = int(self.headers.get('Content-Length', 0))
            body = self.rfile.read(content_length)
            
            try:
                data = json.loads(body) if body else {}
            except:
                data = {}
            
            os.makedirs("study", exist_ok=True)
            
            # Write mesh.json
            if "mesh" in data:
                mesh_path = os.path.join("study", "mesh.json")
                with open(mesh_path, "w", encoding="utf-8") as f:
                    json.dump(data["mesh"], f, indent=4)
                print(f"[Server] Wrote {mesh_path}")
            
            # Write ddl_impo.json
            ddl_data = {"ddl_impo": data.get("ddl_impo", [])}
            ddl_path = os.path.join("study", "ddl_impo.json")
            with open(ddl_path, "w", encoding="utf-8") as f:
                json.dump(ddl_data, f, indent=4)
            print(f"[Server] Wrote {ddl_path}")
            
            # Write pesanteur.json
            if "pesanteur" in data:
                pesanteur_path = os.path.join("study", "pesanteur.json")
                with open(pesanteur_path, "w", encoding="utf-8") as f:
                    json.dump(data["pesanteur"], f, indent=4)
                print(f"[Server] Wrote {pesanteur_path}")
            
            # Write load_cases.json
            if "load_cases" in data:
                lc_path = os.path.join("study", "load_cases.json")
                with open(lc_path, "w", encoding="utf-8") as f:
                    json.dump(data["load_cases"], f, indent=4)
                print(f"[Server] Wrote {lc_path}")

            # Write geometry.json
            if "geometries" in data:
                geom_list = data["geometries"]
                geom_data = {"geometries": geom_list}
                geom_path = os.path.join("study", "geometry.json")
                with open(geom_path, "w", encoding="utf-8") as f:
                    json.dump(geom_data, f, indent=4)
                print(f"[Server] Wrote {geom_path} ({len(geom_list)} items)")
            else:
                print("[Server] WARNING: 'geometries' not found in payload")
            
            # Run Generator and then Code_Aster
            print("[Server] Generating .comm and Running Code_Aster...")
            subprocess.Popen('start cmd /k "generate_comm.bat && run_aster.bat"', shell=True)
            
            self.send_response(200)
            self.send_header('Content-type', 'text/plain')
            self.end_headers()
            self.wfile.write(b"OK")
        else:
            self.send_error(404)

    def end_headers(self):
        self.send_header('Access-Control-Allow-Origin', '*')
        super().end_headers()

if __name__ == "__main__":
    os.chdir(os.path.dirname(os.path.abspath(__file__)))
    webbrowser.open(f"http://localhost:{PORT}/studio.html")
    print(f"Server running on http://localhost:{PORT}")
    http.server.HTTPServer(("", PORT), Handler).serve_forever()

--- ARQUIVO: backend/services/jinja/teste.py ---
from medcoupling import MEDFileMesh

print("MEDCoupling OK")


################################################################################
# PASTA: backend/services/jinja/builders
################################################################################

--- ARQUIVO: backend/services/jinja/builders/affe_cara_elem_shell.py ---
def build_affe_cara_elem_shell(shell_list, model_name="MODELE", result_name="CARA_ELEM"):
    """
    Constrói a estrutura para AFFE_CARA_ELEM focada apenas em COQUE (Cascas).
    """
    items = []

    for item in shell_list:
        group = item["group"]
        thickness = item["thickness"]
        excentricity = item.get("excentricity", 0.0)
        # Vetor padrão para cascas
        vector = tuple(item.get("vector", [1.0, 0.0, 0.0]))

        items.append({
            "group": group,
            "epais": thickness,
            "vecteur": vector,
            "excentrement": excentricity,
            
            # Constantes fixas para este tipo de análise
            "a_cis": 0.8333333,
            "coef_rigi_drz": 1e-05,
            "coque_ncou": 1,
            "iner_rota": "OUI",
            "modi_metrique": "NON"
        })

    return {
        "result_name": result_name,
        "model_name": model_name,
        "items": items
    }

--- ARQUIVO: backend/services/jinja/builders/affe_char_meca_ddl.py ---
def build_affe_char_meca_ddl(ddl_list, model_name="MODELE", result_name="CHARGE_DDL"):
    """
    Constrói a estrutura para AFFE_CHAR_MECA focada EXCLUSIVAMENTE em DDL_IMPO.
    Retorna uma lista de dicionários, um para cada comando individual.
    """
    commands = []
    order_keys = ["DRX", "DRY", "DRZ", "DX", "DY", "DZ"]

    for item in ddl_list:
        raw_name = item.get("name", result_name)
        # Sanitize for Python variable compatibility
        name = raw_name.replace(" ", "_").replace("-", "_")
        input_params = item.get("dof") or item.get("params", {})
        
        active_params = []
        for key in order_keys:
            if key in input_params and input_params[key] is not None:
                active_params.append((key, input_params[key]))

        commands.append({
            "name": name,
            "group": item["group"],
            "params": active_params,
            "model_name": model_name,
            "double_lagrange": "OUI",
            "info": 1,
            "veri_affe": "OUI",
            "veri_norm": "OUI"
        })

    return commands

--- ARQUIVO: backend/services/jinja/builders/affe_materiau.py ---
def build_affe_materiau(assignments_list, model_name="MODELE", result_name="CHMAT"):
    """
    Constrói a estrutura para AFFE_MATERIAU baseada em um JSON de atribuições.
    assignments_list: Lista de dicts { "material": "NOME", "groups": [...] }
    """
    items = []

    for item in assignments_list:
        raw_mat_name = item["material"]
        groups = item.get("groups", [])
        
        if not groups:
            continue
            
        # Reconstrói o nome da variável igual ao DEFI (ex: ACIER -> M_ACIER)
        # Importante: A regra de nomeação deve ser idêntica ao builder do DEFI
        safe_name = raw_mat_name.upper().replace(" ", "_")
        var_name = f"M_{safe_name}"

        items.append({
            "mater": var_name,
            "groups": groups
        })

    return {
        "result_name": result_name,
        "model_name": model_name,
        "items": items
    }

--- ARQUIVO: backend/services/jinja/builders/affe_modele.py ---
def build_affe_modele(models_list, mesh_name="MAIL", result_name="MODELE"):
    """
    Prepara os dados para o comando AFFE_MODELE.
    """
    items = []
    
    for model in models_list:
        # Aqui você pode adicionar lógica extra, validações, etc.
        item = {
            "group": model["group"],
            "modelisation": model["type"],
            "phenomene": "MECANIQUE" # Padrão, mas poderia vir do JSON
        }
        items.append(item)

    return {
        "result_name": result_name,
        "mesh_name": mesh_name,
        "items": items
    }

--- ARQUIVO: backend/services/jinja/builders/asse_maillage.py ---
# =========================================================
# Builder: ASSE_MAILLAGE
# Junta N malhas em UMA
# =========================================================

def build_asse_maillage(mesh_names, result_name="MAIL"):
    if not isinstance(mesh_names, list):
        raise ValueError("mesh_names deve ser lista")

    if len(mesh_names) == 0:
        raise RuntimeError("FATAL ERROR: No mesh units defined.")

    # Se só uma malha, nem precisa ASSE_MAILLAGE
    if len(mesh_names) == 1:
        return {
            "mode": "SINGLE",
            "final_mesh": mesh_names[0],
            "result_name": mesh_names[0],
            "items": []
        }

    items = [{"mesh": m} for m in mesh_names]

    return {
        "mode": "ASSE",
        "result_name": result_name,
        "final_mesh": result_name,
        "items": items
    }

--- ARQUIVO: backend/services/jinja/builders/defi_materiau.py ---
def build_defi_materiau(materials_list):
    """
    Prepara os dados para o comando DEFI_MATERIAU.
    Retorna uma lista de dicionários contendo var_name, E, NU e RHO.
    """
    definitions = []

    for mat in materials_list:
        # Sanitização do nome para criar variável no Code_Aster
        # Ex: "Concrete C30" -> "M_CONCRETE_C30"
        clean_name = mat["name"].upper().replace(" ", "_")
        var_name = f"M_{clean_name}"

        props = mat # The unified config is flat
        
        definitions.append({
            "var_name": var_name,
            "E": props.get("E"),
            "NU": props.get("nu") or props.get("NU"),
            "RHO": props.get("rho") or props.get("RHO")
        })

    return definitions

--- ARQUIVO: backend/services/jinja/builders/force_coque.py ---
def build_force_coque(config, model_name="MODELE"):
    """
    Constrói dados para FORCE_COQUE com normalização de área.
    """
    foc_list = config.get("force_coque", [])
    if not foc_list:
        return {}, {} # Sem dados

    # 1. Configuração para o loop de cálculo de área (Python puro dentro do .comm)
    # Lista de tuplas: (nome, group_ma, total_force)
    press_config = []
    
    # 2. Configuração para o comando AFFE_CHAR_MECA
    # Lista de items de carga
    load_items = []

    for item in foc_list:
        name = item["name"]
        group = item["group"]
        force = item["total_force"]
        direction = item.get("direction", "PRES") # Default PRES, or FX, FY, FZ...

        # Adiciona à lista de cálculo
        press_config.append( (name, group, force) )

        # Constrói o item de carga que vai usar o valor calculado
        # O valor calculado estará em press_lookup[name][1]
        # Sintaxe JINJA vai inserir isso
        load_items.append({
            "name": name,
            "group": group,
            "direction": direction,
            # Placeholder para o template saber que deve pegar do dict
            "lookup_key": name 
        })

    calc_data = {
        "model_name": model_name,
        "press_config": press_config,
        # Precisamos de uma lista de TODOS os grupos envolvidos para criar o campo dummy corretamente?
        # O script original usa 'all_groups_ma'. Vamos passar a lista de grupos usados aqui.
        "groups": list(set([item["group"] for item in foc_list]))
    }

    load_data = {
        "model_name": model_name,
        "result_name": "CHARGE_FOC",
        "load_items": load_items,
        "double_lagrange": "OUI",
        "info": 1,
        "veri_affe": "OUI"
    }

    return calc_data, load_data

--- ARQUIVO: backend/services/jinja/builders/force_nodale.py ---
def build_force_nodale(force_nodale_list, model_name="MODELE", result_name="CHARGE_NOD"):
    """
    Constrói a configuração para FORCE_NODALE (Cargas Pontuais).
    """
    if not force_nodale_list:
        return {}

    items = []
    for item in force_nodale_list:
        if item.get("type") == "FORCE_NODALE":
            # Extrai componentes de força
            fz = item.get("fz", 0)
            fx = item.get("fx", 0)
            fy = item.get("fy", 0)
            group = item.get("group")
            
            if group and (float(fx) != 0 or float(fy) != 0 or float(fz) != 0):
                cmd = {
                    "GROUP_MA": group
                }
                if float(fx) != 0: cmd["FX"] = fx
                if float(fy) != 0: cmd["FY"] = fy
                if float(fz) != 0: cmd["FZ"] = fz
                
                items.append(cmd)

    if not items:
        return {}

    return {
        "result_name": result_name,
        "modele": model_name,
        "items": items
    }

--- ARQUIVO: backend/services/jinja/builders/geometry.py ---
def build_geometry(geometry_list, model_name="MODELE", result_name="CARA_ELEM"):
    """
    Unified builder for GEOMETRY.
    Processes Shells, Beams, and Volumes.
    Returns data for model assignment and element characteristics.
    """
    model_items = []
    cara_items = []

    for item in geometry_list:
        group = item["group"]
        element_type = item.get("type", "Solid").upper()
        
        # Skip Node groups for AFFE_MODELE assignment
        if element_type == "NODE" or item.get("_category") == "Node":
            continue

        # Determine type: use explicit section_type if available (New Frontend), else guess from type (Legacy)
        section_type = item.get("section_type", "").upper()
        # Fallback for legacy data or inferred types
        if not section_type:
            if element_type in ["SHELL", "DKT", "DST", "COQUE"]:
                section_type = "SHELL"
            elif element_type in ["BEAM", "POUTRE", "BARRE"]:
                section_type = "BEAM" # or RECTANGLE default?
            else:
                section_type = "SOLID"

        params = item.get("section_params", {})

        if section_type == "SHELL":
            # Model assignment
            model_items.append({
                "group": group,
                "modelisation": item.get("formulation", "DKT").upper()
            })
            # Cara assignment
            thickness = params.get("thickness") if params.get("thickness") is not None else item.get("thickness", 1.0)
            offset = params.get("offset") if params.get("offset") is not None else item.get("offset", 0.0)
            vx = params.get("vx") if params.get("vx") is not None else item.get("vx", 1.0)
            vy = params.get("vy") if params.get("vy") is not None else item.get("vy", 0.0)
            vz = params.get("vz") if params.get("vz") is not None else item.get("vz", 0.0)

            cara_items.append({
                "type": "COQUE",
                "group": group,
                "epais": thickness,
                "excentrement": offset,
                "vecteur": f"({vx}, {vy}, {vz})"
            })

        elif section_type in ["I_SECTION", "RECTANGLE", "BOX", "CIRCLE", "TUBE", "BEAM"]:
            # Model assignment
            model_items.append({
                "group": group,
                "modelisation": "POU_D_T"
            })
            
            # Cara assignment
            props = item.get("section_properties")
            if props:
                # 1. ADVANCED MAPPING: SectionProperties (SP) -> Code_Aster (CA)
                # CA local Y corresponds to SP Y (vertical in SP viewport)
                # CA local Z corresponds to SP X (horizontal in SP viewport)
                
                area = props.get("Area (A)", 1.0)
                # Use Nodal Inertias (already includes Steiner offset to 0,0)
                iy = props.get("Iyy (Node 0,0)", 1.0) # Moment about Node Y (SP Y)
                iz = props.get("Izz (Node 0,0)", 1.0) # Moment about Node Z (SP X)
                jx = props.get("Torsion J", 1.0)
                jg = props.get("Warping Iw", 0.0)
                
                # Shear coefficients: CA Ay = A / CA Ay'
                # CA Ay' (along local Y) = SP asy (along SP Y) -> recorded as "Shear Area Az" in calculator
                # CA Az' (along local Z) = SP asx (along SP X) -> recorded as "Shear Area Ay" in calculator
                as_y_ca = props.get("Shear Area Az", area)
                as_z_ca = props.get("Shear Area Ay", area)
                ay = area / as_y_ca if as_y_ca > 0 else 1.0
                az = area / as_z_ca if as_z_ca > 0 else 1.0
                
                # External fiber distances (RY, RZ) for stress calculation
                # Check for user-override from UI (fiber_y, fiber_z in section_params)
                f_y = float(params.get("fiber_y", 0) or 0)
                f_z = float(params.get("fiber_z", 0) or 0)
                
                if abs(f_y) > 1.0e-6 or abs(f_z) > 1.0e-6:
                    # User specified fiber coordinates - use them directly
                    ry = abs(f_y)
                    rz = abs(f_z)
                else:
                    # Default: use extreme fibers from section properties
                    ry = max(abs(props.get("Min Y", 0.0)), abs(props.get("Max Y", 0.0)))
                    rz = max(abs(props.get("Min X", 0.0)), abs(props.get("Max X", 0.0)))
                
                # Set minimum to avoid division by zero
                ry = max(ry, 1.0e-3)
                rz = max(rz, 1.0e-3)

                aster_section = "GENERALE"
                # Removed EY/EZ: properties already provided at the Node (Point of Interest)
                cara = "('A', 'IY', 'IZ', 'AY', 'AZ', 'JX', 'JG', 'RY', 'RZ')"
                vale = f"({area}, {iy}, {iz}, {ay}, {az}, {jx}, {jg}, {ry}, {rz})"
            
            else:
                # 2. LEGACY DATA / FALLBACK
                def get_p(k, default):
                    return params.get(k) if params.get(k) is not None else item.get(k, default)

                if section_type == "CIRCLE" or section_type == "TUBE":
                    aster_section = "CERCLE"
                    r = get_p("r", 50.0)
                    cara = "('R')"
                    vale = f"({r})"
                else:
                    aster_section = "RECTANGLE"
                    hy = get_p("hy", 100.0)
                    hz = get_p("hz", 50.0)
                    cara = "('HY', 'HZ')"
                    vale = f"({hy}, {hz})"
            
            cara_items.append({
                "type": "POUTRE",
                "group": group,
                "section": aster_section,
                "cara": cara,
                "vale": vale
            })

        else: # SOLID / VOLUME
            model_items.append({
                "group": group,
                "modelisation": "3D"
            })

    return {
        "model_items": model_items,
        "cara_items": cara_items,
        "model_result_name": model_name,
        "cara_result_name": result_name
    }

--- ARQUIVO: backend/services/jinja/builders/lire_maillage.py ---
# =========================================================
# Builder: LIRE_MAILLAGE (Code_Aster)
# Responsabilidade:
# - Validar dados
# - Gerar corpo do comando
# - Calcular UNITE automaticamente
# =========================================================

def build_lire_maillages(meshes, unit_start):
    """
    meshes: lista de dicts com:
        - name   : nome do concept (obrigatório)
        - format : MED | ASTER (opcional, default MED)

    unit_start: inteiro (ex: 80)
    """

    if not isinstance(meshes, list) or not meshes:
        raise ValueError("meshes deve ser uma lista não vazia")

    if not isinstance(unit_start, int):
        raise ValueError("unit_start deve ser inteiro")

    seen_names = set()
    maillages = []

    for idx, mesh in enumerate(meshes):
        if not isinstance(mesh, dict):
            raise ValueError("Cada mesh deve ser um dicionário")

        name = mesh.get("name")
        format = mesh.get("format", "MED")

        if not name or not isinstance(name, str):
            raise ValueError("Cada mesh precisa de um 'name' válido")

        if name in seen_names:
            raise ValueError(f"Nome de malha duplicado: {name}")
        seen_names.add(name)

        if format not in ("MED", "ASTER"):
            raise ValueError(
                f"Formato inválido para {name}: {format}"
            )

        unite = unit_start + idx

        body_lines = [
            f"    FORMAT='{format}',",
            f"    UNITE={unite},",
        ]

        maillages.append({
            "name": name,
            "body": "\n".join(body_lines)
        })

    return maillages

--- ARQUIVO: backend/services/jinja/builders/load_cases.py ---
def build_load_cases(lc_list, meca_config, pes_data, ddl_data, model_name="MODELE", reaction_extraction_data=None, **kwargs):
    """
    Constrói a lista de configurações de Load Cases (MECA_STATIQUE).
    """
    # Lista de NOMES válidos gerados pelos builds anteriores
    # ddl_data e pes_data são agora listas de configurações (cada uma tem 'name' ou 'result_name')
    valid_names = [d["name"] for d in ddl_data]
    
    if isinstance(pes_data, list):
        for p in pes_data:
            if "result_name" in p:
                valid_names.append(p["result_name"])
    elif pes_data and "result_name" in pes_data:
        # Fallback para objeto único
        valid_names.append(pes_data["result_name"])
    
    foc_data = kwargs.get("foc_data")
    if foc_data and "result_name" in foc_data:
        valid_names.append(foc_data["result_name"])

    nod_data = kwargs.get("nod_data")
    if nod_data and "result_name" in nod_data:
        valid_names.append(nod_data["result_name"])

    meca_runs = []
    
    if not lc_list:
        # Fallback se não houver LC, tenta usar o primeiro DDL disponível ou avisa
        default_load = valid_names[0] if valid_names else "CHARGE_DDL"
        lc_list = [{"name": "MECA", "loads": [default_load]}]

    base_config = meca_config.get("meca_statique", {})

    for lc in lc_list:
        raw_name = lc.get("name", "CASE")
        # Sanitize name: remove spaces and special chars for Python variable compatibility
        case_name = raw_name.replace(" ", "_").replace("-", "_")
        result_name = f"RESU_{case_name}"
        
        excit_list_lc = []
        
        # Merge loads and restrictions from the case definition
        combined_names = lc.get("loads", []) + lc.get("restrictions", [])
        
        for load_name in combined_names:
            # Validação: se o nome existe nos loads gerados
            if load_name in valid_names:
                excit_list_lc.append({
                    "charge": load_name,
                    "type_charge": "FIXE_CSTE"
                })

        # Prepara dados para o template load_cases.j2
        meca_runs.append({
            "case_name": case_name,
            "result_name": result_name,
            "cara_elem": base_config.get("cara_elem", "CARA_ELEM"),
            "cham_mater": base_config.get("cham_mater", "CHAM_MATER"),
            "modele": model_name,
            "excit_list": excit_list_lc,
            "option": base_config.get("option", "SIEF_ELGA"),
            "solveur": base_config.get("solveur", {}),
            "info": base_config.get("info", 1),
            "reaction_extraction": reaction_extraction_data
        })

    return {
        "runs": meca_runs
    }

--- ARQUIVO: backend/services/jinja/builders/meca_statique.py ---
def build_meca_statique(config, result_name="RESU"):
    """
    Constrói a estrutura para MECA_STATIQUE.
    """
    data = config.get("meca_statique", {})
    
    # Extrai listas e valores simples
    excit_list = data.get("excit", [])
    
    return {
        "result_name": result_name,
        "cara_elem": data.get("cara_elem"),
        "cham_mater": data.get("cham_mater"),
        "excit_list": excit_list,
        "modele": data.get("modele"),
        # Valores fixos/default conforme solicitação
        "info": 1,
        "inst": 0.0,
        "option": "SIEF_ELGA",
        "solveur": {
            "acceleration": "AUTO",
            "elim_lagr": "LAGR2",
            "gestion_memoire": "AUTO",
            "low_rank_seuil": 0.0,
            "matr_distribuee": "NON",
            "methode": "MUMPS",
            "nb_rhs": 1,
            "nprec": 8,
            "pcent_pivot": 35,
            "posttraitements": "AUTO",
            "pretraitements": "AUTO",
            "reduction_mpi": 0,
            "renum": "AUTO",
            "resi_rela": 1e-06,
            "stop_singulier": "OUI",
            "type_resol": "AUTO"
        }
    }

--- ARQUIVO: backend/services/jinja/builders/pesanteur.py ---
def build_pesanteur(config, model_name="MODELE", result_name="CHARGE_PES"):
    """
    Constrói a estrutura para uma lista de cargas de PESANTEUR em AFFE_CHAR_MECA.
    Retorna uma lista de configurações, uma para cada comando.
    """
    if isinstance(config, list):
        pes_list = config
    else:
        pes_list = config.get("pesanteur", [])
        if not isinstance(pes_list, list):
            # Fallback para o formato antigo
            if isinstance(pes_list, dict) and pes_list:
                pes_list = [pes_list]
            else:
                return []

    commands = []
    for item in pes_list:
        final_name = item.get("name", result_name)
        commands.append({
            "result_name": final_name,
            "model_name": model_name,
            "gravite": item.get("gravite", 9.81),
            "direction": tuple(item.get("direction", [0.0, 0.0, -1.0])),
            "group_ma": item.get("group_ma", None),
            "double_lagrange": "OUI",
            "info": 1,
            "veri_affe": "OUI",
            "veri_norm": "OUI"
        })

    return commands

--- ARQUIVO: backend/services/jinja/builders/post_elem_mass.py ---
def build_post_elem_mass(config, model_name="MODELE", field_mat_name="CHAM_MATER", cara_elem_name="CARA_ELEM"):
    """
    Builds data for POST_ELEM (MASS_INER) and IMPR_TABLE.
    """
    calculations = config.get("mass_calculations", [])
    
    # Professional Default: Always calculate global mass if not specified
    if not calculations:
        calculations = [{
            "result_name": "tab_mass",
            "title": "Global_Model_Mass",
            "export_title": "MASS_PROPERTIES"
        }]
    
    commands = []
    for item in calculations:
        commands.append({
            "result_name": item.get("result_name", "tab_mass"),
            "model_name": model_name,
            "field_mat_name": field_mat_name,
            "cara_elem_name": cara_elem_name,
            "title": item.get("title", "Physical_Mass_Structure"),
            "unit": item.get("unit", 26),
            "format": item.get("format", "TABLEAU"),
            "separator": item.get("separator", ","),
            "export_title": item.get("export_title", "MASS")
        })
        
    return commands

--- ARQUIVO: backend/services/jinja/builders/post_releve_t_reactions.py ---
def build_post_releve_t_reactions(config, ddl_list):
    """
    Builds data for reaction extraction.
    Identifies which groups have constraints (from ddl_list).
    """
    reac_config = config.get("reaction_extraction", {})
    
    # Professional default: If there are constraints, we usually want reactions
    # unless explicitly disabled in the config.
    enabled = reac_config.get("enabled", True) 
    
    if not enabled:
        return None

    # Get unique groups from ddl_list
    constrained_groups = list(set([item["group"] for item in ddl_list if "group" in item]))

    # Fallback to 'TOUT_NO' if no specific groups but enabled
    if not constrained_groups and enabled:
        constrained_groups = ['TOUT_NO']
    
    if not constrained_groups:
        return None

    return {
        "groups": constrained_groups,
        "unit": reac_config.get("unit", 27), # Fixed conflict with mass (26)
        "format": reac_config.get("format", "TABLEAU"),
        "separator": reac_config.get("separator", ","),
        "resultante": tuple(reac_config.get("extract_components", ["DX", "DY", "DZ"])),
        "moment": tuple(reac_config.get("moment_components", ["DRX", "DRY", "DRZ"]))
    }


################################################################################
# PASTA: backend/services/jinja/templates
################################################################################

--- ARQUIVO: backend/services/jinja/templates/affe_cara_elem.j2 ---
{{ cara_result_name }} = AFFE_CARA_ELEM(
    MODELE={{ model_result_name }},
    {% for item in cara_items %}
    {{ item.type }} = _F(
        GROUP_MA = '{{ item.group }}',
        {% if item.type == 'COQUE' %}
        EPAIS = {{ item.epais }},
        EXCENTREMENT = {{ item.excentrement }},
        VECTEUR = {{ item.vecteur }},
        INER_ROTA = 'OUI',
        {% elif item.type == 'POUTRE' %}
        SECTION = '{{ item.section }}',
        CARA = {{ item.cara }},
        VALE = {{ item.vale }},
        {% endif %}
    ),
    {% endfor %}
)

--- ARQUIVO: backend/services/jinja/templates/affe_cara_elem_shell.j2 ---
{{ result_name }} = AFFE_CARA_ELEM(
    MODELE={{ model_name }},
    INFO=1,
    COQUE=(
    {%- for item in items %}
        _F(
            GROUP_MA='{{ item.group }}',
            EPAIS={{ item.epais }},
            VECTEUR={{ item.vecteur }},
            EXCENTREMENT={{ item.excentrement }},
            A_CIS={{ item.a_cis }},
            COEF_RIGI_DRZ={{ item.coef_rigi_drz }},
            COQUE_NCOU={{ item.coque_ncou }},
            INER_ROTA='{{ item.iner_rota }}',
            MODI_METRIQUE='{{ item.modi_metrique }}'
        ),
    {%- endfor %}
    ),
);

--- ARQUIVO: backend/services/jinja/templates/affe_char_meca_ddl.j2 ---
{% for item in commands %}
{{ item.name }} = AFFE_CHAR_MECA(
    DDL_IMPO=_F(
    {%- for key, val in item.params %}
        {{ key }}={{ val }},
    {%- endfor %}
        GROUP_NO='{{ item.group }}'),
    DOUBLE_LAGRANGE='{{ item.double_lagrange }}',
    INFO={{ item.info }},
    MODELE={{ item.model_name }},
    VERI_AFFE='{{ item.veri_affe }}',
    VERI_NORM='{{ item.veri_norm }}',
);
{% endfor %}

--- ARQUIVO: backend/services/jinja/templates/affe_materiau.j2 ---
{{ result_name }} = AFFE_MATERIAU(
    MODELE={{ model_name }},
    AFFE=(
    {%- for item in items %}
        _F(
            GROUP_MA=({{ "'" + item.groups|join("', '") + "'" }}),
            MATER={{ item.mater }}
        ),
    {%- endfor %}
    ),
);

--- ARQUIVO: backend/services/jinja/templates/affe_modele.j2 ---
{{ result_name }} = AFFE_MODELE(
    MAILLAGE={{ mesh_name }},
    AFFE=(
    {%- for item in items %}
        _F(GROUP_MA='{{ item.group }}', PHENOMENE='{{ item.phenomene }}', MODELISATION='{{ item.modelisation }}'),
    {%- endfor %}
    ),
);

--- ARQUIVO: backend/services/jinja/templates/asse_maillage.j2 ---
{# 
    Lógica para N malhas:
    O Code_Aster ASSE_MAILLAGE funciona melhor de forma binária (1+1).
    Este loop pega a primeira malha física e vai 'superpondo' as demais
    sequencialmente, acumulando tudo na variável definida em {{ result_name }}.
#}

{% set first_mesh = items[0].mesh %}

{% for item in items[1:] %}

{{ result_name }} = ASSE_MAILLAGE(
    {# Se for a primeira iteração, une a Malha 0 com a Malha 1 #}
    {# Nas próximas, une o Resultado Acumulado (MAIL) com a Malha Atual #}
    MAILLAGE_1 = {% if loop.first %}{{ first_mesh }}{% else %}{{ result_name }}{% endif %},
    
    MAILLAGE_2 = {{ item.mesh }},
    
    OPERATION = 'SUPERPOSE',
);

{% endfor %}

--- ARQUIVO: backend/services/jinja/templates/defi_materiau.j2 ---
{% for mat in definitions %}
{{ mat.var_name }} = DEFI_MATERIAU(
    ELAS=_F(
        E={{ mat.E }},
        NU={{ mat.NU }},
        RHO={{ mat.RHO }}
    )
);
{% endfor %}

--- ARQUIVO: backend/services/jinja/templates/export.j2 ---
P actions make_etude
P rep_trav {{ temp_path }}
P memory_limit 2048
P time_limit 900.0
P version stable
P ncpus 1
P mpi_nbcpu 1
P mode interactif
F comm {{ comm_path }} D 1
F mess {{ message_path }} R 6
R base {{ base_path }} R 0
{% for mesh in meshes %}
F mmed {{ mesh.path }} D {{ mesh.unit }}
{% endfor %}
{% if resu_med_path %}
F resu {{ resu_med_path }} R 8
{% endif %}
{% if mass_csv_path %}
F dat {{ mass_csv_path }} R 26
{% endif %}
{% if reactions_csv_path %}
F dat {{ reactions_csv_path }} R 27
{% endif %}

--- ARQUIVO: backend/services/jinja/templates/extract_results.j2 ---
{% if runs %}
{% for run in runs %}
{% set shell_groups = cara_items | selectattr('type', 'equalto', 'COQUE') | map(attribute='group') | list %}
{% set beam_groups = cara_items | selectattr('type', 'equalto', 'POUTRE') | map(attribute='group') | list %}



{% if shell_groups %}
{{ run.result_name }} = CALC_CHAMP(reuse={{ run.result_name }},
                         RESULTAT={{ run.result_name }},
                         CONTRAINTE=('SIGM_ELNO',),
                         CRITERES=('SIEQ_ELNO',),
                         FORCE=('REAC_NODA', 'FORC_NODA'))



RESU_{{ run.case_name }}_INF = POST_CHAMP(RESULTAT={{ run.result_name }},
                                         GROUP_MA=({{ "'" + shell_groups|join("', '") + "'" }}),
                                         EXTR_COQUE=_F(NOM_CHAM='SIGM_ELNO', NUME_COUCHE=1, NIVE_COUCHE='INF'))

RESU_{{ run.case_name }}_INF = CALC_CHAMP(reuse=RESU_{{ run.case_name }}_INF,
                                         RESULTAT=RESU_{{ run.case_name }}_INF,
                                         CONTRAINTE=('SIGM_NOEU',),
                                         CRITERES=('SIEQ_NOEU',))



RESU_{{ run.case_name }}_MOY = POST_CHAMP(RESULTAT={{ run.result_name }},
                                         GROUP_MA=({{ "'" + shell_groups|join("', '") + "'" }}),
                                         EXTR_COQUE=_F(NOM_CHAM='SIGM_ELNO', NUME_COUCHE=1, NIVE_COUCHE='MOY'))

RESU_{{ run.case_name }}_MOY = CALC_CHAMP(reuse=RESU_{{ run.case_name }}_MOY,
                                         RESULTAT=RESU_{{ run.case_name }}_MOY,
                                         CONTRAINTE=('SIGM_NOEU',),
                                         CRITERES=('SIEQ_NOEU',))



RESU_{{ run.case_name }}_SUP = POST_CHAMP(RESULTAT={{ run.result_name }},
                                         GROUP_MA=({{ "'" + shell_groups|join("', '") + "'" }}),
                                         EXTR_COQUE=_F(NOM_CHAM='SIGM_ELNO', NUME_COUCHE=1, NIVE_COUCHE='SUP'))

RESU_{{ run.case_name }}_SUP = CALC_CHAMP(reuse=RESU_{{ run.case_name }}_SUP,
                                         RESULTAT=RESU_{{ run.case_name }}_SUP,
                                         CONTRAINTE=('SIGM_NOEU',),
                                         CRITERES=('SIEQ_NOEU',))



IMPR_RESU(FORMAT='MED',
          UNITE=8,
          RESU=(
              _F(RESULTAT={{ run.result_name }},
                 NOM_CHAM=('DEPL', 'REAC_NODA', 'FORC_NODA'),
                 NOM_CHAM_MED=('DEPL_{{ run.case_name }}', 'REAC_{{ run.case_name }}', 'FORC_{{ run.case_name }}')),
              _F(RESULTAT=RESU_{{ run.case_name }}_INF,
                 NOM_CHAM=('SIGM_NOEU', 'SIEQ_NOEU'),
                 NOM_CHAM_MED=('S_INF_{{ run.case_name }}', 'VM_INF_{{ run.case_name }}')),
              _F(RESULTAT=RESU_{{ run.case_name }}_MOY,
                 NOM_CHAM=('SIGM_NOEU', 'SIEQ_NOEU'),
                 NOM_CHAM_MED=('S_MID_{{ run.case_name }}', 'VM_MID_{{ run.case_name }}')),
              _F(RESULTAT=RESU_{{ run.case_name }}_SUP,
                 NOM_CHAM=('SIGM_NOEU', 'SIEQ_NOEU'),
                 NOM_CHAM_MED=('S_SUP_{{ run.case_name }}', 'VM_SUP_{{ run.case_name }}')),
          ))
{% elif beam_groups %}
{# Para vigas: usar SIPO_ELNO (tensao detalhada) e SIRO_ELEM (tensao max fibra) #}
{{ run.result_name }} = CALC_CHAMP(reuse={{ run.result_name }},
                         RESULTAT={{ run.result_name }},
                         CONTRAINTE=('SIPO_ELNO', 'SIRO_ELEM'),
                         FORCE=('REAC_NODA', 'FORC_NODA'))



IMPR_RESU(FORMAT='MED',
          UNITE=8,
          RESU=(
              _F(RESULTAT={{ run.result_name }},
                 NOM_CHAM=('DEPL', 'REAC_NODA', 'FORC_NODA'),
                 NOM_CHAM_MED=('DEPL_{{ run.case_name }}', 'REAC_{{ run.case_name }}', 'FORC_{{ run.case_name }}')),
              _F(RESULTAT={{ run.result_name }},
                 NOM_CHAM=('SIPO_ELNO', 'SIRO_ELEM'),
                 NOM_CHAM_MED=('SIPO_{{ run.case_name }}', 'SIRO_{{ run.case_name }}')),
          ))
{% else %}
{# Fallback para solidos 3D #}
{{ run.result_name }} = CALC_CHAMP(reuse={{ run.result_name }},
                         RESULTAT={{ run.result_name }},
                         CONTRAINTE=('SIGM_ELNO', 'SIGM_NOEU'),
                         CRITERES=('SIEQ_ELNO', 'SIEQ_NOEU'),
                         FORCE=('REAC_NODA', 'FORC_NODA'))



IMPR_RESU(FORMAT='MED',
          UNITE=8,
          RESU=(
              _F(RESULTAT={{ run.result_name }},
                 NOM_CHAM=('DEPL', 'REAC_NODA', 'FORC_NODA'),
                 NOM_CHAM_MED=('DEPL_{{ run.case_name }}', 'REAC_{{ run.case_name }}', 'FORC_{{ run.case_name }}')),
              _F(RESULTAT={{ run.result_name }},
                 NOM_CHAM=('SIGM_NOEU', 'SIEQ_NOEU'),
                 NOM_CHAM_MED=('SIGM_{{ run.case_name }}', 'SIEQ_{{ run.case_name }}')),
          ))
{% endif %}



tab_{{ run.case_name }} = POST_RELEVE_T(ACTION=_F(OPERATION='EXTRACTION',
                                                 INTITULE='REAC_{{ run.case_name }}',
                                                 RESULTAT={{ run.result_name }},
                                                 NOM_CHAM='REAC_NODA',
                                                 GROUP_NO='TOUT_NO',
                                                 RESULTANTE=('DX','DY','DZ'),
                                                 MOMENT=('DRX','DRY','DRZ'),
                                                 POINT=(0,0,0)))



IMPR_TABLE(TABLE=tab_{{ run.case_name }},
           UNITE=27,
           FORMAT='TABLEAU',
           SEPARATEUR=',',
           TITRE='REACTIONS_{{ run.case_name }}')



{% endfor %}
{% endif %}

--- ARQUIVO: backend/services/jinja/templates/force_coque_calc.j2 ---
# -----------------------------------------------------------------------
# FORCE TO PRESSURE CONVERSION LOGIC
# -----------------------------------------------------------------------
# Dados de entrada populados pelo Jinja
PRESS_CONFIG = {{ press_config }}
GROUPS_CALC = {{ groups }}

# Cria materiais dummy para cálculo
M_CALC = DEFI_MATERIAU(ELAS=_F(E=1.0, NU=0.3, RHO=1.0))
FIELD_CALC = AFFE_MATERIAU(
    MODELE={{ model_name }}, 
    AFFE=(
        _F(GROUP_MA=GROUPS_CALC, MATER=M_CALC),
    )
)
CARA_CALC = AFFE_CARA_ELEM(
    MODELE={{ model_name }}, 
    COQUE=(
        _F(GROUP_MA=GROUPS_CALC, EPAIS=1.0),
    )
)

press_lookup = {}
print("INFO: --- CONVERTING PRESSURE LOADS ---")
for p_name, p_group, p_force in PRESS_CONFIG:
    # Calcula geometria (mass_iner retorna massa = area * rho(1) * epais(1) = area)
    tbl_geom = POST_ELEM(
        MODELE={{ model_name }}, 
        CHAM_MATER=FIELD_CALC, 
        CARA_ELEM=CARA_CALC, 
        MASS_INER=_F(GROUP_MA=(p_group,))
    )
    val_dict = tbl_geom.EXTR_TABLE().values()
    # 'MASSE' é a chave usual para massa em POST_ELEM com MASS_INER
    # Verifica-se se é MASSE ou outra coisa, mas MASS_INER gera MASSE.
    area_val = sum(val_dict['MASSE'])
    
    if area_val > 0.0:
        pressure_pa = p_force / area_val
        print(f"      Load '{p_name}' on '{p_group}': {pressure_pa:.2f} Pa (Force: {p_force}, Area: {area_val:.2f})")
        press_lookup[p_name] = (p_group, pressure_pa)
    else:
        print(f"WARNING: Area is zero for group {p_group}. Force {p_name} set to 0.")
        press_lookup[p_name] = (p_group, 0.0)

--- ARQUIVO: backend/services/jinja/templates/force_coque_load.j2 ---
{{ result_name }} = AFFE_CHAR_MECA(
    MODELE={{ model_name }},
    FORCE_COQUE=(
    {%- for item in load_items %}
        _F(
            GROUP_MA='{{ item.group }}',
            # Usa o valor calculado no Python (press_lookup[nome][1])
            {{ item.direction }}=press_lookup['{{ item.name }}'][1]
        ),
    {%- endfor %}
    ),
    DOUBLE_LAGRANGE='{{ double_lagrange }}',
    INFO={{ info }},
    VERI_AFFE='{{ veri_affe }}',
);

--- ARQUIVO: backend/services/jinja/templates/force_nodale.j2 ---
{{ result_name }} = AFFE_CHAR_MECA(
    MODELE={{ modele }},
    FORCE_NODALE=(
        {%- for item in items %}
        _F(
            GROUP_MA='{{ item.GROUP_MA }}',
            {%- if item.FX is defined %}
            FX={{ item.FX }},
            {%- endif %}
            {%- if item.FY is defined %}
            FY={{ item.FY }},
            {%- endif %}
            {%- if item.FZ is defined %}
            FZ={{ item.FZ }},
            {%- endif %}
        ),
        {%- endfor %}
    ),
);

--- ARQUIVO: backend/services/jinja/templates/geometric_check.j2 ---

M_DUMMY = DEFI_MATERIAU(ELAS=_F(E=1.0, NU=0.3, RHO=1.0))

CH_DUMMY = AFFE_MATERIAU(MODELE={{ model_name }},
                         AFFE=_F(TOUT='OUI', MATER=M_DUMMY))

TAB_GEOM = POST_ELEM(MODELE={{ model_name }},
                    CHAM_MATER=CH_DUMMY,
                    CARA_ELEM={{ cara_elem_name }},
                    MASS_INER=_F(TOUT='OUI'))

IMPR_TABLE(TABLE=TAB_GEOM,
           UNITE=26,
           FORMAT='TABLEAU',
           SEPARATEUR=',',
           TITRE='GEOMETRIC_PROPERTIES')

--- ARQUIVO: backend/services/jinja/templates/inspect_mesh.j2 ---
# -----------------------------------------------------------------------------
# 3. DIAGNOSTICO DETALHADO + EXPORTACAO JSON (INJETADO VIA JINJA)
# -----------------------------------------------------------------------------
import sys
import json
import os

def imprimir_na_tela(texto):
    """Forca a impressao no console"""
    sys.stdout.write(texto + "\n")
    sys.stdout.flush()

# Cabecalho da tabela no terminal
print("\n" + "="*90)
print(f"{'NOME DO GRUPO':<30} | {'TOTAL':<8} | {'COMPOSICAO (TIPO:QTD)'}")
print("="*90)

# Dicionario que vai guardar os dados para o JSON
dados_output = {
    "source_mesh": "Code_Aster_Inspection",
    "groups": {}
}

# 1. Pega lista de grupos da malha '{{ mesh_variable }}'
lista_grupos = {{ mesh_variable }}.getGroupsOfCells()
lista_grupos.sort()

for g_nome in lista_grupos:
    # 2. Pega IDs e quantidade
    elementos_ids = {{ mesh_variable }}.getCells(g_nome)
    total_elementos = len(elementos_ids)
    
    # 3. Conta tipos
    contagem_tipos = {}
    for elem_id in elementos_ids:
        tipo = {{ mesh_variable }}.getCellTypeName(elem_id)
        if tipo in contagem_tipos:
            contagem_tipos[tipo] += 1
        else:
            contagem_tipos[tipo] = 1
            
    # 4. Formata para o terminal
    lista_composicao = []
    for tipo, qtd in contagem_tipos.items():
        lista_composicao.append(f"{tipo}:{qtd}")
    str_composicao = ", ".join(lista_composicao)
    
    print(f"{g_nome:<30} | {total_elementos:<8} | {str_composicao}")
    
    # 5. Guarda no dicionario
    dados_output["groups"][g_nome] = {
        "count": total_elementos,
        "types": contagem_tipos
    }

# 6. Processamento de Grupos de NOS (Nodes)
lista_grupos_nos = {{ mesh_variable }}.getGroupsOfNodes()
lista_grupos_nos.sort()

for g_nome in lista_grupos_nos:
    # A API getNodes pede uma lista de nomes [g_nome]
    nos_ids = {{ mesh_variable }}.getNodes([g_nome])
    total_nos = len(nos_ids)
    
    # Formata para o terminal
    str_composicao = f"Node:{total_nos}"
    print(f"{g_nome:<30} | {total_nos:<8} | {str_composicao}")
    
    # Guarda no dicionario
    # O tipo 'Node' eh usado para o JS detectar corretamente na UI
    dados_output["groups"][g_nome] = {
        "count": total_nos,
        "types": {"Node": total_nos}
    }

print("="*90 + "\n")

# -----------------------------------------------------------------------------
# SALVANDO O MESH_GROUPS.JSON (CAMINHO ABSOLUTO INJETADO)
# -----------------------------------------------------------------------------
try:
    # A variavel output_folder e injetada pelo script Python inspect_mesh.py
    # Usamos string raw (r) para evitar problemas com barras
    pasta_destino = r"{{ output_folder }}"
    
    # Garante que a pasta existe (embora o Python main ja tenha criado)
    if not os.path.exists(pasta_destino):
        os.makedirs(pasta_destino)
    
    # Define o caminho completo do arquivo JSON
    caminho_json = os.path.join(pasta_destino, "mesh_groups.json")
    
    with open(caminho_json, 'w') as f:
        json.dump(dados_output, f, indent=4)
        
    imprimir_na_tela(f"[ASTER] Resultado da inspecao salvo em:\n{caminho_json}")

except Exception as e:
    imprimir_na_tela(f"[ERRO] Falha ao salvar mesh_groups.json: {e}")

--- ARQUIVO: backend/services/jinja/templates/lire_maillage.j2 ---
{{ mesh_name }} = LIRE_MAILLAGE(
    UNITE={{ unit }},
    FORMAT='MED',
    NOM_MED='{{ mesh_name }}'
);

--- ARQUIVO: backend/services/jinja/templates/load_cases.j2 ---
{{ result_name }} = MECA_STATIQUE(
    CARA_ELEM={{ cara_elem }},
    CHAM_MATER={{ cham_mater }},
    EXCIT=(
    {%- for excit in excit_list %}
        _F(CHARGE={{ excit.charge }}, TYPE_CHARGE='{{ excit.type_charge }}'),
    {%- endfor %}
    ),
    INFO={{ info }},
    MODELE={{ modele }},
    OPTION='{{ option }}',
    SOLVEUR=_F(
        {%- for key, val in solveur.items() %}
        {{ key }}={{ val|tojson if val is string else val }},
        {%- endfor %}
    ),
);



{% if reaction_extraction %}
{{ result_name }} = CALC_CHAMP(reuse={{ result_name }},
                         RESULTAT={{ result_name }},
                         FORCE=('REAC_NODA',))



REAC_{{ name }} = POST_RELEVE_T(ACTION=_F(OPERATION='EXTRACTION',
                                          INTITULE='Reac_{{ name }}',
                                          RESULTAT={{ result_name }},
                                          NOM_CHAM='REAC_NODA',
                                          GROUP_NO=({% for g in reaction_extraction.groups %}'{{ g }}'{{ ', ' if not loop.last }}{% endfor %}),
                                          RESULTANTE=('DX','DY','DZ'),
                                          MOMENT=('DRX','DRY','DRZ'),
                                          POINT=(0,0,0)))



IMPR_TABLE(TABLE=REAC_{{ name }},
           UNITE={{ reaction_extraction.unit }},
           FORMAT='{{ reaction_extraction.format }}',
           SEPARATEUR='{{ reaction_extraction.separator }}',
           TITRE='REAC_{{ name }}')
{% endif %}

--- ARQUIVO: backend/services/jinja/templates/meca_statique.j2 ---
{{ result_name }} = MECA_STATIQUE(
    CARA_ELEM={{ cara_elem }},
    CHAM_MATER={{ cham_mater }},
    MODELE={{ modele }},
    EXCIT=(
        {%- for item in excit_list %}
        _F(CHARGE={{ item.charge }}, TYPE_CHARGE='{{ item.type_charge }}'),
        {%- endfor %}
    ),
    SOLVEUR=_F(METHODE='{{ solveur.get('methode', 'MULT_FRONT') }}'),
)

--- ARQUIVO: backend/services/jinja/templates/pesanteur.j2 ---
{% for item in commands %}
{{ item.result_name }} = AFFE_CHAR_MECA(
    MODELE={{ item.model_name }},
    PESANTEUR=_F(
        GRAVITE={{ item.gravite }},
        DIRECTION={{ item.direction }},
        {%- if item.group_ma %}
        GROUP_MA='{{ item.group_ma }}',
        {%- endif %}
    ),
    DOUBLE_LAGRANGE='{{ item.double_lagrange }}',
    INFO={{ item.info }},
    VERI_AFFE='{{ item.veri_affe }}',
    VERI_NORM='{{ item.veri_norm }}',
);
{% endfor %}

--- ARQUIVO: backend/services/jinja/templates/post_elem_mass.j2 ---
{% for cmd in commands %}
{{ cmd.result_name }} = POST_ELEM(MODELE={{ cmd.model_name }}, 
                       CHAM_MATER={{ cmd.field_mat_name }}, 
                       CARA_ELEM={{ cmd.cara_elem_name }},
                       MASS_INER=_F(TOUT='OUI'), 
                       TITRE='{{ cmd.title }}')

IMPR_TABLE(TABLE={{ cmd.result_name }}, 
           UNITE={{ cmd.unit }}, 
           FORMAT='{{ cmd.format }}', 
           SEPARATEUR='{{ cmd.separator }}', 
           TITRE='{{ cmd.export_title }}')
{% endfor %}

--- ARQUIVO: backend/services/jinja/templates/preamble.j2 ---
DEFI_FICHIER(UNITE=8, TYPE='BINARY')




################################################################################
# PASTA: backend/services/med
################################################################################

--- ARQUIVO: backend/services/med/call_med_mesher.py ---
import subprocess
import os
import sys
from services.med.vtk_extruder import run_processing

# 1. CONFIGURAÇÃO DE CAMINHOS (Relativos ao local deste script)
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
ROOT_DIR = os.path.abspath(os.path.join(BASE_DIR, "..", "..", ".."))

def run_batch_med_processing(target_folder):
    """
    Recebe um endereço completo, identifica arquivos .med e processa cada um,
    apenas se o arquivo .json correspondente ainda não existir.
    """
    if not os.path.exists(target_folder):
        print(f"[ERRO] O diretório informado não existe: {target_folder}")
        return

    med_env_dir = os.path.join(ROOT_DIR, "MEDCOUPLING-9.15.0", "MEDCOUPLING-9.15.0")
    mesher_script = os.path.join(ROOT_DIR, "backend", "services", "med", "med_mesher.py")

    if not os.path.exists(med_env_dir):
        print(f"[ERRO] Ambiente MEDCOUPLING não encontrado em: {med_env_dir}")
        return

    # 2. IDENTIFICAÇÃO DOS ARQUIVOS .MED
    med_files = [f for f in os.listdir(target_folder) if f.lower().endswith('.med')]
    
    if not med_files:
        print(f"[AVISO] Nenhum arquivo .med encontrado em: {target_folder}")
        return

    print(f"--- Batch Processing Iniciado ---")
    print(f"Raiz do Projeto: {ROOT_DIR}")
    print(f"Diretório Alvo: {target_folder}")
    print("-" * 40)

    # 3. LOOP DE PROCESSAMENTO
    for index, filename in enumerate(med_files, 1):
        # Extrai o nome sem extensão e define o caminho do JSON esperado
        base_name = os.path.splitext(filename)[0]
        json_filename = base_name + ".json"
        
        full_med_path = os.path.join(target_folder, filename)
        full_json_path = os.path.join(target_folder, json_filename)

        # VERIFICAÇÃO: Se o JSON existe, pula para o próximo
        if os.path.exists(full_json_path):
            print(f"[{index}/{len(med_files)}] Ignorando: {filename} (JSON já existe)")
            continue
        
        print(f"[{index}/{len(med_files)}] Processando: {filename}...")

        # Montagem do comando CMD
        command = (
            f'cmd /c "cd /d "{med_env_dir}" && '
            f'call env_launch.bat && '
            f'cd /d "{ROOT_DIR}" && '
            f'python "{mesher_script}" "{full_med_path}""'
        )

        try:
            subprocess.run(command, shell=True, check=True, capture_output=True, text=True)
            print(f"   [OK] Sucesso: {json_filename} gerado.")
        except subprocess.CalledProcessError as e:
            print(f"   [FALHA] Erro ao processar {filename}")
            print(f"   Detalhes: {e.stderr}")

    print("-" * 40)
    print(f"--- Batch Processing Finalizado ---")

    run_processing(target_folder)    



if __name__ == "__main__":
    if len(sys.argv) > 1:
        input_path = sys.argv[1]
    else:
        input_path = os.path.join(ROOT_DIR, "testcases", "hibrido")

    run_batch_med_processing(input_path)

--- ARQUIVO: backend/services/med/debug_api.py ---
import sys
import os

def list_all(mod_name):
    print(f"\n--- Listing {mod_name} ---")
    try:
        mod = __import__(mod_name)
        attrs = [a for a in dir(mod) if not a.startswith('_')]
        print(f"Count: {len(attrs)}")
        # Look for anything containing 'Loader' or 'Loader'
        matches = [a for a in attrs if 'Loader' in a or 'Read' in a]
        print(f"Potential loaders: {matches}")
        if 'MEDLoader' in attrs:
            obj = getattr(mod, 'MEDLoader')
            print(f"MEDLoader type: {type(obj)}")
    except Exception as e:
        print(f"Error {mod_name}: {e}")

list_all('medcoupling')
list_all('MEDLoader')

--- ARQUIVO: backend/services/med/debug_env.py ---
import sys
import os
import json

print("\n--- PYTHON ENVIRONMENT DEBUG ---")
print(f"Executable: {sys.executable}")
print(f"Path: {sys.path}")
print(f"PYTHONPATH: {os.environ.get('PYTHONPATH', 'NOT SET')}")

try:
    import medcoupling
    print(f"SUCCESS: medcoupling imported from {medcoupling.__file__}")
    from medcoupling import MEDLoader
    print("SUCCESS: MEDLoader imported")
except Exception as e:
    print(f"ERROR: medcoupling import failed: {e}")

print("--- END DEBUG ---\n")

--- ARQUIVO: backend/services/med/debug_env_deep.py ---
import sys
import os

print("\n--- DEEP INSPECT ---")
try:
    import medcoupling
    print(f"medcoupling found: {medcoupling.__file__}")
    print("Top-level attributes:", [a for a in dir(medcoupling) if not a.startswith('_')][:20], "...")
    
    if hasattr(medcoupling, 'MEDLoader'):
        print("SUCCESS: MEDLoader found in medcoupling")
    else:
        print("FAIL: MEDLoader NOT in medcoupling")
        
    # Try importing directly
    try:
        import MEDLoader
        print("SUCCESS: MEDLoader found as top-level module")
    except:
        print("FAIL: MEDLoader NOT a top-level module")
        
except Exception as e:
    print(f"CRITICAL: {e}")

print("--- END INSPECT ---\n")

--- ARQUIVO: backend/services/med/debug_medloader.py ---
import sys
import os

print("\n--- MEDLOADER MODULE INSPECT ---")
try:
    import MEDLoader
    print(f"MEDLoader module file: {MEDLoader.__file__}")
    print("Attributes:", [a for a in dir(MEDLoader) if not a.startswith('_')][:30])
except Exception as e:
    print(f"CRITICAL: {e}")
print("--- END ---")

--- ARQUIVO: backend/services/med/med_extractor.py ---
import sys
import os
import json

# ==============================================================================
# MED_EXTRACTOR.PY - PROTOCOLO DE MISSÃO: PONTE DE DADOS (MED -> GLOBAL STATE)
# Objetivo: Gerar o "DNA" da malha para o window.projectState do Frontend.
# Regra: Agnóstico, formatado para consumo via API.
# Ambiente: MEDCOUPLING 9.15.0
# ==============================================================================

try:
    import MEDLoader as ml
    import medcoupling as mc
    import numpy as np
except ImportError:
    # Fail-safe para ambientes externos ao MEDCoupling
    pass

def map_med_to_vtk_protocol(mc_type):
    """Mapeamento rigoroso conforme tabelas VTK."""
    mapping = {
        mc.NORM_SEG2: 3,   # VTK_LINE
        mc.NORM_TRI3: 5,   # VTK_TRIANGLE
        mc.NORM_QUAD4: 9,  # VTK_QUAD
        mc.NORM_TETRA4: 10, # VTK_TETRA
        mc.NORM_HEXA8: 12  # VTK_HEXAHEDRON
    }
    return mapping.get(mc_type, 0)

def extract_med_data(file_path):
    """
    DNA Extractor: Transforma arquivo MED em JSON compatível com Estado Global.
    """
    if not os.path.exists(file_path):
        return {"status": "error", "message": f"File not found: {file_path}"}
        
    try:
        # ETAPA 1: LEITURA HIERÁRQUICA
        mesh_names = ml.GetMeshNames(file_path)
        if not mesh_names:
            return {"status": "error", "message": "No meshes found."}
        
        mesh_name = mesh_names[0]
        group_names = ml.GetMeshGroupsNames(file_path, mesh_name)
        
        results = {}
        
        for g_name in group_names:
            # ETAPA 2: ITERAÇÃO E CLASSIFICAÇÃO DE GRUPOS
            try:
                # 1. Tentar carregar como malha de células (Nível 0)
                sub_mesh = ml.ReadUMeshFromGroups(file_path, mesh_name, 0, [g_name])
                num_cells = sub_mesh.getNumberOfCells()
                
                if num_cells > 0:
                    d_mesh = sub_mesh.getMeshDimension()
                    # Inferência de Categoria (Protocolo)
                    category = {3: "3D", 2: "2D", 1: "1D"}.get(d_mesh, str(d_mesh) + "D")
                    
                    # VTK Type Mapping (da primeira célula)
                    mc_type = sub_mesh.getTypeOfCell(0)
                    vtk_id = map_med_to_vtk_protocol(mc_type)
                    
                    # Otimização de Arrays: Points e Connectivity
                    points = sub_mesh.getCoords().toNumPyArray().flatten().tolist()
                    connectivity = sub_mesh.getNodalConnectivity().toNumPyArray().flatten().tolist()
                    
                    # Normais Condicionais (Protocolo: Apenas 2D)
                    normals = None
                    if category == "2D":
                        try:
                            norm_field = sub_mesh.buildOrthogonalField()
                            normals = norm_field.getArray().toNumPyArray().flatten().tolist()
                        except:
                            normals = None
                    
                    results[g_name] = {
                        "dimension": int(d_mesh),
                        "count": int(num_cells),
                        "category": category,
                        "type_vtk": int(vtk_id),
                        "points": points,
                        "connectivity": connectivity,
                        "normals": normals
                    }
                    continue # Sucesso como grupo de células

            except:
                pass # Pode ser um grupo de nós

            # 2. Tentar carregar como grupo de nós (Se necessário para Restrictions)
            try:
                # Nota: Em algumas versões, grupos de nós são lidado diferentemente. 
                # Se d_mesh falhar acima, classificamos como Node aqui.
                # Para simplificar e seguir o protocolo:
                mfile = ml.MEDFileUMesh.New(file_path, mesh_name)
                # Verifica se g_name é um grupo de nós
                node_arr = mfile.getNodeGroupArr(g_name)
                if node_arr and node_arr.getNumberOfTuples() > 0:
                    results[g_name] = {
                        "dimension": 0,
                        "count": int(node_arr.getNumberOfTuples()),
                        "category": "Node",
                        "type_vtk": 1, # VTK_VERTEX
                        "points": [], # No global state, nodes groups usually reference global points
                        "connectivity": node_arr.toNumPyArray().flatten().tolist(), # IDs dos nós
                        "normals": None
                    }
            except:
                continue

        return {
            "status": "success",
            "filename": os.path.basename(file_path),
            "data": {
                "mesh_name": mesh_name,
                "groups": results
            }
        }
        
    except Exception as e:
        import traceback
        return {
            "status": "error",
            "message": str(e),
            "traceback": traceback.format_exc()
        }

def extract_to_dict(file_path):
    """Protocol Alias for extract_med_data"""
    return extract_med_data(file_path)

if __name__ == "__main__":
    if len(sys.argv) < 2:
        sys.exit(1)
    
    input_med = sys.argv[1]
    output_json = sys.argv[2] if len(sys.argv) > 2 else None
    
    result_data = extract_med_data(input_med)
    
    # OUTPUT DUPLO
    # 1. Stdout Minificado (Produção)
    print(json.dumps(result_data, separators=(',', ':')))
    
    # 2. Arquivo (Opcional - Debug)
    if output_json:
        with open(output_json, 'w', encoding='utf-8') as f:
            json.dump(result_data, f, indent=2)

--- ARQUIVO: backend/services/med/med_extruder.py ---
import sys
import os
import json

# Modular MED Extruder
# Purpose: High-performance shell extrusion and boundary extraction.
# Runs in MEDCOUPLING 9.15.0 environment (Python 3.9)

try:
    import MEDLoader as ml
    import medcoupling as mc
    import numpy as np
except ImportError as e:
    print(json.dumps({"status": "error", "message": f"MEDCOUPLING modules not found: {e}"}))
    sys.exit(1)

def get_aster_cell_type(mc_type):
    mapping = {
        mc.NORM_SEG2: "SEG2", mc.NORM_SEG3: "SEG3",
        mc.NORM_TRI3: "TRI3", mc.NORM_TRI6: "TRI6",
        mc.NORM_QUAD4: "QUAD4", mc.NORM_QUAD8: "QUAD8",
        mc.NORM_TETRA4: "TETRA4", mc.NORM_TETRA10: "TETRA10",
        mc.NORM_HEXA8: "HEXA8", mc.NORM_HEXA20: "HEXA20",
        mc.NORM_PENTA6: "PENTA6", mc.NORM_PENTA15: "PENTA15",
        mc.NORM_PYRA5: "PYRA5", mc.NORM_PYRA13: "PYRA13",
        mc.NORM_POINT1: "POIN1"
    }
    return mapping.get(mc_type, f"TYPE_{mc_type}")

def perform_extrusion(file_path, geometries=None):
    if not os.path.exists(file_path):
        return {"status": "error", "message": f"File not found: {file_path}"}
        
    geom_map = {}
    if geometries:
        for g in geometries:
            group = g.get('group')
            if group:
                geom_map[group.strip().upper()] = g
                
    try:
        mesh_names = ml.GetMeshNames(file_path)
        if not mesh_names:
            return {"status": "error", "message": "No meshes found in file"}
            
        mesh_name = mesh_names[0]
        mfile = ml.MEDFileUMesh.New(file_path, mesh_name)
        all_groups = mfile.getGroupsNames()
        
        # We will collect sub-meshes for a SINGLE MergeUMeshes call at the end
        submeshes_to_merge = []
        group_meta = [] # List of (final_name, is_extruded)
        
        for g_name in all_groups:
            # 1. ATTEMPT CELL GROUP
            try:
                g_arr = mfile.getGroupArr(0, g_name)
                if g_arr.getNumberOfTuples() > 0:
                    # Read this group sub-mesh
                    group_mesh = ml.ReadUMeshFromGroups(file_path, mesh_name, 0, [g_name])
                    if group_mesh.getSpaceDimension() == 2:
                        group_mesh.changeSpaceDimension(3)
                    
                    # SYSTEMATIC CHECK: Should we extrude this group?
                    geom = geom_map.get(g_name.strip().upper())
                    is_shell = False
                    thickness = 0.0
                    offset_val = 0.0
                    
                    if geom:
                        params = geom.get('section_params', {})
                        thickness = float(params.get('thickness', 0.0))
                        offset_val = float(params.get('offset', 0.0))
                        model_type = str(geom.get('type', '')).upper()
                        is_shell = model_type in ('DKT', 'DST', 'COQUE_3D', 'MEMBRANE') or \
                                   'SHELL' in model_type or 'QUAD' in model_type or 'TRIA' in model_type
                    
                    if is_shell and thickness > 0:
                        # --- THE 2-COMMAND SYSTEMATIC ---
                        
                        # Command A: Create 1D Path Mesh for extrusion
                        z_start = offset_val - thickness/2.0
                        z_end = offset_val + thickness/2.0
                        path_coords = mc.DataArrayDouble([0,0,z_start, 0,0,z_end], 2, 3)
                        path_mesh = mc.MEDCouplingUMesh("Path", 1)
                        path_mesh.setCoords(path_coords)
                        path_mesh.allocateCells(1)
                        path_mesh.insertNextCell(mc.NORM_SEG2, [0, 1])
                        
                        # Command 1: EXTRUDE (Generates volumes/quals depending on input)
                        vol_mesh = group_mesh.buildExtrudedMesh(path_mesh, 0)
                        
                        # Command 2: SKIN (Extracts boundary surfaces only)
                        skin_mesh = vol_mesh.buildBoundaryMesh(False) # False = independent mesh
                        
                        # Add Original to merge list
                        submeshes_to_merge.append(group_mesh)
                        group_meta.append((g_name, False))
                        
                        # Add Extruded Skin to merge list
                        submeshes_to_merge.append(skin_mesh)
                        group_meta.append((g_name + "_EXTRUDED", True))
                    else:
                        # Regular group without extrusion
                        submeshes_to_merge.append(group_mesh)
                        group_meta.append((g_name, False))
                    continue # Succeeded with cell group
            except: pass
            
            # 2. ATTEMPT NODE GROUP
            try:
                node_arr = mfile.getNodeGroupArr(g_name)
                if node_arr.getNumberOfTuples() > 0:
                    # We don't merge node groups into the UMesh, but we track them for counts
                    # Actually MergeUMeshes only works for meshes with cells.
                    # We'll handle node groups as special metadata
                    pass
            except: pass

        # NATIVE AGGREGATION: Single call to MergeUMeshes
        if not submeshes_to_merge:
            return {"status": "error", "message": "No meshes to merge"}
            
        merged_mesh = mc.MEDCouplingUMesh.MergeUMeshes(submeshes_to_merge)
        
        all_points = merged_mesh.getCoords().toNumPyArray().tolist()
        merged_con = merged_mesh.getNodalConnectivity().toNumPyArray().flatten()
        merged_idx = merged_mesh.getNodalConnectivityIndex().toNumPyArray().flatten()
        
        cells_output = {}
        cell_ptr = 0
        
        for i, (final_name, is_ext) in enumerate(group_meta):
            mesh = submeshes_to_merge[i]
            n_c = mesh.getNumberOfCells()
            connectivity = []
            
            # Identify VTK type
            mc_type = mesh.getTypeOfCell(0)
            vtk_type = "unknown"
            if mc_type == mc.NORM_SEG2: vtk_type = "line"
            elif mc_type == mc.NORM_TRI3: vtk_type = "triangle"
            elif mc_type == mc.NORM_QUAD4: vtk_type = "quad"
            
            for _ in range(n_c):
                start, end = int(merged_idx[cell_ptr]), int(merged_idx[cell_ptr+1])
                connectivity.append(merged_con[start:end].tolist())
                cell_ptr += 1
                
            cells_output[final_name] = {
                "type": vtk_type,
                "connectivity": connectivity,
                "count": n_c,
                "is_extruded": is_ext
            }
            
        return {
            "status": "success",
            "points": all_points,
            "cells": cells_output,
            "num_points": len(all_points),
            "num_groups": len(cells_output)
        }
    except Exception as e:
        import traceback
        return {"status": "error", "message": str(e), "traceback": traceback.format_exc()}

if __name__ == "__main__":
    if len(sys.argv) < 2: sys.exit(1)
    geos = None
    if len(sys.argv) > 2:
        try:
            with open(sys.argv[2], 'r') as f:
                d = json.load(f)
                geos = d if isinstance(d, list) else d.get("geometries")
        except: pass
    print(json.dumps(perform_extrusion(sys.argv[1], geos)))

--- ARQUIVO: backend/services/med/med_mesher.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MedMesher Standalone (Background)
Processes .med files in testcases/hibrido and generates individual JSON files.
Systeem: 1 Call per Mesh, 1 JSON per Mesh.
"""
import sys
import os
import json
import traceback

try:
    import MEDLoader as ml
    import medcoupling as mc
    import numpy as np
except ImportError:
    ml = None
    mc = None

def extract_single_mesh(file_path):
    """Extracts points and structured connectivity using ml/mc module functions."""
    if not os.path.exists(file_path):
        return {"status": "error", "message": f"File not found: {file_path}"}
        
    if ml is None or mc is None:
        return {"status": "error", "message": "MEDCoupling modules (MEDLoader/medcoupling) not available"}

    try:
        # 1. LEITURA HIERÁRQUICA (Protocolo med_extractor)
        mesh_names = ml.GetMeshNames(file_path)
        if not mesh_names:
            return {"status": "error", "message": "No meshes found in MED file"}
        
        mesh_name = mesh_names[0]
        
        # Carregar Malha Completa (Nível 0)
        # ReadUMeshFromFile(fileName, meshName, iteration)
        mesh = ml.ReadUMeshFromFile(file_path, mesh_name, 0)
        
        if mesh is None:
            return {"status": "error", "message": "ReadUMeshFromFile returned None"}

        # 2. EXTRACT POINTS
        coords = mesh.getCoords()
        points = coords.toNumPyArray().flatten().tolist()

        # 3. EXTRACT CONNECTIVITY (Structured - No Flatten!)
        num_cells = mesh.getNumberOfCells()
        conn_obj = mesh.getNodalConnectivity()
        connectivity = []

        if conn_obj is not None:
            raw_conn = conn_obj.toNumPyArray()
            if num_cells > 0:
                if len(raw_conn.shape) > 1:
                    connectivity = raw_conn.tolist()
                else:
                    # Flat list -> element-by-element chunking
                    total_len = len(raw_conn)
                    nodes_per_elem_total = total_len // num_cells
                    flat_list = raw_conn.tolist()
                    
                    connectivity = []
                    for i in range(num_cells):
                        chunk = flat_list[i * nodes_per_elem_total : (i + 1) * nodes_per_elem_total]
                        # 🕸️ FIX SPIDERWEB (Enhanced): 
                        # Strip prefix if it exists. Common patterns:
                        # 1. Size prefix: [N, node1, ..., nodeN] (chunk[0] == N)
                        # 2. Type prefix: [TypeID, node1, ..., nodeN] (seen in solids, chunk[0] != N)
                        if len(chunk) > 1:
                            # If we have N+1 items, the first is almost certainly a prefix (Size or Type)
                            # Most standard finite elements (Beam2, Quad4, Hexa8) come in N+1 or N chunks.
                            # We assume N+1 indicates a prefix.
                            connectivity.append(chunk[1:])
                        else:
                            connectivity.append(chunk)

        # 4. EXTRACT NORMALS (Conditionals)
        normals = None
        try:
            # buildOrthogonalField calculates normals for the mesh
            norm_field = mesh.buildOrthogonalField()
            if norm_field:
                normals = norm_field.getArray().toNumPyArray().flatten().tolist()
        except:
            normals = None

        # 5. GET VTK TYPE
        # Use first cell type
        vtk_type = 5 # Default
        if num_cells > 0:
            try:
                mc_type = mesh.getTypeOfCell(0)
                # Mapping conform med_extractor
                mapping = {
                    mc.NORM_SEG2: 3, mc.NORM_TRI3: 5, mc.NORM_QUAD4: 9,
                    mc.NORM_TETRA4: 10, mc.NORM_HEXA8: 12
                }
                vtk_type = mapping.get(mc_type, 5)
            except: pass

        return {
            "status": "success",
            "filename": os.path.basename(file_path),
            "points": points,
            "connectivity": connectivity,
            "normals": normals,
            "vtk_type": vtk_type,
            "num_points": len(points) // 3,
            "num_elements": len(connectivity)
        }
    except Exception as e:
        return {"status": "error", "message": f"{type(e).__name__}: {str(e)}", "traceback": traceback.format_exc()}

def process_directory(dir_path):
    """Scans and processes all .med files in the directory."""
    print(f"\n[START] Scanning directory: {dir_path}")
    
    if not os.path.exists(dir_path):
        print(f"[ABORT] Path does not exist: {dir_path}")
        return

    files = [f for f in os.listdir(dir_path) if f.lower().endswith('.med') and 'resu' not in f.lower()]
    print(f"[FOUND] {len(files)} mesh files to process.\n")

    for f_name in files:
        full_path = os.path.join(dir_path, f_name)
        json_path = os.path.join(dir_path, f_name.replace('.med', '.json'))
        
        print(f"[PROCESS] Extracting: {f_name}...")
        result = extract_single_mesh(full_path)
        
        if result["status"] == "success":
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(result, f, separators=(',', ':'))
            print(f"[SUCCESS] Saved to: {os.path.basename(json_path)} ({result['num_points']} nodes, {result['num_elements']} elements)")
        else:
            print(f"[ERROR] Failed {f_name}: {result['message']}")

if __name__ == "__main__":
    if len(sys.argv) > 1:
        path = sys.argv[1]
        if os.path.exists(path) and path.lower().endswith('.med'):
            json_path = path.replace('.med', '.json')
            result = extract_single_mesh(path)
            if result["status"] == "success":
                with open(json_path, 'w', encoding='utf-8') as f:
                    json.dump(result, f, separators=(',', ':'))
                print(f"[EXTRACT] Success: {os.path.basename(json_path)}")
                sys.exit(0)
            else:
                print(f"[EXTRACT] Error: {result['message']}")
                sys.exit(1)

    # Batch processing of hibrido folder
    base_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
    target_dir = os.path.join(base_dir, "testcases", "hibrido")
    
    process_directory(target_dir)
    print("\n[FINISH] Background processing completed.")

--- ARQUIVO: backend/services/med/mesh_viewer_vtk.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MeshViewer VTK Standalone
Renders structured JSON mesh data (points + element connectivity) using native VTK.
Usage: python mesh_viewer_vtk.py <path_to_json>
"""
import sys
import os
import json
import vtk
import subprocess

def get_mesh_data(file_path):
    """Loads mesh data from JSON or triggers generation from MED."""
    if file_path.lower().endswith('.json'):
        if not os.path.exists(file_path): return None
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    if file_path.lower().endswith('.med'):
        json_path = file_path.replace('.med', '.json')
        print(f"[PROCESS] Calling MED extractor for: {os.path.basename(file_path)}...")
        
        # Paths for environment setup
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
        med_env_dir = os.path.join(project_root, "MEDCOUPLING-9.15.0", "MEDCOUPLING-9.15.0")
        mesher_script = os.path.join(project_root, "backend", "services", "med", "med_mesher.py")
        
        # Command to generate JSON on disk
        cmd = f'cmd /c "cd /d {med_env_dir} && call env_launch.bat && python {mesher_script} {file_path}"'
        
        try:
            subprocess.run(cmd, check=True, shell=True, capture_output=True)
            if os.path.exists(json_path):
                with open(json_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                print(f"[SUCCESS] JSON generated and loaded: {os.path.basename(json_path)}")
                return data
            else:
                print(f"[ERROR] Generator finished but JSON not found: {json_path}")
        except subprocess.CalledProcessError as e:
            print(f"[ERROR] MED Generator failed: {e.stderr}")
            
    return None

def render_meshes(json_paths):
    # Setup Renderer and Window
    renderer = vtk.vtkRenderer()
    renderWin = vtk.vtkRenderWindow()
    renderWin.AddRenderer(renderer)
    renderWin.SetSize(1280, 720)
    
    interactor = vtk.vtkRenderWindowInteractor()
    interactor.SetRenderWindow(renderWin)
    
    # 🎨 Color Cycle for differentiation
    colors = [
        [0.2, 0.6, 1.0], # Sky Blue
        [1.0, 0.5, 0.0], # Orange
        [0.0, 1.0, 0.5], # Spring Green
        [1.0, 0.2, 0.2], # Red
        [0.8, 0.8, 0.1], # Yellow
    ]

    print("\n[VIEWER] Loading Assembly...")
    
    for idx, json_path in enumerate(json_paths):
        data = get_mesh_data(json_path)
        if not data:
            print(f"[REJECTED] Could not load or generate: {json_path}")
            continue

        points_data = data.get("points", [])
        connectivity = data.get("connectivity", [])
        vtk_type = data.get("vtk_type")

        # 1. POINTS
        vtk_pts = vtk.vtkPoints()
        for i in range(0, len(points_data), 3):
            vtk_pts.InsertNextPoint(points_data[i], points_data[i+1], points_data[i+2])

        # 2. GRID
        grid = vtk.vtkUnstructuredGrid()
        grid.SetPoints(vtk_pts)
        
        for cell in connectivity:
            current_type = vtk_type
            if current_type is None:
                if len(cell) == 2: current_type = vtk.VTK_LINE
                elif len(cell) == 4: current_type = vtk.VTK_QUAD
                elif len(cell) == 8: current_type = vtk.VTK_HEXAHEDRON
                else: current_type = vtk.VTK_POLYGON
            grid.InsertNextCell(current_type, len(cell), cell)

        # 3. ACTORS
        mapper = vtk.vtkDataSetMapper()
        mapper.SetInputData(grid)

        actor = vtk.vtkActor()
        actor.SetMapper(mapper)
        
        # Apply color from cycle
        mesh_color = colors[idx % len(colors)]
        actor.GetProperty().SetColor(*mesh_color)
        actor.GetProperty().SetEdgeVisibility(True)
        actor.GetProperty().SetPointSize(4)
        
        # 4. OUTLINE per mesh
        outline = vtk.vtkOutlineFilter()
        outline.SetInputData(grid)
        outlineMapper = vtk.vtkPolyDataMapper()
        outlineMapper.SetInputConnection(outline.GetOutputPort())
        outlineActor = vtk.vtkActor()
        outlineActor.SetMapper(outlineMapper)
        outlineActor.GetProperty().SetColor(1, 1, 1) # White

        renderer.AddActor(actor)
        renderer.AddActor(outlineActor)
        
        print(f"[LOADED] {os.path.basename(json_path)}: {vtk_pts.GetNumberOfPoints()} pts, {grid.GetNumberOfCells()} cells")

    # Start interaction
    renderer.SetBackground(0.1, 0.1, 0.1)
    renderWin.SetWindowName(f"ProSolve Multi-Mesh Viewer ({len(json_paths)} components)")
    
    renderWin.Render()
    renderer.ResetCamera()
    renderWin.Render()
    
    print("[INFO] Assembly view ready. 'q' to exit.")
    interactor.Start()

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python mesh_viewer_vtk.py <mesh1.json> [mesh2.json ...]")
    else:
        render_meshes(sys.argv[1:])

--- ARQUIVO: backend/services/med/section_extruder.py ---
import os
import json
import sys
from sectionproperties.pre.library import (
    rectangular_section,
    rectangular_hollow_section,
    circular_section,
    circular_hollow_section,
    mono_i_section
)
from sectionproperties.analysis import Section

# ==========================================================
# CONFIGURAÇÃO DE SAÍDA E PARÂMETROS
# ==========================================================
DIRETORIO_SAIDA = r"C:\Users\jorge\OneDrive\ProSolveSimulation\testcases\hibrido"
NOME_ARQUIVO = "section_mesh.json"

TIPO_SECAO = 'I_SECTION'
PARAMS = {
    'h': 200, 'bf_top': 150, 'bf_bot': 150, 
    'tf_top': 12, 'tf_bot': 12, 'tw': 8, 'r': 5,
    'offset_y': 0.0, 'offset_z': 0.0, 'rotation': 0.0
}
# ==========================================================

def calculate_and_save_json():
    try:
        # 1. PARSE E GEOMETRIA
        p = {k: float(v) for k, v in PARAMS.items() if v is not None}
        geometry = None
        
        if TIPO_SECAO == 'RECTANGLE':
            geometry = rectangular_section(d=p.get('hy', 100), b=p.get('hz', 50))
        elif TIPO_SECAO == 'I_SECTION':
            geometry = mono_i_section(
                d=p.get('h', 200), b_t=p.get('bf_top', 100), b_b=p.get('bf_bot', 100), 
                t_ft=p.get('tf_top', 10), t_fb=p.get('tf_bot', 10), t_w=p.get('tw', 6), 
                r=p.get('r', 0), n_r=8
            )

        # 2. TRANSFORMAÇÕES (3-STEP)
        geometry = geometry.align_center(align_to=(0, 0))
        if abs(p.get('rotation', 0)) > 1e-9:
            geometry = geometry.rotate_section(angle=p['rotation'], rot_point=(0, 0))
        if abs(p.get('offset_y', 0)) > 1e-9 or abs(p.get('offset_z', 0)) > 1e-9:
            geometry = geometry.shift_section(x_offset=p['offset_z'], y_offset=p['offset_y'])

        # 3. GERAÇÃO DA MALHA
        # Mesh size baseado na menor espessura
        m_size = min(p.get('tw', 10), p.get('tf_top', 10), 10.0) / 1.5
        geometry.create_mesh(mesh_sizes=[max(m_size, 2.0)])
        
        # 4. ANÁLISE (Proteção contra erro plástico do Shapely)
        sec = Section(geometry)
        sec.calculate_geometric_properties()
        try:
            sec.calculate_plastic_properties()
        except:
            print("[AVISO] Propriedades plásticas ignoradas devido a erro de biblioteca.")

        # 5. CONVERSÃO PARA FORMATO VTK (JSON)
        sp_mesh = geometry.mesh
        vertices = sp_mesh.get('vertices')
        triangles = sp_mesh.get('triangles')
        
        # Converter vértices 2D (Y, Z) para lista flat 3D (0, Y, Z)
        points_3d = []
        for v in vertices:
            points_3d.extend([0.0, float(v[0]), float(v[1])]) # X=0

        # Garantir triângulos de 3 nós (Linear)
        if triangles is not None and len(triangles[0]) == 6:
            triangles = triangles[:, :3]
        
        # Montar dicionário final
        output_data = {
            "status": "success",
            "vtk_type": 5, # VTK_TRIANGLE
            "points": points_3d,
            "connectivity": triangles.tolist(),
            "properties": {
                "Area": float(sec.section_props.area),
                "Iyy": float(sec.section_props.ixx_c), # Inércia Y do SP
                "Izz": float(sec.section_props.iyy_c)  # Inércia Z do SP
            }
        }

        # 6. SALVAR NO DISCO
        if not os.path.exists(DIRETORIO_SAIDA):
            os.makedirs(DIRETORIO_SAIDA)
            
        caminho_final = os.path.join(DIRETORIO_SAIDA, NOME_ARQUIVO)
        with open(caminho_final, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, separators=(',', ':'))
            
        print(f"--- SUCESSO ---")
        print(f"Arquivo salvo em: {caminho_final}")
        print(f"Área: {output_data['properties']['Area']:.2f} mm2")

    except Exception as e:
        print(f"Erro no processamento: {e}")

if __name__ == "__main__":
    calculate_and_save_json()

--- ARQUIVO: backend/services/med/vtk_extruder.py ---
import os
import json
import vtk
import numpy as np
from vtk.util import numpy_support
import sys

# ==========================================================
# PARÂMETROS FÍSICOS
# ==========================================================
SHELL_THICKNESS = 15.0
SHELL_OFFSET = 0.0
BEAM_OFFSET_Y = 0.0  
BEAM_OFFSET_Z = 0.0

def load_json(path):
    if not os.path.exists(path): return None
    with open(path, 'r', encoding='utf-8') as f:
        return json.load(f)

def save_json(data, path):
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(data, f)
    print(f"   [GERADO] {os.path.basename(path)}")

def vtk_dataset_to_dict(vtk_data):
    """
    Converte um objeto VTK processado (PolyData) de volta para o formato JSON.
    """
    # 1. Extrair Pontos
    points_array = vtk_data.GetPoints().GetData()
    numpy_points = numpy_support.vtk_to_numpy(points_array)
    # Flatten para lista [x1, y1, z1, x2, y2, z2...]
    points_list = numpy_points.flatten().tolist()

    # 2. Extrair Conectividade
    connectivity_list = []
    num_cells = vtk_data.GetNumberOfCells()
    
    # Tipo da primeira célula (referência)
    first_cell_type = vtk_data.GetCellType(0) if num_cells > 0 else 0

    for i in range(num_cells):
        cell = vtk_data.GetCell(i)
        pid = cell.GetPointIds()
        ids = [pid.GetId(j) for j in range(pid.GetNumberOfIds())]
        connectivity_list.append(ids)

    return {
        "vtk_type": first_cell_type,
        "count": num_cells,
        "points": points_list,
        "connectivity": connectivity_list
    }

def process_beam_headless(beam_data, section_data, output_path):
    """
    Gera a geometria 3D das vigas (Tubos/Perfis) e salva em JSON.
    Replica a lógica 'process_beam' do visualizador.
    """
    # 1. Prepara a Seção Transversal
    s_pts = vtk.vtkPoints()
    for i in range(0, len(section_data['points']), 3):
        s_pts.InsertNextPoint(section_data['points'][i], section_data['points'][i+1], section_data['points'][i+2])
    
    sect_poly = vtk.vtkPolyData()
    sect_poly.SetPoints(s_pts)
    cells = vtk.vtkCellArray()
    for tri in section_data['connectivity']:
        cells.InsertNextCell(len(tri), tri)
    sect_poly.SetPolys(cells)

    # Offset Local
    off_trans = vtk.vtkTransform()
    off_trans.Translate(0, BEAM_OFFSET_Y, BEAM_OFFSET_Z)
    off_filt = vtk.vtkTransformPolyDataFilter()
    off_filt.SetInputData(sect_poly)
    off_filt.SetTransform(off_trans)
    off_filt.Update()

    # 2. Prepara os dados da linha de centro
    beam_pts = np.array(beam_data['points']).reshape(-1, 3)
    
    # Coletor para juntar todos os segmentos extrudados
    append_filter = vtk.vtkAppendPolyData()

    # 3. Processa cada segmento
    for seg in beam_data['connectivity']:
        p1, p2 = beam_pts[seg[0]], beam_pts[seg[1]]
        vec = p2 - p1
        length = np.linalg.norm(vec)
        if length < 1e-6: continue
        dir_v = vec / length

        # Orientação (Quaternion math implícito no RotateWXYZ)
        trans = vtk.vtkTransform()
        trans.Translate(p1)
        v_ref = np.array([1.0, 0.0, 0.0])
        axis = np.cross(v_ref, dir_v)
        angle = np.degrees(np.arccos(np.clip(np.dot(v_ref, dir_v), -1.0, 1.0)))
        
        if np.linalg.norm(axis) > 1e-6: 
            trans.RotateWXYZ(angle, axis)
        elif np.dot(v_ref, dir_v) < 0: 
            trans.RotateWXYZ(180, [0, 1, 0])

        t_filt = vtk.vtkTransformPolyDataFilter()
        t_filt.SetInputData(off_filt.GetOutput())
        t_filt.SetTransform(trans)
        
        # Extrusão Linear
        extruder = vtk.vtkLinearExtrusionFilter()
        extruder.SetInputConnection(t_filt.GetOutputPort())
        extruder.SetExtrusionTypeToVectorExtrusion()
        extruder.SetVector(dir_v)
        extruder.SetScaleFactor(length)
        extruder.Update()

        # Cópia profunda para o append
        poly_copy = vtk.vtkPolyData()
        poly_copy.DeepCopy(extruder.GetOutput())
        append_filter.AddInputData(poly_copy)

    # 4. Finaliza e Salva
    append_filter.Update()
    final_mesh = append_filter.GetOutput()
    
    if final_mesh.GetNumberOfPoints() > 0:
        json_output = vtk_dataset_to_dict(final_mesh)
        save_json(json_output, output_path)

def process_shell_headless(data, output_path):
    """
    Gera a geometria 3D (sólida) das cascas.
    APLICA A CORREÇÃO DE SERRILHADO DO SEU SCRIPT (Normals -> Warp -> Extrude).
    """
    # 1. Reconstrói a malha original no VTK
    pts = vtk.vtkPoints()
    p_list = data['points']
    for i in range(0, len(p_list), 3):
        pts.InsertNextPoint(p_list[i], p_list[i+1], p_list[i+2])
    
    grid = vtk.vtkUnstructuredGrid()
    grid.SetPoints(pts)
    for cell in data['connectivity']:
        grid.InsertNextCell(data['vtk_type'], len(cell), cell)

    # 2. Converte UnstructuredGrid -> PolyData (Geometria de Superfície)
    geom = vtk.vtkGeometryFilter()
    geom.SetInputData(grid)
    
    # 3. CRÍTICO: Calcula Normais (AQUI ESTÁ A CORREÇÃO DO SERRILHADO)
    norm = vtk.vtkPolyDataNormals()
    norm.SetInputConnection(geom.GetOutputPort())
    # Opcional: SplittingOff garante que vértices compartilhados tenham a mesma normal
    # norm.SplittingOff() 
    
    # 4. Aplica Offset (Baseado nas Normais)
    warp = vtk.vtkWarpVector()
    warp.SetInputConnection(norm.GetOutputPort())
    warp.SetInputArrayToProcess(0, 0, 0, 0, vtk.vtkDataSetAttributes.NORMALS)
    warp.SetScaleFactor(SHELL_OFFSET - (SHELL_THICKNESS / 2.0))
    
    # 5. Extrusão na direção da Normal (Normal Extrusion)
    ext = vtk.vtkLinearExtrusionFilter()
    ext.SetInputConnection(warp.GetOutputPort())
    ext.SetExtrusionTypeToNormalExtrusion()
    ext.SetScaleFactor(SHELL_THICKNESS)
    ext.Update()

    # 6. Salva
    final_mesh = ext.GetOutput()
    if final_mesh.GetNumberOfPoints() > 0:
        json_output = vtk_dataset_to_dict(final_mesh)
        save_json(json_output, output_path)

def run_processing(dir_alvo):
    """
    Função principal.
    Gera arquivos *_extrusion.json para visualização 3D.
    """
    if not os.path.exists(dir_alvo):
        print(f"[ERRO] Pasta não encontrada: {dir_alvo}")
        return

    print(f"--- Processamento de Geometria VTK (Headless) ---")
    
    # 1. Processamento de Vigas (Requer section_mesh.json)
    path_beam = os.path.join(dir_alvo, "beam.json")
    path_sect = os.path.join(dir_alvo, "section_mesh.json")
    path_out_beam = os.path.join(dir_alvo, "beam_extrusion.json")

    # Verifica se beam existe e processa
    if os.path.exists(path_beam) and os.path.exists(path_sect):
        print("Processando Vigas (Beam -> 3D)...")
        beam_j = load_json(path_beam)
        sect_j = load_json(path_sect)
        if beam_j and sect_j:
            process_beam_headless(beam_j, sect_j, path_out_beam)

    # 2. Processamento de Cascas (Shells)
    files = [f for f in os.listdir(dir_alvo) if f.endswith('.json')]
    # Ignora arquivos de sistema e os que acabamos de gerar
    ignore_list = ['beam.json', 'section_mesh.json', 'project.json', 'package.json', 'tsconfig.json', 'mesh_groups.json']
    
    for f in files:
        if f in ignore_list or f.endswith('_extrusion.json'):
            continue
            
        full_path = os.path.join(dir_alvo, f)
        data = load_json(full_path)
        
        if not data: continue
        v_type = data.get('vtk_type')
        
        # Se for CASCA (Triângulo=5 ou Quad=9)
        if v_type in [5, 9]:
            out_name = os.path.splitext(f)[0] + "_extrusion.json"
            out_path = os.path.join(dir_alvo, out_name)
            print(f"Processando Casca: {f} -> {out_name}")
            process_shell_headless(data, out_path)

    print("--- Processamento Concluído ---")

if __name__ == "__main__":
    # Permite rodar diretamente passando a pasta como argumento
    path_arg = sys.argv[1] if len(sys.argv) > 1 else r"C:\Users\jorge\OneDrive\ProSolveSimulation\testcases\hibrido"
    run_processing(path_arg)

--- ARQUIVO: backend/services/med/vtk_extruder_test02.py ---
import os
import json
import vtk
import numpy as np
import sys

# Parâmetros Padrão (Podem ser movidos para dentro da função se desejar)
SHELL_THICKNESS = 15.0
SHELL_OFFSET = 0.0
BEAM_OFFSET_Y = 0.0  
BEAM_OFFSET_Z = 0.0

COR_SOLIDO = (0.7, 0.2, 0.2)
COR_SHELL_FACE = (1.0, 1.0, 0.0)
COR_SHELL_VOL = (0.2, 0.6, 1.0)
COR_BEAM_VOL = (0.2, 0.8, 0.4)
COR_LINHA_MALHA = (0, 0, 0)

def load_json(path):
    if not os.path.exists(path): return None
    with open(path, 'r', encoding='utf-8') as f:
        return json.load(f)

def apply_mesh_style(actor, color, show_edges=True):
    prop = actor.GetProperty()
    prop.SetColor(color)
    prop.SetRepresentationToSurface()
    prop.SetInterpolationToPhong()
    if show_edges:
        prop.SetEdgeVisibility(True)
        prop.SetEdgeColor(COR_LINHA_MALHA)
        prop.SetLineWidth(1.0)
    else:
        prop.SetEdgeVisibility(False)

def create_vtk_grid(data):
    pts = vtk.vtkPoints()
    p_list = data['points']
    for i in range(0, len(p_list), 3):
        pts.InsertNextPoint(p_list[i], p_list[i+1], p_list[i+2])
    grid = vtk.vtkUnstructuredGrid()
    grid.SetPoints(pts)
    for cell in data['connectivity']:
        grid.InsertNextCell(data['vtk_type'], len(cell), cell)
    return grid

def process_beam(beam_data, section_data, renderer):
    s_pts = vtk.vtkPoints()
    for i in range(0, len(section_data['points']), 3):
        s_pts.InsertNextPoint(section_data['points'][i], section_data['points'][i+1], section_data['points'][i+2])
    sect_poly = vtk.vtkPolyData()
    sect_poly.SetPoints(s_pts)
    cells = vtk.vtkCellArray()
    for tri in section_data['connectivity']:
        cells.InsertNextCell(len(tri), tri)
    sect_poly.SetPolys(cells)

    off_trans = vtk.vtkTransform()
    off_trans.Translate(0, BEAM_OFFSET_Y, BEAM_OFFSET_Z)
    off_filt = vtk.vtkTransformPolyDataFilter()
    off_filt.SetInputData(sect_poly)
    off_filt.SetTransform(off_trans)
    off_filt.Update()

    beam_pts = np.array(beam_data['points']).reshape(-1, 3)
    for seg in beam_data['connectivity']:
        p1, p2 = beam_pts[seg[0]], beam_pts[seg[1]]
        vec = p2 - p1
        length = np.linalg.norm(vec)
        if length < 1e-6: continue
        dir_v = vec / length
        trans = vtk.vtkTransform()
        trans.Translate(p1)
        v_ref = np.array([1.0, 0.0, 0.0])
        axis = np.cross(v_ref, dir_v)
        angle = np.degrees(np.arccos(np.clip(np.dot(v_ref, dir_v), -1.0, 1.0)))
        if np.linalg.norm(axis) > 1e-6: trans.RotateWXYZ(angle, axis)
        elif np.dot(v_ref, dir_v) < 0: trans.RotateWXYZ(180, [0, 1, 0])

        t_filt = vtk.vtkTransformPolyDataFilter()
        t_filt.SetInputData(off_filt.GetOutput())
        t_filt.SetTransform(trans)
        
        extruder = vtk.vtkLinearExtrusionFilter()
        extruder.SetInputConnection(t_filt.GetOutputPort())
        extruder.SetExtrusionTypeToVectorExtrusion()
        extruder.SetVector(dir_v)
        extruder.SetScaleFactor(length)
        extruder.Update()

        actor = vtk.vtkActor()
        mapper = vtk.vtkPolyDataMapper()
        mapper.SetInputConnection(extruder.GetOutputPort())
        actor.SetMapper(mapper)
        apply_mesh_style(actor, COR_BEAM_VOL, show_edges=False)
        renderer.AddActor(actor)

def run_visualization(dir_alvo):
    """Função principal que será chamada externamente"""
    if not os.path.exists(dir_alvo):
        print(f"[ERRO] Pasta não encontrada: {dir_alvo}")
        return

    renderer = vtk.vtkRenderer()
    renderWin = vtk.vtkRenderWindow()
    renderWin.AddRenderer(renderer)
    
    # 1. Vigas
    beam_j = load_json(os.path.join(dir_alvo, "beam.json"))
    sect_j = load_json(os.path.join(dir_alvo, "section_mesh.json"))
    if beam_j and sect_j:
        process_beam(beam_j, sect_j, renderer)

    # 2. Outros
    files = [f for f in os.listdir(dir_alvo) if f.endswith('.json') and f not in ['beam.json', 'section_mesh.json', 'project.json']]
    
    for f in files:
        data = load_json(os.path.join(dir_alvo, f))
        v_type = data.get('vtk_type')

        if v_type in [5, 9]: # Cascas
            grid = create_vtk_grid(data)
            geom = vtk.vtkGeometryFilter()
            geom.SetInputData(grid)
            act_f = vtk.vtkActor()
            map_f = vtk.vtkDataSetMapper()
            map_f.SetInputData(grid)
            act_f.SetMapper(map_f)
            apply_mesh_style(act_f, COR_SHELL_FACE, show_edges=True)
            map_f.SetRelativeCoincidentTopologyPolygonOffsetParameters(-1, -1)
            renderer.AddActor(act_f)

            norm = vtk.vtkPolyDataNormals()
            norm.SetInputConnection(geom.GetOutputPort())
            warp = vtk.vtkWarpVector()
            warp.SetInputConnection(norm.GetOutputPort())
            warp.SetInputArrayToProcess(0, 0, 0, 0, vtk.vtkDataSetAttributes.NORMALS)
            warp.SetScaleFactor(SHELL_OFFSET - (SHELL_THICKNESS / 2.0))
            ext = vtk.vtkLinearExtrusionFilter()
            ext.SetInputConnection(warp.GetOutputPort())
            ext.SetExtrusionTypeToNormalExtrusion()
            ext.SetScaleFactor(SHELL_THICKNESS)
            act_v = vtk.vtkActor()
            map_v = vtk.vtkPolyDataMapper()
            map_v.SetInputConnection(ext.GetOutputPort())
            act_v.SetMapper(map_v)
            apply_mesh_style(act_v, COR_SHELL_VOL, show_edges=False)
            renderer.AddActor(act_v)

        elif v_type in [10, 11, 12, 13, 14]: # Sólidos
            grid = create_vtk_grid(data)
            mapper = vtk.vtkDataSetMapper()
            mapper.SetInputData(grid)
            actor = vtk.vtkActor()
            actor.SetMapper(mapper)
            apply_mesh_style(actor, COR_SOLIDO, show_edges=True)
            renderer.AddActor(actor)

    renderer.SetBackground(0.1, 0.1, 0.1)
    renderWin.SetSize(1280, 720)
    interactor = vtk.vtkRenderWindowInteractor()
    interactor.SetRenderWindow(renderWin)
    renderWin.Render()
    renderer.ResetCamera()
    interactor.Start()

if __name__ == "__main__":
    # Permite rodar diretamente: python visualizador.py "C:/caminho/da/pasta"
    path_arg = sys.argv[1] if len(sys.argv) > 1 else r"C:\Users\jorge\OneDrive\ProSolveSimulation\testcases\hibrido"
    run_visualization(path_arg)


################################################################################
# PASTA: deletar
################################################################################

--- ARQUIVO: deletar/main.py ---
import os
import sys
import subprocess
import json
import tkinter as tk
from tkinter import filedialog
from flask import Flask, request, jsonify
from flask_cors import CORS

app = Flask(__name__)

# 🔌 SEGURANÇA: Permite que qualquer porta do Frontend (dinâmica) fale com a 5000
CORS(app, resources={r"/*": {"origins": "*"}})




# ==============================================================================
# 🤝 ROTA DE HANDSHAKE (COM LOG VISUAL)
# ==============================================================================
@app.route('/api/handshake', methods=['POST'])
def handshake():
    try:
        data = request.get_json()
        frontend_url = data.get('origin')
        
        print("\n" + "="*60)
        print(f"🤝 HANDSHAKE CONFIRMADO NO BACKEND!")
        print(f"📍 O Frontend se identificou na porta: {frontend_url}")
        print("🔓 Canal de comunicação estabelecido.")
        print("="*60 + "\n")
        
        return jsonify({"status": "paired", "message": "Backend connected", "backend_port": 5000})
    except Exception as e:
        print(f"❌ Erro no Handshake: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500




@app.route('/api/health', methods=['GET'])
def health():
    return jsonify({"status": "online", "service": "ProSolve Backend"})




# ==============================================================================
# 📂 ROTAS DE ARQUIVO (Necessárias para o Scan funcionar)
# ==============================================================================
@app.route('/api/browse_folder', methods=['POST'])
def browse_folder():
    try:
        root = tk.Tk()
        root.withdraw()
        root.attributes('-topmost', True)
        folder_selected = filedialog.askdirectory()
        root.destroy()
        
        if folder_selected:
            return jsonify({"status": "success", "path": os.path.normpath(folder_selected)})
        return jsonify({"status": "cancelled", "path": None})
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500



@app.route('/api/scan_workspace', methods=['POST'])
def scan_workspace():
    data = request.get_json()
    folder_path = data.get('folder_path')
    if not folder_path or not os.path.exists(folder_path):
        return jsonify({"Geometry": False, "Mesh": False}) # Resposta simplificada
    
    files = os.listdir(folder_path)
    # Lógica simplificada de retorno
    return jsonify({
        "Geometry": any(f.lower().endswith(('.stp', '.step')) for f in files),
        "Mesh": any(f.lower().endswith(('.med', '.msh')) for f in files),
        "Config": any(f.lower().endswith(('.comm', '.py')) for f in files),
        "Post-Pro": any(f.lower().endswith(('.rmed', '.res')) for f in files),
        "files": {
            "Geometry": [f for f in files if f.lower().endswith(('.stp', '.step'))],
            "Mesh": [f for f in files if f.lower().endswith(('.med', '.msh'))],
            "Config": [f for f in files if f.lower().endswith(('.comm', '.py'))],
            "Post-Pro": [f for f in files if f.lower().endswith(('.rmed', '.res'))]
        }
    })




# ==============================================================================
# 🧬 MED CONVERTER (Localizador Inteligente)
# # ==============================================================================
# @app.route('/api/mesh_dna', methods=['POST'])
# def api_mesh_dna():
#     try:
#         data = request.get_json()
#         file_path = data.get('file_path')

#         if not file_path or not os.path.exists(file_path):
#             return jsonify({"status": "error", "message": "Arquivo inválido"}), 400

#         print(f"🧬 [MED_API] Solicitado DNA para: {os.path.basename(file_path)}")

#         # --- LOCALIZADOR DE CAMINHOS ---
#         base_dir = os.path.dirname(os.path.abspath(__file__))
        
#         # 1. Acha o MEDCOUPLING (Tenta na raiz ou pasta acima)
#         med_roots = [
#             os.path.join(base_dir, "MEDCOUPLING-9.15.0", "MEDCOUPLING-9.15.0"),
#             os.path.join(base_dir, "..", "MEDCOUPLING-9.15.0", "MEDCOUPLING-9.15.0")
#         ]
#         med_dir = next((p for p in med_roots if os.path.exists(p)), None)

#         # 2. Acha o script med_extractor.py
#         script_locs = [
#             os.path.join(base_dir, "services", "med", "med_extractor.py"),
#             os.path.join(base_dir, "med_extractor.py"),
#             os.path.join(base_dir, "prosolve", "med_extractor.py") # Caso esteja na pasta prosolve
#         ]
#         extractor_script = next((p for p in script_locs if os.path.exists(p)), None)

#         if not med_dir or not extractor_script:
#             print(f"❌ [MED_API] Erro Fatal: Dependências não encontradas.\nMED: {med_dir}\nScript: {extractor_script}")
#             return jsonify({"status": "error", "message": "Server misconfiguration"}), 500

#         # Executa
#         cmd = f'cmd /c "cd /d "{med_dir}" && call env_launch.bat && python "{extractor_script}" "{file_path}""'
#         result = subprocess.run(cmd, capture_output=True, text=True, shell=True)

#         if result.returncode == 0:
#             # Parseia a saída
#             output_lines = result.stdout.strip().split('\n')
#             for line in reversed(output_lines):
#                 line = line.strip()
#                 if line.startswith('{') and line.endswith('}'):
#                     try:
#                         return jsonify(json.loads(line))
#                     except: continue
#             return jsonify({"status": "error", "message": "JSON não encontrado na saída"}), 500
#         else:
#             print(f"❌ [MED_API] Erro no subprocesso:\n{result.stderr}")
#             return jsonify({"status": "error", "message": result.stderr}), 500

#     except Exception as e:
#         print(f"❌ [MED_API] Exceção: {e}")
#         return jsonify({"status": "error", "message": str(e)}), 500



if __name__ == '__main__':
    print("🚀 ProSolve Global Server (Porta 5000) - Aguardando Handshake...")
    app.run(host='0.0.0.0', port=5000, debug=True)


################################################################################
# PASTA: frontend/src
################################################################################

--- ARQUIVO: frontend/src/App.tsx ---
import { useState } from 'react'
import Dashboard from './components/Dashboard'
import StructuralWorkspace from './components/StructuralWorkspace'

import ConsoleOverlay from './components/ConsoleOverlay'

type View = 'dashboard' | 'structural'

function App() {
  const [currentView, setCurrentView] = useState<View>('dashboard')
  const [projectPath, setProjectPath] = useState<string | null>(null)

  return (
    <div className="h-screen w-screen flex flex-col bg-slate-950">
      {/* Title Bar */}
      <div className="h-10 bg-slate-900 border-b border-slate-800 flex items-center px-4 shrink-0">
        <h1 className="text-xs font-bold tracking-widest text-slate-400 uppercase">
          ProSolve Professional {currentView === 'structural' && '/ Structural'}
        </h1>
      </div>

      {/* Main Content */}
      <div className="flex-1 overflow-hidden">
        {currentView === 'dashboard' && (
          <Dashboard onNavigate={(view) => setCurrentView(view as View)} />
        )}
        {currentView === 'structural' && (
          <StructuralWorkspace
            onBack={() => setCurrentView('dashboard')}
            projectPath={projectPath}
            setProjectPath={setProjectPath}
          />
        )}
      </div>

      {/* Global Debug Console */}
      <ConsoleOverlay />
    </div>
  )
}

export default App

--- ARQUIVO: frontend/src/main.tsx ---
import React from 'react'
import ReactDOM from 'react-dom/client'
import App from './App.tsx'
import './index.css'

ReactDOM.createRoot(document.getElementById('root')!).render(
  <React.StrictMode>
    <App />
  </React.StrictMode>,
)


################################################################################
# PASTA: frontend/src/components
################################################################################

--- ARQUIVO: frontend/src/components/ConsoleOverlay.tsx ---
import { useState, useEffect, useRef } from 'react'
import { X, Trash2, Terminal } from 'lucide-react'

// Global event bus for logs
const logListeners: ((log: any) => void)[] = []

const originalLog = console.log
const originalError = console.error
const originalWarn = console.warn

// Hook into console
console.log = (...args) => {
    originalLog(...args)
    notifyListeners('log', args)
}

console.error = (...args) => {
    originalError(...args)
    notifyListeners('error', args)
}

console.warn = (...args) => {
    originalWarn(...args)
    notifyListeners('warn', args)
}

function notifyListeners(type: 'log' | 'error' | 'warn', args: any[]) {
    const message = args.map(arg => {
        if (typeof arg === 'object') return JSON.stringify(arg)
        return String(arg)
    }).join(' ')

    const logEntry = {
        id: Date.now() + Math.random(),
        timestamp: new Date().toLocaleTimeString(),
        type,
        message
    }

    logListeners.forEach(l => l(logEntry))
}

export default function ConsoleOverlay() {
    const [isOpen, setIsOpen] = useState(false)
    const [logs, setLogs] = useState<any[]>([])
    const endRef = useRef<HTMLDivElement>(null)

    useEffect(() => {
        const handler = (newLog: any) => {
            setLogs(prev => [...prev.slice(-100), newLog]) // Keep last 100
        }
        logListeners.push(handler)
        return () => {
            const idx = logListeners.indexOf(handler)
            if (idx !== -1) logListeners.splice(idx, 1)
        }
    }, [])

    useEffect(() => {
        if (isOpen && endRef.current) {
            endRef.current.scrollIntoView({ behavior: 'smooth' })
        }
    }, [logs, isOpen])

    if (!isOpen) {
        return (
            <button
                onClick={() => setIsOpen(true)}
                className="fixed bottom-4 right-4 z-[9999] bg-slate-900 border border-slate-700 p-2 rounded-full shadow-lg hover:bg-slate-800 transition-colors text-slate-400 hover:text-white"
                title="Open Debug Console"
            >
                <Terminal className="w-5 h-5" />
            </button>
        )
    }

    return (
        <div className="fixed bottom-0 right-0 w-full md:w-2/3 lg:w-1/2 h-1/3 min-h-[300px] z-[9999] bg-slate-950 border-t border-l border-slate-800 shadow-2xl flex flex-col font-mono text-xs">
            {/* Header */}
            <div className="flex items-center justify-between px-4 py-2 bg-slate-900 border-b border-slate-800 shrink-0">
                <div className="flex items-center gap-2 text-slate-300 font-bold uppercase tracking-wider">
                    <Terminal className="w-3 h-3" />
                    <span>Debug Console</span>
                </div>
                <div className="flex items-center gap-2">
                    <button
                        onClick={() => setLogs([])}
                        className="p-1 hover:bg-slate-800 rounded text-slate-400 hover:text-white"
                        title="Clear"
                    >
                        <Trash2 className="w-4 h-4" />
                    </button>
                    <button
                        onClick={() => setIsOpen(false)}
                        className="p-1 hover:bg-slate-800 rounded text-slate-400 hover:text-white"
                        title="Close"
                    >
                        <X className="w-4 h-4" />
                    </button>
                </div>
            </div>

            {/* Logs in reverse order (newest at bottom) like real terminal */}
            <div className="flex-1 overflow-auto p-4 space-y-1">
                {logs.map(log => (
                    <div key={log.id} className="flex gap-2 font-mono">
                        <span className="text-slate-600 shrink-0">[{log.timestamp}]</span>
                        <span className={`break-all whitespace-pre-wrap ${log.type === 'error' ? 'text-red-400' :
                                log.type === 'warn' ? 'text-yellow-400' :
                                    'text-slate-300'
                            }`}>
                            {log.message}
                        </span>
                    </div>
                ))}
                <div ref={endRef} />
            </div>
        </div>
    )
}

--- ARQUIVO: frontend/src/components/Dashboard.tsx ---
import { useState } from 'react'
import { motion } from 'framer-motion'
import { Building2, Ship, BookOpen, Settings as SettingsIcon } from 'lucide-react'
import SettingsModal from './Settings'

interface DashboardProps {
    onNavigate: (view: string) => void
}

export default function Dashboard({ onNavigate }: DashboardProps) {
    const [showSettings, setShowSettings] = useState(false)
    return (
        <div className="h-full w-full flex flex-col items-center justify-center bg-gradient-to-br from-slate-900 via-slate-800 to-slate-900 p-8">
            {/* Settings Button */}
            <motion.button
                initial={{ opacity: 0 }}
                animate={{ opacity: 1 }}
                onClick={() => setShowSettings(true)}
                className="absolute top-6 right-6 p-3 bg-slate-800/50 hover:bg-slate-700/50 border border-slate-700 hover:border-slate-600 rounded-lg transition-all"
            >
                <SettingsIcon className="w-5 h-5 text-slate-400 hover:text-slate-200" />
            </motion.button>

            <motion.h1
                initial={{ opacity: 0, y: -20 }}
                animate={{ opacity: 1, y: 0 }}
                className="text-5xl font-light mb-16 text-slate-100 tracking-tight"
            >
                ProSolve Professional
            </motion.h1>

            <div className="grid grid-cols-3 gap-8 max-w-6xl w-full">
                {/* Structural Card */}
                <motion.div
                    initial={{ opacity: 0, y: 20 }}
                    animate={{ opacity: 1, y: 0 }}
                    transition={{ delay: 0.1 }}
                    onClick={() => onNavigate('structural')}
                    className="group relative bg-slate-800/50 backdrop-blur-sm hover:bg-slate-700/50 border border-slate-700 hover:border-blue-500 p-10 rounded-2xl cursor-pointer transition-all duration-300 shadow-xl hover:shadow-2xl hover:scale-105"
                >
                    <div className="absolute inset-0 bg-gradient-to-br from-blue-500/10 to-transparent rounded-2xl opacity-0 group-hover:opacity-100 transition-opacity" />
                    <Building2 className="w-12 h-12 mb-6 text-blue-400 group-hover:text-blue-300 transition-colors" />
                    <h2 className="text-3xl font-bold mb-3 text-slate-100 group-hover:text-blue-300 transition-colors">
                        Structural
                    </h2>
                    <p className="text-slate-400 text-sm leading-relaxed">
                        FEA Analysis & Modeling with Code_Aster
                    </p>
                </motion.div>

                {/* Marine Card (Disabled) */}
                <motion.div
                    initial={{ opacity: 0, y: 20 }}
                    animate={{ opacity: 1, y: 0 }}
                    transition={{ delay: 0.2 }}
                    className="relative bg-slate-800/30 border border-slate-800 p-10 rounded-2xl opacity-50 cursor-not-allowed"
                >
                    <Ship className="w-12 h-12 mb-6 text-slate-600" />
                    <h2 className="text-3xl font-bold mb-3 text-slate-500">Marine</h2>
                    <p className="text-slate-600 text-sm">Coming soon...</p>
                </motion.div>

                {/* Utility Card */}
                <motion.div
                    initial={{ opacity: 0, y: 20 }}
                    animate={{ opacity: 1, y: 0 }}
                    transition={{ delay: 0.3 }}
                    className="group relative bg-slate-800/50 backdrop-blur-sm hover:bg-slate-700/50 border border-slate-700 hover:border-purple-500 p-10 rounded-2xl cursor-pointer transition-all duration-300 shadow-xl hover:shadow-2xl hover:scale-105"
                >
                    <div className="absolute inset-0 bg-gradient-to-br from-purple-500/10 to-transparent rounded-2xl opacity-0 group-hover:opacity-100 transition-opacity" />
                    <BookOpen className="w-12 h-12 mb-6 text-purple-400 group-hover:text-purple-300 transition-colors" />
                    <h2 className="text-3xl font-bold mb-3 text-slate-100 group-hover:text-purple-300 transition-colors">
                        Utility
                    </h2>
                    <p className="text-slate-400 text-sm leading-relaxed">
                        Calculation Tools & Engineering Notebooks
                    </p>
                </motion.div>
            </div>

            {/* Settings Modal */}
            {showSettings && <SettingsModal onClose={() => setShowSettings(false)} />}
        </div>
    )
}

--- ARQUIVO: frontend/src/components/Settings.tsx ---
import { useState, useEffect } from 'react'
import { motion } from 'framer-motion'
import { Settings as SettingsIcon, Save, FolderOpen, AlertCircle } from 'lucide-react'

interface SettingsProps {
    onClose: () => void
}

interface ToolSettings {
    freecad_path: string
    salome_path: string
    aster_path: string
}

export default function Settings({ onClose }: SettingsProps) {
    const [settings, setSettings] = useState<ToolSettings>({
        freecad_path: '',
        salome_path: '',
        aster_path: '',
    })
    const [isSaving, setIsSaving] = useState(false)
    const [message, setMessage] = useState<{ type: 'success' | 'error'; text: string } | null>(null)

    useEffect(() => {
        // Load existing settings
        async function loadConfig() {
            try {
                const res = await fetch('/api/get_settings')
                const data = await res.json()
                if (data.status === 'success' && data.settings) {
                    setSettings({
                        freecad_path: data.settings.freecad_path || '',
                        salome_path: data.settings.salome_path || '',
                        aster_path: data.settings.aster_path || ''
                    })
                }
            } catch (e) {
                console.error("Failed to load settings", e)
            }
        }
        loadConfig()
    }, [])

    const handleBrowse = async (field: keyof ToolSettings) => {
        try {
            const response = await fetch('/api/open_folder_dialog')
            const data = await response.json()

            if (data.status === 'success' && data.path) {
                setSettings(prev => ({ ...prev, [field]: data.path }))
            }
        } catch (error) {
            console.error('Failed to open dialog:', error)
        }
    }

    const handleSave = async () => {
        setIsSaving(true)
        try {
            const res = await fetch('/api/save_settings', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify(settings)
            })
            const data = await res.json()

            if (data.status === 'success') {
                setMessage({ type: 'success', text: 'Settings saved successfully!' })
                setTimeout(() => setMessage(null), 3000)
            } else {
                setMessage({ type: 'error', text: 'Failed: ' + data.message })
            }
        } catch (error) {
            setMessage({ type: 'error', text: 'Failed to save settings' })
        } finally {
            setIsSaving(false)
        }
    }

    return (
        <div className="fixed inset-0 bg-black/50 backdrop-blur-sm flex items-center justify-center z-50">
            <motion.div
                initial={{ opacity: 0, scale: 0.95 }}
                animate={{ opacity: 1, scale: 1 }}
                className="bg-slate-800 border border-slate-700 rounded-2xl shadow-2xl w-full max-w-2xl mx-4"
            >
                {/* Header */}
                <div className="flex items-center justify-between p-6 border-b border-slate-700">
                    <div className="flex items-center gap-3">
                        <SettingsIcon className="w-6 h-6 text-blue-400" />
                        <h2 className="text-2xl font-bold text-slate-100">Application Settings</h2>
                    </div>
                    <button
                        onClick={onClose}
                        className="text-slate-400 hover:text-slate-200 transition-colors"
                    >
                        ✕
                    </button>
                </div>

                {/* Content */}
                <div className="p-6 space-y-6">
                    {message && (
                        <div
                            className={`flex items-center gap-2 p-4 rounded-lg ${message.type === 'success'
                                ? 'bg-green-500/10 border border-green-500/20 text-green-400'
                                : 'bg-red-500/10 border border-red-500/20 text-red-400'
                                }`}
                        >
                            <AlertCircle className="w-5 h-5" />
                            <span className="text-sm">{message.text}</span>
                        </div>
                    )}

                    {/* FreeCAD */}
                    <div>
                        <label className="block text-sm font-medium text-slate-300 mb-2">
                            FreeCAD Executable
                        </label>
                        <div className="flex gap-2">
                            <input
                                type="text"
                                value={settings.freecad_path}
                                onChange={(e) => setSettings(prev => ({ ...prev, freecad_path: e.target.value }))}
                                placeholder="C:\Program Files\FreeCAD\bin\FreeCAD.exe"
                                className="flex-1 bg-slate-900 border border-slate-700 rounded-lg px-4 py-2 text-slate-200 focus:outline-none focus:border-blue-500 transition-colors font-mono text-sm"
                            />
                            <button
                                onClick={() => handleBrowse('freecad_path')}
                                className="bg-slate-700 hover:bg-slate-600 px-4 py-2 rounded-lg transition-colors flex items-center gap-2"
                            >
                                <FolderOpen className="w-4 h-4" />
                                Browse
                            </button>
                        </div>
                    </div>

                    {/* Salome */}
                    <div>
                        <label className="block text-sm font-medium text-slate-300 mb-2">
                            Salome Executable
                        </label>
                        <div className="flex gap-2">
                            <input
                                type="text"
                                value={settings.salome_path}
                                onChange={(e) => setSettings(prev => ({ ...prev, salome_path: e.target.value }))}
                                placeholder="C:\SALOME\salome.bat"
                                className="flex-1 bg-slate-900 border border-slate-700 rounded-lg px-4 py-2 text-slate-200 focus:outline-none focus:border-blue-500 transition-colors font-mono text-sm"
                            />
                            <button
                                onClick={() => handleBrowse('salome_path')}
                                className="bg-slate-700 hover:bg-slate-600 px-4 py-2 rounded-lg transition-colors flex items-center gap-2"
                            >
                                <FolderOpen className="w-4 h-4" />
                                Browse
                            </button>
                        </div>
                    </div>

                    {/* Code_Aster */}
                    <div>
                        <label className="block text-sm font-medium text-slate-300 mb-2">
                            Code_Aster Path
                        </label>
                        <div className="flex gap-2">
                            <input
                                type="text"
                                value={settings.aster_path}
                                onChange={(e) => setSettings(prev => ({ ...prev, aster_path: e.target.value }))}
                                placeholder="C:\CodeAster\bin\as_run"
                                className="flex-1 bg-slate-900 border border-slate-700 rounded-lg px-4 py-2 text-slate-200 focus:outline-none focus:border-blue-500 transition-colors font-mono text-sm"
                            />
                            <button
                                onClick={() => handleBrowse('aster_path')}
                                className="bg-slate-700 hover:bg-slate-600 px-4 py-2 rounded-lg transition-colors flex items-center gap-2"
                            >
                                <FolderOpen className="w-4 h-4" />
                                Browse
                            </button>
                        </div>
                    </div>
                </div>

                {/* Footer */}
                <div className="flex items-center justify-end gap-3 p-6 border-t border-slate-700">
                    <button
                        onClick={onClose}
                        className="px-6 py-2 bg-slate-700 hover:bg-slate-600 rounded-lg transition-colors"
                    >
                        Cancel
                    </button>
                    <button
                        onClick={handleSave}
                        disabled={isSaving}
                        className="px-6 py-2 bg-blue-600 hover:bg-blue-500 rounded-lg transition-colors flex items-center gap-2 disabled:opacity-50"
                    >
                        <Save className="w-4 h-4" />
                        {isSaving ? 'Saving...' : 'Save Settings'}
                    </button>
                </div>
            </motion.div>
        </div>
    )
}

--- ARQUIVO: frontend/src/components/StructuralWorkspace.tsx ---
import { useState, useEffect, useCallback } from 'react'
import { ArrowLeft, FolderOpen, Save, Play } from 'lucide-react'
import { motion } from 'framer-motion'
import ModelConfig from './config/ModelConfig'
import MaterialConfig from './config/MaterialConfig'
import RestrictionConfig from './config/RestrictionConfig'
import LoadConfig from './config/LoadConfig'
import GeometryConfig from './config/GeometryConfig'
import LoadCaseConfig from './config/LoadCaseConfig'
import MeshConfig from './config/MeshConfig'
import VtkMeshViewer from './config/VtkMeshViewer'
import VerificationConfig from './config/VerificationConfig'

interface StructuralWorkspaceProps {
    onBack: () => void
    projectPath: string | null
    setProjectPath: (path: string | null) => void
}

type Tab = 'model' | 'mesh' | 'material' | 'geometry' | 'restrictions' | 'loads' | 'loadcases' | '3d-view' | 'verification'

interface ProjectConfig {
    geometries: any[]
    materials: any[]
    restrictions: any[]
    loads: any[]
    load_cases: any[]
    post_elem_mass?: any
    post_releve_t_reactions?: any
}

export default function StructuralWorkspace({
    onBack,
    projectPath,
    setProjectPath,
}: StructuralWorkspaceProps) {
    const [isLoading, setIsLoading] = useState(false)
    const [activeTab, setActiveTab] = useState<Tab>('mesh')
    const [projectConfig, setProjectConfig] = useState<ProjectConfig>({
        geometries: [],
        materials: [],
        restrictions: [],
        loads: [],
        load_cases: [],
        post_elem_mass: { mass_calculations: [] },
        post_releve_t_reactions: { reaction_extraction: { enabled: true } }
    })

    // DEBUG LOG
    const [availableGroups, setAvailableGroups] = useState<string[]>([]) // Group names
    const [nodeGroups, setNodeGroups] = useState<string[]>([])
    const [allGroupsData, setAllGroupsData] = useState<any>({}) // Full group metadata
    const [meshFiles, setMeshFiles] = useState<string[]>([])
    const [simulationRunning, setSimulationRunning] = useState(false)
    // Adicione isso junto com os outros estados (ex: logo abaixo de meshFiles)
    const [vtkGeometries, setVtkGeometries] = useState<any[]>([])

    // CONSOLIDATION PROTOCOL: Mesh DNA Pipeline Smart Consumer (Multi-Mesh)
    useEffect(() => {
        if (projectPath && meshFiles.length > 0) {
            const medFiles = meshFiles.filter(f => f.toLowerCase().endsWith('.med'))
            console.log(`[MeshDNA] Multi-Mesh Protocol: Detected ${medFiles.length} files.`)

            medFiles.forEach(file => {
                // If not already in global state or if it's a new session
                fetchMeshDNA(file)
            })
        }
    }, [projectPath, meshFiles])

    const fetchMeshDNA = async (fileName: string) => {
        try {
            // Telemetry: Log Port (Protocol)
            const port = window.location.port || '3000' // Frontend port representation
            console.log(`[MeshDNA] Fetching from Port: ${port}`)

            const res = await fetch('/api/mesh_dna', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({ file_path: `${projectPath}\\${fileName}` })
            })
            const result = await res.json()

            if (result.status === 'success') {
                const groups = result.data.groups

                // Telemetry: Filtered Log (Protocol)
                console.log(`[MeshDNA] Groups Received from ${fileName}:`, Object.keys(groups))

                // Update Independent Global State (Protocol)
                const globalWindow = window as any
                if (!globalWindow.projectState) globalWindow.projectState = {}
                if (!globalWindow.projectState.meshes) globalWindow.projectState.meshes = {}

                // Store mesh data independently by filename
                globalWindow.projectState.meshes[fileName] = result.data

                // Update Independent Local UI States
                setAllGroupsData((prev: any) => ({
                    ...prev,
                    [fileName]: groups
                }))

                // 🌟 Update Flat States for Tabs (Accumulation)
                const newGroupNames = Object.keys(groups)
                setAvailableGroups(prev => Array.from(new Set([...prev, ...newGroupNames])))

                const newNodes = newGroupNames.filter(name => groups[name].category === 'Node')
                setNodeGroups(prev => Array.from(new Set([...prev, ...newNodes])))

                // Event Dispatch (Protocol)
                window.dispatchEvent(new Event('meshDataLoaded'))
            }
        } catch (error) {
            console.error("[MeshDNA] Fetch failed:", error)
        }
    }



    useEffect(() => {
        if (activeTab === '3d-view' && projectPath) {

            // Define função interna assíncrona para garantir a ordem (1 -> 2)
            const runVtkSequence = async () => {
                try {
                    // PASSO 1: Manda gerar e ESPERA (await) o backend responder
                    const resGen = await fetch('/api/vtk', {
                        method: 'POST',
                        headers: { 'Content-Type': 'application/json' },
                        body: JSON.stringify({ project_path: projectPath })
                    });

                    if (!resGen.ok) {
                        console.error('[VTK] Erro na geração:', resGen.statusText);
                        return; // Para se der erro na geração
                    }

                    // PASSO 2: Busca os dados (Só executa depois que o passo 1 acabou)
                    const resData = await fetch('/api/get_vtk_geometry', {
                        method: 'POST',
                        headers: { 'Content-Type': 'application/json' },
                        body: JSON.stringify({ folder_path: projectPath })
                    });

                    const json = await resData.json();

                    // PASSO 3: Guarda no Estado
                    if (json.status === 'success') {
                        console.log("📦 [PARENT] Dados prontos para entrega:", json.data);
                        setVtkGeometries(json.data);
                    }

                } catch (err) {
                    console.error('[VTK] Erro na sequência:', err);
                }
            };

            runVtkSequence();
        }
    }, [activeTab, projectPath])


    const handleRunSimulation = async () => {
        if (!projectPath) return
        setSimulationRunning(true)
        try {
            // Auto save first? Maybe safer to ask user to save, but let's assume Save Project flow handles generation.
            // We should ideally save first to ensure export is fresh.
            await handleSaveProject()

            const res = await fetch('/api/run_simulation', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({ folder_path: projectPath })
            })
            const data = await res.json()

            if (data.status === 'success') {
                alert('Simulation Completed!\nCheck "simulation_files/message" for details.')
            } else {
                alert('Simulation Failed:\n' + data.message)
            }
        } catch (error) {
            console.error(error)
            alert('Error running simulation')
        } finally {
            setSimulationRunning(false)
        }
    }

    const handleSaveProject = async () => {
        if (!projectPath) return
        try {
            console.log("Saving project...")

            const response = await fetch('/api/save_project', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({
                    folder_path: projectPath,
                    config: {
                        ...projectConfig,
                        meshes: meshFiles.map(f => ({
                            name: f.split('.')[0].replace(/[- ]/g, '_'),
                            filename: f
                        }))
                    }
                })
            })
            const data = await response.json()
            if (data.status === 'success') {
                alert("Project Saved & Generated Successfully!")
            } else {
                alert("Save Failed: " + data.message)
            }
        } catch (e) {
            console.error(e)
            alert("Error saving project")
        }
    }

    // Memoized update handlers to prevent infinite loops and DATA LOSS (Persistence Fix)
    const updateGeometries = useCallback((updatedGeos: any[]) => {
        setProjectConfig(prev => {
            // Identify which domains (categories) this update represents
            const currentDomains = new Set(updatedGeos.map(g => g._category).filter(Boolean))

            // If empty (e.g., initial load or deletion), we might need to be careful.
            // But typical logic is: if update covers 1D/2D, we replace 1D/2D and KEEP 3D/0D.
            const preserved = prev.geometries.filter(g => !currentDomains.has(g._category))

            return {
                ...prev,
                geometries: [...preserved, ...updatedGeos]
            }
        })
    }, [])

    const updateMaterials = useCallback((materials: any[]) => {
        setProjectConfig(prev => ({ ...prev, materials }))
    }, [])

    const updateRestrictions = useCallback((restrictions: any[]) => {
        setProjectConfig(prev => ({ ...prev, restrictions }))
    }, [])

    const updateLoads = useCallback((loads: any[]) => {
        setProjectConfig(prev => ({ ...prev, loads }))
    }, [])

    const updateLoadCases = useCallback((cases: any[]) => {
        setProjectConfig(prev => ({ ...prev, load_cases: cases }))
    }, [])


    const handleOpenFolder = async () => {
        try {
            setIsLoading(true)
            const response = await fetch('/api/open_folder_dialog')
            const data = await response.json()

            if (data.status === 'success' && data.path) {
                setProjectPath(data.path)
                setMeshFiles([]) // 🛡️ CRITICAL: Clear old files immediately to prevent Race Condition (Error 400)

                // Try load config, but start with empty if not found
                setProjectConfig({
                    geometries: [],
                    materials: [],
                    restrictions: [],
                    loads: [],
                    load_cases: []
                })

                // 2. Trigger Scan & Inspection
                // This will generate mesh.json, export.export and RUN inspect_mesh.py -> mesh_groups.json
                setIsLoading(true)
                const scanResp = await fetch('/api/scan_workspace', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ folder_path: data.path })
                })
                const scanData = await scanResp.json()

                // 3. Read the generated groups (and list files from scanData)
                if (scanData.status === 'success') {
                    if (scanData.files && scanData.files.mesh) {
                        setMeshFiles(scanData.files.mesh)
                    } else {
                        setMeshFiles([])
                    }

                    const groupsResp = await fetch('/api/read_mesh_groups', {
                        method: 'POST',
                        headers: { 'Content-Type': 'application/json' },
                        body: JSON.stringify({ folder_path: data.path })
                    })
                    const groupsData = await groupsResp.json()

                    if (groupsData.status === 'success' && groupsData.data && groupsData.data.groups) {
                        const allGroups = groupsData.data.groups
                        const groupNames = Object.keys(allGroups)

                        // 🛡️ DATA PROTECTION: DNA protocol is now the source of truth for allGroupsData.
                        // We do NOT overwrite it with potentially empty legacy scan results.
                        // setAllGroupsData(allGroups)
                        // setAvailableGroups(groupNames)

                        // Categorize
                        const nodes = groupNames.filter(n => allGroups[n].type === 'node')
                        // setNodeGroups(nodes)

                        console.log("ROOT: Groups centralized (DNA-only mode):", { nodes, allCount: groupNames.length })

                        // ALWAYS refresh geometries when groups are loaded
                        setProjectConfig(prev => {
                            const newGeos = groupNames
                                .filter(g => allGroups[g].type !== 'node')
                                .map(g => {
                                    const existing = prev.geometries.find(ex => ex.group === g)
                                    if (existing) return existing

                                    return {
                                        group: g,
                                        type: '3D',
                                        phenomenon: 'MECANIQUE',
                                        _category: '3D'
                                    }
                                })
                            return { ...prev, geometries: newGeos }
                        })
                    }
                }
            }
        } catch (error) {
            console.error('Failed to open folder:', error)
        } finally {
            setIsLoading(false)
        }
    }

    const tabs = [
        { id: 'mesh' as Tab, label: 'Mesh', icon: '🕸️' },
        { id: 'model' as Tab, label: 'Model', icon: '🏗️' },
        { id: 'geometry' as Tab, label: 'Geometry', icon: '📐' },
        { id: '3d-view' as Tab, label: '3D View', icon: '🧊' },
        { id: 'material' as Tab, label: 'Material', icon: '⚙️' },
        { id: 'restrictions' as Tab, label: 'Restrictions', icon: '🔒' },
        { id: 'loads' as Tab, label: 'Loads', icon: '⚡' },
        { id: 'loadcases' as Tab, label: 'Load Cases', icon: '📊' },
        { id: 'verification' as Tab, label: 'Verification', icon: '✅' }
    ]

    return (
        <div className="h-full w-full flex flex-col bg-slate-900">
            {/* Toolbar */}
            <div className="h-14 bg-slate-800 border-b border-slate-700 flex items-center px-4 gap-3 shrink-0">
                <motion.button
                    whileHover={{ scale: 1.05 }}
                    whileTap={{ scale: 0.95 }}
                    onClick={onBack}
                    className="flex items-center gap-2 px-4 py-2 bg-slate-700 hover:bg-slate-600 rounded-lg transition-colors"
                >
                    <ArrowLeft className="w-4 h-4" />
                    <span className="text-sm font-medium">Back</span>
                </motion.button>

                <div className="h-8 w-px bg-slate-700" />

                <motion.button
                    whileHover={{ scale: 1.05 }}
                    whileTap={{ scale: 0.95 }}
                    onClick={handleOpenFolder}
                    disabled={isLoading}
                    className="flex items-center gap-2 px-4 py-2 bg-blue-600 hover:bg-blue-500 rounded-lg transition-colors disabled:opacity-50"
                >
                    <FolderOpen className="w-4 h-4" />
                    <span className="text-sm font-medium">
                        {isLoading ? 'Opening...' : 'Open Project'}
                    </span>
                </motion.button>

                {projectPath && (
                    <>
                        <motion.button
                            whileHover={{ scale: 1.05 }}
                            whileTap={{ scale: 0.95 }}
                            onClick={handleSaveProject}
                            className="flex items-center gap-2 px-4 py-2 bg-green-600 hover:bg-green-500 rounded-lg transition-colors"
                        >
                            <Save className="w-4 h-4" />
                            <span className="text-sm font-medium">Save Project</span>
                        </motion.button>

                        <motion.button
                            whileHover={{ scale: 1.05 }}
                            whileTap={{ scale: 0.95 }}
                            onClick={handleRunSimulation}
                            disabled={simulationRunning}
                            className={`flex items-center gap-2 px-4 py-2 bg-purple-600 hover:bg-purple-500 rounded-lg transition-colors ${simulationRunning ? 'opacity-50' : ''}`}
                        >
                            <Play className="w-4 h-4" />
                            <span className="text-sm font-medium">
                                {simulationRunning ? 'Running...' : 'Run Simulation'}
                            </span>
                        </motion.button>
                    </>
                )}

                {projectPath && (
                    <div className="ml-auto text-sm text-slate-400 font-mono truncate max-w-md">
                        {projectPath}
                    </div>
                )}
            </div>

            {/* Main Content */}
            <div className="flex-1 flex overflow-hidden">
                {!projectPath ? (
                    <div className="flex-1 flex flex-col items-center justify-center">
                        <FolderOpen className="w-24 h-24 text-slate-600 mb-6" />
                        <h2 className="text-2xl font-semibold text-slate-400 mb-2">
                            No Project Selected
                        </h2>
                        <p className="text-slate-500 text-center max-w-md">
                            Click "Open Project" to select a folder containing your geometry
                            and mesh files.
                        </p>
                    </div>
                ) : (
                    <>
                        {/* Tab Navigation */}
                        <div className="w-48 bg-slate-800 border-r border-slate-700 p-4 space-y-2">
                            {tabs.map((tab) => (
                                <button
                                    key={tab.id}
                                    onClick={() => setActiveTab(tab.id)}
                                    className={`w-full flex items-center gap-3 px-4 py-3 rounded-lg transition-all ${activeTab === tab.id
                                        ? 'bg-blue-600 text-white shadow-lg'
                                        : 'bg-slate-700/50 text-slate-300 hover:bg-slate-700'
                                        }`}
                                >
                                    <span className="text-xl">{tab.icon}</span>
                                    <span className="font-medium text-sm">{tab.label}</span>
                                </button>
                            ))}
                        </div>

                        {/* Tab Content */}
                        <div className="flex-1 overflow-hidden">
                            {activeTab === 'model' && (
                                <ModelConfig
                                    key={projectPath}
                                    projectPath={projectPath}
                                    meshGroups={allGroupsData}
                                    currentGeometries={projectConfig.geometries}
                                    onUpdate={updateGeometries}
                                />
                            )}
                            {activeTab === 'mesh' && (
                                <MeshConfig
                                    key={projectPath}
                                    projectPath={projectPath}
                                    meshes={meshFiles}
                                />
                            )}
                            {activeTab === 'material' && (
                                <MaterialConfig
                                    key={projectPath}
                                    projectPath={projectPath}
                                    availableGroups={availableGroups}
                                    nodeGroups={nodeGroups}
                                    initialMaterials={projectConfig.materials}
                                    onUpdate={updateMaterials}
                                />
                            )}

                            {activeTab === 'geometry' && (
                                <GeometryConfig
                                    key={projectPath}
                                    projectPath={projectPath}
                                    meshGroups={allGroupsData}
                                    availableGeometries={projectConfig.geometries}
                                    onUpdate={updateGeometries}
                                />
                            )}

                            {activeTab === '3d-view' && (
                                <VtkMeshViewer
                                    projectPath={projectPath}
                                    meshKey={Date.now()}
                                    geometries={vtkGeometries}
                                />
                            )}

                            {activeTab === 'restrictions' && (
                                <RestrictionConfig
                                    key={projectPath}
                                    projectPath={projectPath}
                                    availableGroups={nodeGroups.length > 0 ? nodeGroups : availableGroups} // Prefer node groups
                                    initialRestrictions={projectConfig.restrictions}
                                    onUpdate={updateRestrictions}
                                />
                            )}
                            {activeTab === 'loads' && (
                                <LoadConfig
                                    key={projectPath}
                                    projectPath={projectPath}
                                    availableGroups={availableGroups}
                                    initialLoads={projectConfig.loads}
                                    onUpdate={updateLoads}
                                />
                            )}
                            {activeTab === 'loadcases' && (
                                <LoadCaseConfig
                                    key={projectPath}
                                    projectPath={projectPath}
                                    availableLoads={projectConfig.loads}
                                    availableRestrictions={projectConfig.restrictions}
                                    availableGroups={availableGroups}
                                    initialLoadCases={projectConfig.load_cases}
                                    onUpdate={updateLoadCases}
                                />
                            )}
                            {activeTab === 'verification' && (
                                <VerificationConfig
                                    key={projectPath}
                                    projectPath={projectPath}
                                    config={{
                                        mass: projectConfig.post_elem_mass,
                                        reactions: projectConfig.post_releve_t_reactions
                                    }}
                                    onUpdate={(type, data) => {
                                        setProjectConfig(prev => ({
                                            ...prev,
                                            [type === 'mass' ? 'post_elem_mass' : 'post_releve_t_reactions']: data
                                        }))
                                    }}
                                />
                            )}
                        </div>
                    </>
                )}
            </div>
        </div>
    )
}


################################################################################
# PASTA: frontend/src/components/config
################################################################################

--- ARQUIVO: frontend/src/components/config/GeometryConfig.tsx ---
import { useState, useEffect, useRef, useCallback } from 'react'

interface GeometryConfigProps {
    projectPath: string | null
    meshGroups?: any
    availableGeometries?: any[]
    onUpdate?: (geometries: any[]) => void
}

const PROFILE_TYPES = {
    'RECTANGLE': { label: 'Solid Rectangle', default: { hy: 100, hz: 50, offset_y: 0, offset_z: 0, rotation: 0, fiber_y: 0, fiber_z: 0 } },
    'BOX': { label: 'Rectangular Tube', default: { hy: 100, hz: 50, t: 5, offset_y: 0, offset_z: 0, rotation: 0, fiber_y: 0, fiber_z: 0 } },
    'CIRCLE': { label: 'Solid Circle', default: { r: 50, offset_y: 0, offset_z: 0, rotation: 0, fiber_y: 0, fiber_z: 0 } },
    'TUBE': { label: 'Circular Tube', default: { r: 50, t: 5, offset_y: 0, offset_z: 0, rotation: 0, fiber_y: 0, fiber_z: 0 } },
    'I_SECTION': { label: 'I-Section', default: { h: 200, tw: 6.3, bf_top: 100, tf_top: 8, bf_bot: 100, tf_bot: 8, offset_y: 0, offset_z: 0, rotation: 0, fiber_y: 0, fiber_z: 0 } }
}

const SHELL_DEFAULT = { thickness: 10.0, offset: 0.0, vx: 1.0, vy: 0.0, vz: 0.0 }

export default function GeometryConfig({ projectPath, availableGeometries = [], onUpdate }: GeometryConfigProps) {
    const [geometries, setGeometries] = useState<any[]>([])
    const [selectedIdx, setSelectedIdx] = useState(0)
    const [sectionImage, setSectionImage] = useState<string | null>(null)
    const [calculatedProps, setCalculatedProps] = useState<any>(null)
    const [calcLoading, setCalcLoading] = useState(false)
    const [calcError, setCalcError] = useState<string | null>(null)

    const [splitRatio, setSplitRatio] = useState(55)

    const containerRef = useRef<HTMLDivElement>(null)
    const isResizing = useRef(false)


    // Load geometries from parent
    useEffect(() => {
        // Only initialize if we haven't loaded anything yet, OR if the upstream count is different 
        // (rudimentary check to allow initial load but prevent looop)
        if (availableGeometries.length > 0 && geometries.length === 0) {
            const initialized = availableGeometries
                .filter((g: any) => {
                    const cat = g._category || ''
                    return cat === '1D' || cat === '2D'
                })
                .map((g: any) => {
                    const isBeam = g._category === '1D'
                    const isShell = g._category === '2D'

                    let params = {}
                    if (isBeam) {
                        params = { ...PROFILE_TYPES['I_SECTION'].default, ...(g.section_params || {}) }
                    } else if (isShell) {
                        params = { ...SHELL_DEFAULT, ...(g.section_params || {}) }
                    }

                    return {
                        ...g,
                        section_type: g.section_type || (isBeam ? 'I_SECTION' : 'SHELL'),
                        profile_name: g.profile_name || 'Custom',
                        section_params: params
                    }
                })

            setGeometries(initialized)
        }
    }, [availableGeometries])

    // Sync to parent
    useEffect(() => {
        if (geometries.length > 0 && onUpdate) {
            onUpdate(geometries)
        }
    }, [geometries, onUpdate])

    // Sync local calculation state when selection changes
    useEffect(() => {
        if (geometries[selectedIdx]) {
            setSectionImage(geometries[selectedIdx].section_image || null)
            setCalculatedProps(geometries[selectedIdx].section_properties || null)
        }
    }, [selectedIdx, geometries])

    // Calculate section
    const calculateSection = async () => {
        if (geometries.length === 0) return
        const selected = geometries[selectedIdx]
        const isBeam = selected.type?.includes('POU') || selected.type?.includes('BARRE')

        if (!isBeam) {
            setSectionImage(null)
            setCalculatedProps(null)
            return
        }

        setCalcLoading(true)
        setCalcError(null)

        try {
            const response = await fetch('/api/calculate_section', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({
                    type: selected.section_type,
                    params: selected.section_params
                })
            })

            const data = await response.json()

            if (data.status === 'success') {
                setSectionImage(data.image)
                setCalculatedProps(data.properties)

                // SAVE RESULT TO GEOMETRIES STATE (Persistence)
                setGeometries(prev => prev.map((g, i) =>
                    i === selectedIdx ? {
                        ...g,
                        section_properties: data.properties,
                        section_mesh: data.mesh,
                        section_image: data.image
                    } : g
                ))
            } else {
                throw new Error(data.message)
            }
        } catch (err: any) {
            setCalcError(`Error: ${err.message}`)
        } finally {
            setCalcLoading(false)
        }
    }

    // Auto trigger


    // Handlers
    const handleParamEdit = (idx: number, key: string, value: string) => {
        setGeometries(prev => prev.map((g, i) =>
            i === idx ? { ...g, profile_name: 'Custom', section_params: { ...g.section_params, [key]: value } } : g
        ))
    }

    const handleSectionTypeChange = (idx: number, type: string) => {
        setGeometries(prev => prev.map((g, i) =>
            i === idx ? { ...g, section_type: type, profile_name: 'Custom', section_params: { ...(PROFILE_TYPES as any)[type].default } } : g
        ))
    }

    // Resizer
    const handleMouseDown = () => {
        isResizing.current = true
        document.body.style.cursor = 'col-resize'
    }

    const handleMouseUp = () => {
        isResizing.current = false
        document.body.style.cursor = ''
    }

    const handleMouseMove = useCallback((e: MouseEvent) => {
        if (!isResizing.current || !containerRef.current) return
        const rect = containerRef.current.getBoundingClientRect()
        let w = ((e.clientX - rect.left) / rect.width) * 100
        if (w < 30) w = 30
        if (w > 70) w = 70
        setSplitRatio(w)
    }, [])

    // TAB-LEAVE TRIGGER: Pre-calculate extrusion when leaving tab
    useEffect(() => {
        return () => {
            const hasShells = geometries.some(g => {
                const isShell = g.type?.includes('DKT') || g.type?.includes('DST') || g.type?.includes('COQUE')
                const thickness = parseFloat(g.section_params?.thickness || '0')
                return isShell && thickness > 0
            })

            if (hasShells && projectPath) {
                console.log("GEOMETRY_CHILD: Leaving tab. Triggering MEDCoupling extrusion pre-calculation...");
                // Trigger warm-up request (VTK converter will call med_extruder.py)
                fetch('/api/get_mesh_vtk', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({
                        folder_path: projectPath,
                        geometries: geometries
                    })
                }).catch(err => console.error("Extrusion pre-calc failed", err))
            }
        }
    }, [projectPath, geometries]) // Captured state at time of unmount or change

    useEffect(() => {
        window.addEventListener('mouseup', handleMouseUp)
        window.addEventListener('mousemove', handleMouseMove)
        return () => {
            window.removeEventListener('mouseup', handleMouseUp)
            window.removeEventListener('mousemove', handleMouseMove)
        }
    }, [handleMouseMove])

    if (!projectPath) {
        return <div className="p-10 text-center text-slate-500">Please select a project.</div>
    }

    if (geometries.length === 0) {
        return (
            <div className="p-10 text-center text-slate-500">
                No 1D/2D groups found. Please configure Model first.
            </div>
        )
    }

    const selected = geometries[selectedIdx] || geometries[0]
    const isBeam = selected._category === '1D'
    const isShell = selected._category === '2D'

    return (
        <div className="flex h-full w-full bg-slate-950 text-slate-200 text-sm overflow-hidden">
            {/* Sidebar */}
            <div className="w-56 border-r border-slate-800 bg-slate-950 flex flex-col shrink-0">
                <div className="h-10 border-b border-slate-800 flex items-center px-4 bg-slate-950 shrink-0">
                    <span className="text-[10px] font-bold text-slate-500 uppercase tracking-widest">Mesh Groups</span>
                </div>
                <div className="flex-1 overflow-y-auto">
                    {geometries.map((geo, idx) => (
                        <div
                            key={idx}
                            onClick={() => setSelectedIdx(idx)}
                            className={`px-4 py-3 border-b border-slate-800/50 cursor-pointer flex justify-between items-center transition-colors ${selectedIdx === idx ? 'bg-blue-600/10 border-l-2 border-l-blue-500 text-white' : 'hover:bg-slate-900 text-slate-400 border-l-2 border-l-transparent'
                                }`}
                        >
                            <span className="text-xs font-medium truncate w-28" title={geo.group}>{geo.group}</span>
                            <span className="text-[9px] font-mono text-slate-500">{geo.type}</span>
                        </div>
                    ))}
                </div>
            </div>

            {/* Split Area */}
            <div className="flex-1 flex overflow-hidden relative" ref={containerRef}>
                {/* Input Panel */}
                <div style={{ width: `${splitRatio}%` }} className="flex flex-col border-r border-slate-800 bg-slate-900 h-full min-w-[350px]">
                    <div className="h-12 border-b border-slate-800 flex items-center justify-between px-4 bg-slate-900 shrink-0">
                        <div className="flex items-center gap-2">
                            <div className="w-1.5 h-1.5 rounded-full bg-blue-500" />
                            <span className="text-[10px] font-bold text-blue-400 uppercase tracking-widest">Definition</span>
                            <span className="text-[10px] text-slate-500 font-mono bg-slate-800 px-2 py-0.5 rounded border border-slate-700 ml-2">
                                {selected.group}
                            </span>
                        </div>
                        {isBeam && (
                            <div className="flex items-center gap-3">
                                <button
                                    onClick={calculateSection}
                                    disabled={calcLoading}
                                    className={`px-2 py-1 rounded text-[9px] font-bold uppercase border transition-all bg-blue-600 hover:bg-blue-500 text-white border-blue-500 ${calcLoading ? 'opacity-50 cursor-not-allowed' : ''}`}
                                >
                                    {calcLoading ? 'PROCESSING...' : 'CALCULATE'}
                                </button>
                            </div>
                        )}
                    </div>

                    <div className="flex-1 overflow-y-auto p-6 space-y-6">
                        {/* BEAM CONFIG */}
                        {isBeam && (
                            <>
                                <div className="grid grid-cols-2 gap-4">
                                    <div>
                                        <label className="text-[10px] font-bold text-blue-400 uppercase mb-1.5 block">Profile Type</label>
                                        <select
                                            className="w-full bg-slate-950 border border-slate-700 text-xs rounded p-2.5 outline-none focus:border-blue-500"
                                            value={selected.section_type || 'I_SECTION'}
                                            onChange={(e) => handleSectionTypeChange(selectedIdx, e.target.value)}
                                        >
                                            {Object.entries(PROFILE_TYPES).map(([k, v]) => (
                                                <option key={k} value={k}>{v.label}</option>
                                            ))}
                                        </select>
                                    </div>
                                </div>

                                <div>
                                    <label className="text-[10px] font-bold text-slate-400 uppercase mb-2 block border-b border-slate-800 pb-1">
                                        Dimensions & Position (mm / deg)
                                    </label>
                                    <div className="grid grid-cols-2 gap-x-4 gap-y-3">
                                        {Object.entries(selected.section_params || {}).map(([k, v]: [string, any]) => {
                                            const isOffset = k.includes('offset')
                                            const isRotation = k === 'rotation'
                                            const isFiber = k.includes('fiber')

                                            let badgeClass = 'bg-slate-800 border-slate-700 text-slate-400'
                                            let inputClass = 'border-slate-700 focus:border-blue-500'
                                            let labelText = k

                                            if (isOffset) {
                                                badgeClass = 'bg-purple-900/30 border-purple-800 text-purple-400'
                                                inputClass = 'border-purple-800 focus:border-purple-500'
                                                labelText = k.replace('offset_', 'off_')
                                            } else if (isRotation) {
                                                badgeClass = 'bg-orange-900/30 border-orange-800 text-orange-400'
                                                inputClass = 'border-orange-800 focus:border-orange-500'
                                                labelText = 'rot°'
                                            } else if (isFiber) {
                                                badgeClass = 'bg-green-900/30 border-green-800 text-green-400'
                                                inputClass = 'border-green-800 focus:border-green-500'
                                                labelText = k.replace('fiber_', 'fib_')
                                            }

                                            return (
                                                <div key={k} className="relative">
                                                    <div className={`absolute inset-y-0 left-0 w-16 rounded-l border-y border-l flex items-center justify-center text-[9px] font-bold uppercase ${badgeClass}`}>
                                                        {labelText}
                                                    </div>
                                                    <input
                                                        type="text"
                                                        className={`w-full bg-slate-900 border text-white text-sm rounded pl-20 pr-2 py-1.5 outline-none font-mono transition-all ${inputClass}`}
                                                        value={v || ''}
                                                        onChange={(e) => handleParamEdit(selectedIdx, k, e.target.value)}
                                                    />
                                                </div>
                                            )
                                        })}
                                    </div>
                                </div>

                                {/* Properties Table - SIMPLIFIED */}
                                {calculatedProps && (
                                    <div className="mt-8 pt-4 border-t border-slate-800">
                                        <div className="text-[10px] font-bold text-green-500 uppercase mb-3">✓ Calculated Properties</div>
                                        <div className="space-y-4 text-xs">
                                            {/* Geometric Center & Area */}
                                            <div>
                                                <div className="text-[9px] font-bold text-slate-500 uppercase mb-1">Geometric</div>
                                                <div className="grid grid-cols-2 gap-2">
                                                    <PropRow label="Area (A)" value={calculatedProps["Area (A)"]} />
                                                    <PropRow label="Centroid Y" value={calculatedProps["Centroid Y (cy)"]} />
                                                    <PropRow label="Centroid Z" value={calculatedProps["Centroid Z (cx)"]} />
                                                </div>
                                            </div>

                                            {/* Principal Inertias */}
                                            <div>
                                                <div className="text-[9px] font-bold text-slate-500 uppercase mb-1">Principal (Local)</div>
                                                <div className="grid grid-cols-2 gap-2">
                                                    <PropRow label="Iyy (Local)" value={calculatedProps["Iyy (Local)"]} />
                                                    <PropRow label="Izz (Local)" value={calculatedProps["Izz (Local)"]} />
                                                    <PropRow label="Angle (deg)" value={calculatedProps["Angle (deg)"]} />
                                                    <PropRow label="I1 (Principal)" value={calculatedProps["I1 (Principal)"]} />
                                                    <PropRow label="I2 (Principal)" value={calculatedProps["I2 (Principal)"]} />
                                                </div>
                                            </div>

                                            {/* Nodal Inertias (The ones that matter for Aster if offset) */}
                                            <div>
                                                <div className="text-[9px] font-bold text-slate-500 uppercase mb-1">Nodal (At 0,0)</div>
                                                <div className="grid grid-cols-2 gap-2">
                                                    <PropRow label="Iyy (Node)" value={calculatedProps["Iyy (Node 0,0)"]} highlight />
                                                    <PropRow label="Izz (Node)" value={calculatedProps["Izz (Node 0,0)"]} highlight />
                                                    <PropRow label="Iyz (Node)" value={calculatedProps["Iyz (Node 0,0)"]} />
                                                    <PropRow label="Torsion J" value={calculatedProps["Torsion J"]} highlight />
                                                    <PropRow label="Warping Iw" value={calculatedProps["Warping Iw"]} />
                                                </div>
                                            </div>

                                            {/* Mechanical */}
                                            <div>
                                                <div className="text-[9px] font-bold text-slate-500 uppercase mb-1">Mechanical</div>
                                                <div className="grid grid-cols-2 gap-2">
                                                    <PropRow label="Shear Ay" value={calculatedProps["Shear Area Ay"]} />
                                                    <PropRow label="Shear Az" value={calculatedProps["Shear Area Az"]} />
                                                    <PropRow label="Elast. Wy" value={calculatedProps["Elastic Mod. Wy (Zxx)"]} />
                                                    <PropRow label="Elast. Wz" value={calculatedProps["Elastic Mod. Wz (Zyy)"]} />
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                )}
                            </>
                        )}

                        {/* SHELL CONFIG */}
                        {isShell && (
                            <div className="space-y-6">
                                <div className="grid grid-cols-2 gap-4">
                                    <div className="relative">
                                        <div className="absolute inset-y-0 left-0 w-24 rounded-l border-y border-l bg-slate-800 border-slate-700 text-slate-400 flex items-center justify-center text-[9px] font-bold uppercase">
                                            Thickness
                                        </div>
                                        <input
                                            type="text"
                                            className="w-full bg-slate-900 border border-slate-700 focus:border-blue-500 text-sm rounded pl-28 pr-2 py-1.5 outline-none font-mono"
                                            value={selected.section_params?.thickness || 0}
                                            onChange={(e) => handleParamEdit(selectedIdx, 'thickness', e.target.value)}
                                        />
                                    </div>
                                    <div className="relative">
                                        <div className="absolute inset-y-0 left-0 w-24 rounded-l border-y border-l bg-slate-800 border-slate-700 text-slate-400 flex items-center justify-center text-[9px] font-bold uppercase">
                                            Eccentricity
                                        </div>
                                        <input
                                            type="text"
                                            className="w-full bg-slate-900 border border-slate-700 focus:border-blue-500 text-sm rounded pl-28 pr-2 py-1.5 outline-none font-mono"
                                            value={selected.section_params?.offset || 0}
                                            onChange={(e) => handleParamEdit(selectedIdx, 'offset', e.target.value)}
                                        />
                                    </div>
                                </div>

                                <div>
                                    <label className="text-[10px] font-bold text-slate-400 uppercase mb-2 block border-b border-slate-800 pb-1">
                                        Normal Vector
                                    </label>
                                    <div className="grid grid-cols-3 gap-2">
                                        {['vx', 'vy', 'vz'].map((axis) => (
                                            <div key={axis} className="relative">
                                                <div className="absolute inset-y-0 left-0 w-8 rounded-l border-y border-l bg-slate-800 border-slate-700 text-slate-400 flex items-center justify-center text-[9px] font-bold uppercase">
                                                    {axis.toUpperCase()}
                                                </div>
                                                <input
                                                    type="text"
                                                    className="w-full bg-slate-900 border border-slate-700 focus:border-blue-500 text-sm rounded pl-10 pr-2 py-1.5 outline-none font-mono"
                                                    value={selected.section_params?.[axis] || 0}
                                                    onChange={(e) => handleParamEdit(selectedIdx, axis, e.target.value)}
                                                />
                                            </div>
                                        ))}
                                    </div>
                                </div>
                            </div>
                        )}
                    </div>
                </div>

                {/* Resizer */}
                <div
                    className="w-1 bg-slate-950 hover:bg-blue-600 cursor-col-resize z-20"
                    onMouseDown={handleMouseDown}
                />

                {/* Preview */}
                <div className="flex-1 flex flex-col bg-white relative min-w-[200px]">
                    <div className="h-12 border-b border-slate-200 flex items-center px-4 bg-slate-50 shrink-0">
                        <span className="text-[10px] font-bold text-slate-500 uppercase tracking-widest">Section Preview</span>
                    </div>
                    <div className="flex-1 flex items-center justify-center">
                        {calcLoading ? (
                            <div className="text-blue-500 animate-pulse">Processing...</div>
                        ) : calcError ? (
                            <div className="text-red-500 text-xs p-4">{calcError}</div>
                        ) : sectionImage ? (
                            <img src={sectionImage} alt="Section" className="max-w-full max-h-full object-contain" />
                        ) : isBeam ? (
                            <div className="text-slate-300 text-center">
                                <span className="text-5xl opacity-20">📐</span>
                                <p className="text-xs mt-2">No Geometry</p>
                            </div>
                        ) : (
                            <div className="text-slate-300 text-center">
                                <span className="text-5xl opacity-20">⬡</span>
                                <p className="text-xs mt-2">3D Shell Element</p>
                            </div>
                        )}
                    </div>
                </div>
            </div>
        </div>
    )
}

const PropRow = ({ label, value, highlight }: { label: string, value: any, highlight?: boolean }) => {
    const formatVal = (v: any) => {
        if (v === undefined || v === null) return '-'
        if (typeof v !== 'number') return v
        if (Math.abs(v) < 1e-9) return '0.00'
        if (Math.abs(v) < 1e-3 || Math.abs(v) > 1e5) return v.toExponential(3)
        return v.toFixed(2)
    }

    return (
        <div className={`flex justify-between px-2 py-1 rounded border ${highlight ? 'bg-purple-900/20 border-purple-500/30' : 'bg-slate-950/50 border-slate-800'}`}>
            <span className="text-[9px] text-slate-500">{label}</span>
            <span className={`text-[10px] font-mono ${highlight ? 'text-white font-bold' : 'text-blue-300'}`}>
                {formatVal(value)}
            </span>
        </div>
    )
}

--- ARQUIVO: frontend/src/components/config/LoadCaseConfig.tsx ---
import { useState, useEffect, useRef } from 'react'
import { Plus, Trash2 } from 'lucide-react'

interface LoadCase {
    id: string
    name: string
    loads: string[]
    restrictions: string[]
}

interface LoadCaseConfigProps {
    projectPath: string | null
    availableLoads?: any[]
    availableRestrictions?: any[] // Explicit restrictions if available
    availableGroups?: string[] // Fallback to groups if restrictions not provided
    initialLoadCases?: any[]
    onUpdate?: (cases: any[]) => void
}

export default function LoadCaseConfig({
    projectPath,
    availableLoads = [],
    availableRestrictions = [],
    availableGroups = [],
    initialLoadCases = [],
    onUpdate
}: LoadCaseConfigProps) {
    const isFirstRender = useRef(true)
    const [cases, setCases] = useState<LoadCase[]>([])
    const lastInitializedPath = useRef<string | null>(null)

    // Sync from props only on project change or initial load
    useEffect(() => {
        if (!projectPath) return

        if (lastInitializedPath.current !== projectPath) {
            if (initialLoadCases && initialLoadCases.length > 0) {
                const formatted = initialLoadCases.map((lc, index) => ({
                    id: (index + 1).toString(),
                    name: lc.name,
                    loads: lc.loads || [],
                    restrictions: lc.restrictions || []
                }))
                setCases(formatted)
                lastInitializedPath.current = projectPath
            } else if (cases.length === 0) {
                setCases([{ id: '1', name: 'Case 1', loads: [], restrictions: [] }])
                lastInitializedPath.current = projectPath
            }
        }
    }, [projectPath, initialLoadCases])

    useEffect(() => {
        if (isFirstRender.current) {
            isFirstRender.current = false
            return
        }

        if (onUpdate && projectPath === lastInitializedPath.current) {
            onUpdate(cases.map(c => ({
                name: c.name,
                loads: c.loads,
                restrictions: c.restrictions
            })))
        }
    }, [cases, onUpdate, projectPath])

    const addCase = () => {
        const newId = (cases.length + 1).toString()
        setCases([...cases, { id: newId, name: `Case ${newId}`, loads: [], restrictions: [] }])
    }

    const removeCase = (id: string) => {
        setCases(cases.filter(c => c.id !== id))
    }

    const updateName = (id: string, name: string) => {
        setCases(cases.map(c => c.id === id ? { ...c, name } : c))
    }

    const toggleItem = (caseId: string, field: 'loads' | 'restrictions', itemName: string) => {
        setCases(cases.map(c => {
            if (c.id !== caseId) return c

            const current = c[field] || []
            const hasItem = current.includes(itemName)
            const next = hasItem ? current.filter(i => i !== itemName) : [...current, itemName]

            return { ...c, [field]: next }
        }))
    }

    if (!projectPath) {
        return <div className="p-10 text-center text-slate-500">Please select a project.</div>
    }

    return (
        <div className="flex flex-col h-full w-full p-4 overflow-hidden">
            <div className="flex justify-between items-center mb-6 shrink-0">
                <div>
                    <h3 className="text-xl font-bold text-slate-100 flex items-center gap-2">
                        <span className="p-2 bg-indigo-500/20 rounded text-indigo-400">📊</span>
                        Load Cases (Combinations)
                    </h3>
                    <p className="text-sm text-slate-400 mt-1">Combine boundary conditions and structural loads.</p>
                </div>
                <button
                    onClick={addCase}
                    className="flex items-center gap-2 px-6 py-2.5 bg-blue-600 hover:bg-blue-500 text-white font-semibold rounded-xl transition-all shadow-lg shadow-blue-500/20 active:scale-95"
                >
                    <Plus className="w-5 h-5" />
                    New Case
                </button>
            </div>

            <div className="flex-1 overflow-y-auto space-y-6 pr-2 custom-scrollbar">
                {cases.map((lc) => (
                    <div key={lc.id} className="bg-slate-800/50 backdrop-blur border border-slate-700/50 rounded-2xl p-6 shadow-xl hover:border-slate-600/50 transition-colors">
                        <div className="flex justify-between items-center mb-6 border-b border-slate-700/50 pb-4">
                            <div className="flex-1 flex items-center gap-4">
                                <span className="text-[10px] font-bold text-slate-500 uppercase tracking-widest bg-slate-900/80 px-3 py-1.5 rounded-lg border border-slate-700">
                                    CASE ID: {lc.id}
                                </span>
                                <input
                                    type="text"
                                    value={lc.name}
                                    onChange={(e) => updateName(lc.id, e.target.value)}
                                    className="bg-slate-900/50 border border-slate-700 rounded-xl px-4 py-2 text-lg font-bold text-white focus:outline-none focus:ring-2 focus:ring-blue-500/50 transition-all min-w-[300px]"
                                    placeholder="Enter case name..."
                                />
                            </div>
                            <button
                                onClick={() => removeCase(lc.id)}
                                className="p-2.5 text-slate-500 hover:text-red-400 hover:bg-red-400/10 rounded-xl transition-all"
                                title="Delete Case"
                            >
                                <Trash2 className="w-5 h-5" />
                            </button>
                        </div>

                        <div className="grid grid-cols-1 md:grid-cols-2 gap-8">
                            {/* SECTION: LOADS */}
                            <div className="space-y-4">
                                <div className="flex items-center gap-2 text-blue-400">
                                    <Layers className="w-4 h-4" />
                                    <h4 className="text-xs font-bold uppercase tracking-widest">Included Loads</h4>
                                </div>
                                <div className="bg-slate-900/40 p-4 rounded-xl border border-slate-700/30 flex flex-wrap gap-2 min-h-[60px]">
                                    {availableLoads.length === 0 ? (
                                        <span className="text-xs text-slate-600 italic">No loads available.</span>
                                    ) : (
                                        availableLoads.map((load) => {
                                            const isActive = lc.loads.includes(load.name)
                                            return (
                                                <button
                                                    key={load.name}
                                                    onClick={() => toggleItem(lc.id, 'loads', load.name)}
                                                    className={`px-3 py-1.5 rounded-lg text-xs font-medium border transition-all ${isActive
                                                        ? 'bg-blue-600 border-blue-500 text-white shadow-lg shadow-blue-500/20'
                                                        : 'bg-slate-800 border-slate-700 text-slate-400 hover:border-slate-500'
                                                        }`}
                                                >
                                                    {load.name}
                                                </button>
                                            )
                                        })
                                    )}
                                </div>
                            </div>

                            {/* SECTION: RESTRICTIONS (DDL) */}
                            <div className="space-y-4">
                                <div className="flex items-center gap-2 text-orange-400">
                                    <button className="p-0 border-0 bg-transparent text-inherit cursor-default">
                                        <Plus className="w-4 h-4" />
                                    </button>
                                    <h4 className="text-xs font-bold uppercase tracking-widest">Boundary Conditions (Restrictions)</h4>
                                </div>
                                <div className="bg-slate-900/40 p-4 rounded-xl border border-slate-700/30 flex flex-wrap gap-2 min-h-[60px]">
                                    {availableRestrictions.length === 0 && availableGroups.length === 0 ? (
                                        <span className="text-xs text-slate-600 italic">No restrictions or groups found.</span>
                                    ) : (
                                        (availableRestrictions.length > 0 ? availableRestrictions : availableGroups).map((item) => {
                                            const name = typeof item === 'string' ? item : item.name
                                            const isActive = (lc.restrictions || []).includes(name)
                                            return (
                                                <button
                                                    key={name}
                                                    onClick={() => toggleItem(lc.id, 'restrictions', name)}
                                                    className={`px-3 py-1.5 rounded-lg text-xs font-medium border transition-all ${isActive
                                                        ? 'bg-orange-600 border-orange-500 text-white shadow-lg shadow-orange-500/20'
                                                        : 'bg-slate-800 border-slate-700 text-slate-400 hover:border-slate-500'
                                                        }`}
                                                >
                                                    {name}
                                                </button>
                                            )
                                        })
                                    )}
                                </div>
                            </div>
                        </div>
                    </div>
                ))}
            </div>
        </div>
    )
}

import { Layers } from 'lucide-react'

--- ARQUIVO: frontend/src/components/config/LoadConfig.tsx ---
import { useState, useEffect, useRef } from 'react'
import { Plus, Trash2 } from 'lucide-react'

interface Load {
    id: string
    name: string
    type: 'gravity' | 'force' | 'pressure'
    group?: string
    fx?: string
    fy?: string
    fz?: string
    pressure?: string
    ax?: string
    ay?: string
    az?: string
    intensity?: string
}

interface LoadConfigProps {
    projectPath: string | null
    availableGroups?: string[]
    initialLoads?: any[]
    onUpdate?: (loads: any[]) => void
}

export default function LoadConfig({
    projectPath,
    availableGroups = [],
    initialLoads = [],
    onUpdate
}: LoadConfigProps) {
    const [loads, setLoads] = useState<Load[]>([])
    const isFirstRender = useRef(true)

    // Load initial from props
    useEffect(() => {
        if (initialLoads.length > 0 && loads.length === 0) {
            const formatted = initialLoads.map((l, index) => {
                // Map saved JSON back to UI state
                let type: 'gravity' | 'force' | 'pressure' = 'force'
                if (l.type === 'PESANTEUR') type = 'gravity'
                else if (l.type === 'PRESSION') type = 'pressure'

                return {
                    id: (index + 1).toString(),
                    name: l.name,
                    type: type,
                    group: l.group || '',
                    fx: l.fx?.toString() || '0',
                    fy: l.fy?.toString() || '0',
                    fz: l.fz?.toString() || '0',
                    pressure: l.pressure?.toString() || '0',
                    ax: l.direction?.[0]?.toString() || '0',
                    ay: l.direction?.[1]?.toString() || '0',
                    az: l.direction?.[2]?.toString() || '-1',
                    intensity: l.gravite?.toString() || '9.81'
                }
            })
            setLoads(formatted)
        }
    }, [initialLoads])

    const lastExportRef = useRef('')

    useEffect(() => {
        if (isFirstRender.current) {
            isFirstRender.current = false
            return
        }

        if (onUpdate) {
            const exportData = loads.map(l => {
                if (l.type === 'gravity') {
                    return {
                        name: String(l.name || ''),
                        type: 'PESANTEUR',
                        direction: [
                            parseFloat(l.ax || '0'),
                            parseFloat(l.ay || '0'),
                            parseFloat(l.az || '-1')
                        ],
                        gravite: parseFloat(l.intensity || '9.81')
                    }
                } else if (l.type === 'force') {
                    return {
                        name: String(l.name || ''),
                        type: 'FORCE_NODALE',
                        group: String(l.group || ''),
                        fx: parseFloat(l.fx || '0'),
                        fy: parseFloat(l.fy || '0'),
                        fz: parseFloat(l.fz || '0')
                    }
                } else {
                    return {
                        name: String(l.name || ''),
                        type: 'PRESSION',
                        group: String(l.group || ''),
                        pressure: parseFloat(l.pressure || '0')
                    }
                }
            })

            const currentString = JSON.stringify(exportData)
            if (lastExportRef.current !== currentString) {
                lastExportRef.current = currentString
                onUpdate(exportData)
            }
        }
    }, [loads, onUpdate])

    const addLoad = (type: 'gravity' | 'force' | 'pressure') => {
        const newId = (loads.length + 1).toString()
        const baseName = type === 'gravity' ? 'ACCEL' : type === 'force' ? 'Force' : 'Pressure'

        setLoads([
            ...loads,
            {
                id: newId,
                name: `${baseName}_${newId}`,
                type,
                group: availableGroups[0] || '',
                fx: '0',
                fy: '0',
                fz: '0',
                pressure: '0',
                ax: '0',
                ay: '0',
                az: '-1',
                intensity: '9.81'
            }
        ])
    }

    const removeLoad = (id: string) => {
        setLoads(loads.filter(l => l.id !== id))
    }

    const updateLoad = (id: string, field: keyof Load, value: any) => {
        setLoads(loads.map(l => (l.id === id ? { ...l, [field]: value } : l)))
    }

    if (!projectPath) {
        return <div className="p-10 text-center text-slate-500">Please select a project.</div>
    }

    return (
        <div className="flex flex-col h-full w-full p-4">
            {/* Header */}
            <div className="flex justify-between items-center mb-4">
                <h3 className="text-lg font-semibold text-slate-200">Load Definitions</h3>
                <div className="flex gap-2">
                    <button
                        onClick={() => addLoad('gravity')}
                        className="flex items-center gap-2 px-3 py-2 bg-orange-600 hover:bg-orange-500 rounded-lg transition-colors text-sm"
                    >
                        <Plus className="w-4 h-4" />
                        Acceleration
                    </button>
                    <button
                        onClick={() => addLoad('force')}
                        disabled={availableGroups.length === 0}
                        className="flex items-center gap-2 px-3 py-2 bg-green-600 hover:bg-green-500 rounded-lg transition-colors text-sm disabled:opacity-50"
                    >
                        <Plus className="w-4 h-4" />
                        Force
                    </button>
                    <button
                        onClick={() => addLoad('pressure')}
                        disabled={availableGroups.length === 0}
                        className="flex items-center gap-2 px-3 py-2 bg-blue-600 hover:bg-blue-500 rounded-lg transition-colors text-sm disabled:opacity-50"
                    >
                        <Plus className="w-4 h-4" />
                        Pressure
                    </button>
                </div>
            </div>

            {/* Loads List */}
            <div className="space-y-4 flex-1 overflow-y-auto">
                {loads.map((load) => (
                    <div
                        key={load.id}
                        className="bg-slate-800 border border-slate-700 rounded-lg p-4"
                    >
                        <div className="flex justify-between items-start mb-4">
                            <div className="flex-1">
                                <div className="flex items-center gap-3 mb-3">
                                    <span className={`px-2 py-1 rounded text-xs font-semibold ${load.type === 'gravity' ? 'bg-orange-500/20 text-orange-400' :
                                        load.type === 'force' ? 'bg-green-500/20 text-green-400' :
                                            'bg-blue-500/20 text-blue-400'
                                        }`}>
                                        {load.type === 'gravity' ? 'ACCELERATION' : load.type.toUpperCase()}
                                    </span>
                                    <input
                                        type="text"
                                        value={load.name}
                                        onChange={(e) => updateLoad(load.id, 'name', e.target.value)}
                                        className="flex-1 bg-slate-900 border border-slate-600 rounded px-3 py-1 text-sm focus:outline-none focus:border-blue-500"
                                        placeholder="Load Name"
                                    />
                                </div>

                                {load.type !== 'gravity' && (
                                    <div className="mb-3">
                                        <label className="block text-xs font-medium text-slate-400 mb-1">
                                            Mesh Group
                                        </label>
                                        <select
                                            value={load.group}
                                            onChange={(e) => updateLoad(load.id, 'group', e.target.value)}
                                            className="w-full bg-slate-900 border border-slate-600 rounded px-3 py-2 text-sm focus:outline-none focus:border-blue-500"
                                        >
                                            {availableGroups.map((group) => (
                                                <option key={group} value={group}>
                                                    {group}
                                                </option>
                                            ))}
                                        </select>
                                    </div>
                                )}

                                {load.type === 'gravity' && (
                                    <div className="grid grid-cols-4 gap-3">
                                        <div>
                                            <label className="block text-xs font-medium text-slate-400 mb-1">
                                                Intensity (m/s²)
                                            </label>
                                            <input
                                                type="text"
                                                value={load.intensity}
                                                onChange={(e) => updateLoad(load.id, 'intensity', e.target.value)}
                                                className="w-full bg-slate-900 border border-slate-600 rounded px-3 py-2 text-sm focus:outline-none focus:border-orange-500"
                                                placeholder="9.81"
                                            />
                                        </div>
                                        {(['ax', 'ay', 'az'] as const).map((dir) => (
                                            <div key={dir}>
                                                <label className="block text-xs font-medium text-slate-400 mb-1">
                                                    Dir {dir.slice(1).toUpperCase()}
                                                </label>
                                                <input
                                                    type="text"
                                                    value={load[dir]}
                                                    onChange={(e) => updateLoad(load.id, dir, e.target.value)}
                                                    className="w-full bg-slate-900 border border-slate-600 rounded px-3 py-2 text-sm focus:outline-none focus:border-orange-500"
                                                    placeholder="0"
                                                />
                                            </div>
                                        ))}
                                    </div>
                                )}

                                {load.type === 'force' && (
                                    <div className="grid grid-cols-3 gap-3">
                                        {(['fx', 'fy', 'fz'] as const).map((dir) => (
                                            <div key={dir}>
                                                <label className="block text-xs font-medium text-slate-400 mb-1">
                                                    {dir.toUpperCase()} (N)
                                                </label>
                                                <input
                                                    type="number"
                                                    value={load[dir]}
                                                    onChange={(e) => updateLoad(load.id, dir, e.target.value)}
                                                    className="w-full bg-slate-900 border border-slate-600 rounded px-3 py-2 text-sm focus:outline-none focus:border-green-500"
                                                    placeholder="0"
                                                />
                                            </div>
                                        ))}
                                    </div>
                                )}

                                {load.type === 'pressure' && (
                                    <div>
                                        <label className="block text-xs font-medium text-slate-400 mb-1">
                                            Pressure (Pa)
                                        </label>
                                        <input
                                            type="number"
                                            value={load.pressure}
                                            onChange={(e) => updateLoad(load.id, 'pressure', e.target.value)}
                                            className="w-full bg-slate-900 border border-slate-600 rounded px-3 py-2 text-sm focus:outline-none focus:border-blue-500"
                                            placeholder="0"
                                        />
                                    </div>
                                )}
                            </div>

                            <button
                                onClick={() => removeLoad(load.id)}
                                className="ml-3 p-2 text-red-400 hover:bg-red-500/10 rounded transition-colors"
                            >
                                <Trash2 className="w-4 h-4" />
                            </button>
                        </div>
                    </div>
                ))}

                {loads.length === 0 && (
                    <div className="text-center text-slate-400 p-10">
                        No loads defined. Click a button above to add loads.
                    </div>
                )}
            </div>
        </div>
    )
}

--- ARQUIVO: frontend/src/components/config/MaterialConfig.tsx ---
import { useState, useEffect, useRef, useMemo } from 'react'
import { Plus, Trash2, Layers } from 'lucide-react'

interface Material {
    id: string
    name: string
    E: string // Young's Modulus
    nu: string // Poisson's Ratio
    rho: string // Density
    assignedGroups: string[]
}

interface MaterialConfigProps {
    projectPath: string | null
    availableGroups?: string[]
    nodeGroups?: string[]
    initialMaterials?: any[]
    onUpdate?: (materials: any[]) => void
}

export default function MaterialConfig({
    projectPath,
    availableGroups = [],
    nodeGroups = [],
    initialMaterials = [],
    onUpdate
}: MaterialConfigProps) {
    const isFirstRender = useRef(true)

    // Filtrar grupos que podem receber material (Remover Nodes)
    const validGroups = useMemo(() => {
        const nodeSet = new Set(nodeGroups)
        return availableGroups.filter(g => !nodeSet.has(g))
    }, [availableGroups, nodeGroups])

    const [materials, setMaterials] = useState<Material[]>([])
    const lastInitializedPath = useRef<string | null>(null)

    // Sync from props only on project change or initial load
    useEffect(() => {
        if (!projectPath) return

        // Se mudou de projeto ou ainda não inicializou
        if (lastInitializedPath.current !== projectPath) {
            if (initialMaterials && initialMaterials.length > 0) {
                const formatted = initialMaterials.map((m, index) => ({
                    id: (index + 1).toString(),
                    name: m.name,
                    E: (m.E / 1e6).toString(), // Pa to MPa
                    nu: (m.nu || 0).toString(),
                    rho: (m.rho || 0).toString(),
                    assignedGroups: m.assignedGroups || []
                }))
                setMaterials(formatted)
                lastInitializedPath.current = projectPath
            } else if (materials.length === 0) {
                // Default if empty
                setMaterials([{ id: '1', name: 'Steel_S355', E: '210000', nu: '0.3', rho: '7850', assignedGroups: [] }])
                lastInitializedPath.current = projectPath
            }
        }
    }, [projectPath, initialMaterials])

    useEffect(() => {
        if (isFirstRender.current) {
            isFirstRender.current = false
            return
        }

        if (onUpdate) {
            const exportData = materials.map(m => ({
                name: m.name,
                E: parseFloat(m.E) * 1e6, // MPa to Pa
                nu: parseFloat(m.nu),
                rho: parseFloat(m.rho),
                assignedGroups: m.assignedGroups
            }))
            onUpdate(exportData)
        }
    }, [materials, onUpdate])

    const addMaterial = () => {
        const newId = (materials.length + 1).toString()
        setMaterials([
            ...materials,
            {
                id: newId,
                name: `Material_${newId}`,
                E: '210000',
                nu: '0.3',
                rho: '7850',
                assignedGroups: []
            }
        ])
    }

    const removeMaterial = (id: string) => {
        if (materials.length === 1) {
            alert('At least one material is required')
            return
        }
        setMaterials(materials.filter(m => m.id !== id))
    }

    const updateMaterial = (id: string, field: keyof Material, value: any) => {
        setMaterials(
            materials.map(m => (m.id === id ? { ...m, [field]: value } : m))
        )
    }

    const toggleGroup = (materialId: string, groupName: string) => {
        const material = materials.find(m => m.id === materialId)
        if (!material) return

        const currentGroups = material.assignedGroups || []
        const newGroups = currentGroups.includes(groupName)
            ? currentGroups.filter(g => g !== groupName)
            : [...currentGroups, groupName]

        updateMaterial(materialId, 'assignedGroups', newGroups)
    }

    if (!projectPath) {
        return <div className="p-10 text-center text-slate-500">Please select a project.</div>
    }

    return (
        <div className="flex flex-col h-full w-full p-4 overflow-hidden">
            {/* Header */}
            <div className="flex justify-between items-center mb-6 shrink-0">
                <div>
                    <h3 className="text-xl font-bold text-slate-100 flex items-center gap-2">
                        <span className="p-2 bg-blue-500/20 rounded text-blue-400">⚙️</span>
                        Material Properties
                    </h3>
                    <p className="text-sm text-slate-400 mt-1">Define properties and assign them to mesh groups.</p>
                </div>
                <button
                    onClick={addMaterial}
                    className="flex items-center gap-2 px-6 py-2.5 bg-blue-600 hover:bg-blue-500 text-white font-semibold rounded-xl transition-all shadow-lg shadow-blue-500/20 active:scale-95"
                >
                    <Plus className="w-5 h-5" />
                    Add Material
                </button>
            </div>

            {/* Materials List */}
            <div className="space-y-6 flex-1 overflow-y-auto pr-2 custom-scrollbar">
                {materials.map((material) => (
                    <div
                        key={material.id}
                        className="bg-slate-800/50 backdrop-blur border border-slate-700/50 rounded-2xl p-6 shadow-xl hover:border-slate-600/50 transition-colors"
                    >
                        <div className="flex justify-between items-start mb-6">
                            <div className="flex-1 max-w-md">
                                <label className="block text-[10px] font-bold text-slate-500 uppercase tracking-widest mb-1 ml-1">
                                    Material Identification
                                </label>
                                <input
                                    type="text"
                                    value={material.name}
                                    onChange={(e) => updateMaterial(material.id, 'name', e.target.value)}
                                    className="w-full text-xl font-bold bg-slate-900/50 border border-slate-700 rounded-xl px-4 py-2 text-white focus:outline-none focus:ring-2 focus:ring-blue-500/50 focus:border-blue-500 transition-all"
                                    placeholder="Ex: Steel S355"
                                />
                            </div>
                            <button
                                onClick={() => removeMaterial(material.id)}
                                className="p-2.5 text-slate-500 hover:text-red-400 hover:bg-red-400/10 rounded-xl transition-all ml-4"
                                title="Delete Material"
                                disabled={materials.length === 1}
                            >
                                <Trash2 className="w-5 h-5" />
                            </button>
                        </div>

                        <div className="grid grid-cols-1 md:grid-cols-3 gap-6 mb-8">
                            {/* Young's Modulus */}
                            <div className="bg-slate-900/40 p-4 rounded-xl border border-slate-700/30">
                                <label className="block text-[10px] font-bold text-slate-500 uppercase tracking-widest mb-2">
                                    Young's Modulus E (MPa)
                                </label>
                                <input
                                    type="text"
                                    value={material.E}
                                    onChange={(e) => updateMaterial(material.id, 'E', e.target.value)}
                                    className="w-full bg-transparent text-lg font-mono text-blue-300 focus:outline-none"
                                    placeholder="210000"
                                />
                            </div>

                            {/* Poisson's Ratio */}
                            <div className="bg-slate-900/40 p-4 rounded-xl border border-slate-700/30">
                                <label className="block text-[10px] font-bold text-slate-500 uppercase tracking-widest mb-2">
                                    Poisson's Ratio ν
                                </label>
                                <input
                                    type="text"
                                    value={material.nu}
                                    onChange={(e) => updateMaterial(material.id, 'nu', e.target.value)}
                                    className="w-full bg-transparent text-lg font-mono text-blue-300 focus:outline-none"
                                    placeholder="0.30"
                                />
                            </div>

                            {/* Density */}
                            <div className="bg-slate-900/40 p-4 rounded-xl border border-slate-700/30">
                                <label className="block text-[10px] font-bold text-slate-500 uppercase tracking-widest mb-2">
                                    Density ρ (kg/m³)
                                </label>
                                <input
                                    type="text"
                                    value={material.rho}
                                    onChange={(e) => updateMaterial(material.id, 'rho', e.target.value)}
                                    className="w-full bg-transparent text-lg font-mono text-blue-300 focus:outline-none"
                                    placeholder="7850"
                                />
                            </div>
                        </div>

                        {/* Group Assignment Section */}
                        <div className="border-t border-slate-700/50 pt-6">
                            <h4 className="text-xs font-bold text-slate-400 uppercase tracking-widest mb-4 flex items-center gap-2">
                                <Layers className="w-3.5 h-3.5" />
                                Assigned Groups (Beams, Shells, Solids)
                            </h4>

                            {validGroups.length > 0 ? (
                                <div className="flex flex-wrap gap-2">
                                    {validGroups.map(group => {
                                        const isAssigned = (material.assignedGroups || []).includes(group)
                                        // Verificar se já está em outro material
                                        const isTaken = materials.some(m => m.id !== material.id && m.assignedGroups.includes(group))

                                        return (
                                            <button
                                                key={group}
                                                disabled={isTaken}
                                                onClick={() => toggleGroup(material.id, group)}
                                                className={`
                                                    px-3 py-1.5 rounded-lg text-xs font-medium border transition-all flex items-center gap-2
                                                    ${isAssigned
                                                        ? 'bg-blue-500 border-blue-400 text-white shadow-lg shadow-blue-500/20'
                                                        : isTaken
                                                            ? 'bg-slate-800/30 border-slate-800 text-slate-700 cursor-not-allowed opacity-50'
                                                            : 'bg-slate-900/50 border-slate-700 text-slate-400 hover:border-slate-500 hover:text-slate-200'
                                                    }
                                                `}
                                            >
                                                {isAssigned ? '✓' : ''}
                                                {group}
                                                {isTaken && <span className="text-[9px] opacity-70">(Used)</span>}
                                            </button>
                                        )
                                    })}
                                </div>
                            ) : (
                                <div className="bg-slate-900/30 border border-dashed border-slate-700 rounded-xl p-4 text-center">
                                    <p className="text-xs text-slate-500 italic">No mesh groups detected. Please open a project with valid mesh files.</p>
                                </div>
                            )}
                        </div>
                    </div>
                ))}
            </div>
        </div>
    )
}

--- ARQUIVO: frontend/src/components/config/MeshConfig.tsx ---
import { FileBox } from 'lucide-react'

interface MeshConfigProps {
    projectPath: string | null
    meshes: string[]
}

export default function MeshConfig({ projectPath, meshes }: MeshConfigProps) {
    if (!projectPath) return null

    return (
        <div className="flex flex-col h-full w-full p-4">
            <h3 className="text-lg font-semibold text-slate-200 mb-4">Detected Meshes</h3>

            {meshes.length === 0 ? (
                <div className="p-10 text-center text-slate-500 border border-dashed border-slate-700 rounded-lg">
                    No .med files detected in project folder.
                </div>
            ) : (
                <div className="space-y-3">
                    {meshes.map((meshName) => (
                        <div key={meshName} className="flex items-center gap-3 p-4 bg-slate-800 border border-slate-700 rounded-lg">
                            <FileBox className="w-8 h-8 text-blue-500" />
                            <div>
                                <div className="text-sm font-bold text-slate-200">{meshName}</div>
                                <div className="text-xs text-slate-500">
                                    Status: <span className="text-green-500">Ready for Inspection</span>
                                </div>
                            </div>
                        </div>
                    ))}
                </div>
            )}

            <div className="mt-8 p-4 bg-slate-900/50 rounded text-xs text-slate-500">
                <p>Mesh inspection runs automatically when opening the project.</p>
                <p>Ensure <b>Code_Aster path</b> is correctly set in Settings to enable group detection.</p>
            </div>
        </div>
    )
}

--- ARQUIVO: frontend/src/components/config/ModelConfig.tsx ---
import { useState, useEffect, useRef } from 'react'
import { Check, X } from 'lucide-react'

interface MeshGroup {
    name: string
    meshFile: string // NEW: Source identification
    selected: boolean
    count: number
    composition: string
    category: string
    model: string
    phenomenon: string
}

interface ModelConfigProps {
    projectPath: string | null
    meshGroups?: any // NEW: Provided by StructuralWorkspace
    currentGeometries?: any[]
    onUpdate?: (geometries: any[]) => void
}

const MODEL_OPTIONS = {
    '1D': [
        { value: 'POU_D_T', label: 'Beam (Timoshenko) - POU_D_T' },
        { value: 'POU_D_E', label: 'Beam (Euler) - POU_D_E' },
        { value: 'BARRE', label: 'Truss/Bar - BARRE' },
        { value: 'CABLE', label: 'Cable - CABLE' }
    ],
    '2D': [
        { value: 'DKT', label: 'Plate (Thin) - DKT' },
        { value: 'DST', label: 'Plate (Thick) - DST' },
        { value: 'COQUE_3D', label: 'Shell 3D - COQUE_3D' },
        { value: 'MEMBRANE', label: 'Membrane - MEMBRANE' },
        { value: 'C_PLAN', label: 'Plane Strain - C_PLAN' },
        { value: 'D_PLAN', label: 'Plane Stress - D_PLAN' },
        { value: 'AXIS', label: 'Axisymmetric - AXIS' }
    ],
    '3D': [
        { value: '3D', label: 'Solid/Volume - 3D' }
    ]
}

export default function ModelConfig({ projectPath, meshGroups, currentGeometries = [], onUpdate }: ModelConfigProps) {
    const [groups, setGroups] = useState<MeshGroup[]>([])
    const isFirstRender = useRef(true)
    const lastGroupsSignatureRef = useRef<string>('')
    const lastExportSignatureRef = useRef<string>('')

    useEffect(() => {
        if (meshGroups) {
            // console.log("🔍 [DEBUG MODEL] Component Props meshGroups:", meshGroups); // Removed redundant log
            // console.log("🔍 [DEBUG MODEL] Global State meshes:", (window as any).projectState?.meshes); // Removed redundant log
        }
    }, [meshGroups]);

    useEffect(() => {
        if (isFirstRender.current) {
            isFirstRender.current = false
            return
        }

        if (onUpdate) {
            const exportData = groups
                .filter(g => g.selected)
                .map(g => ({
                    group: g.name,
                    _meshFile: g.meshFile, // Internal tracking
                    // ...MERGE logic
                    ...(() => {
                        const existing = currentGeometries.find(c => c.group === g.name && c._meshFile === g.meshFile) || {}
                        return {
                            ...existing,
                            type: g.model,
                            formulation: (g.model === 'DKT' || g.model === 'DST') ? g.model : undefined,
                            phenomenon: 'MECANIQUE',
                            _category: g.category
                        }
                    })()
                }))

            // Stability check: Only update parent if serialized data changed
            const expSignature = JSON.stringify(exportData)
            if (expSignature !== lastExportSignatureRef.current) {
                lastExportSignatureRef.current = expSignature
                onUpdate(exportData)
            }
        }
    }, [groups, onUpdate])

    // React to prop changes independently
    useEffect(() => {
        if (meshGroups) {
            // PERFORMANCE PROTOCOL: Signature-based check (only files and group names)
            // This prevents the infinite loop while remaining extremely fast
            const signature = JSON.stringify(Object.entries(meshGroups).map(([file, groups]: [string, any]) => ({
                file,
                groups: Object.keys(groups).sort()
            })))

            if (signature === lastGroupsSignatureRef.current) {
                return // Structure hasn't changed, skip re-mapping to stop loop
            }
            lastGroupsSignatureRef.current = signature

            console.log("MODEL_CHILD: Mapping independent meshGroups keys:", Object.keys(meshGroups))
            const loadedGroups: MeshGroup[] = []

            // Iterate through each mesh file independently
            Object.entries(meshGroups).forEach(([fileName, groupsInFile]: [string, any]) => {
                Object.entries(groupsInFile).forEach(([groupName, info]: [string, any]) => {
                    const category = info.category || detectCategory(info.types || {})
                    const compStr = info.types ? Object.entries(info.types)
                        .map(([t, q]) => `${t}:${q}`)
                        .join(', ') : category

                    const existingConfig = currentGeometries.find((c: any) => c.group === groupName && c._meshFile === fileName)

                    loadedGroups.push({
                        name: groupName,
                        meshFile: fileName,
                        selected: currentGeometries.length > 0 ? !!existingConfig : true,
                        count: info.count,
                        composition: compStr,
                        category: category,
                        model: existingConfig?.type || detectDefaultModel(category),
                        phenomenon: 'MECANIQUE'
                    })
                })
            })
            setGroups(loadedGroups)
        }
    }, [meshGroups, currentGeometries])

    const detectCategory = (typesObj: any): string => {
        const types = Object.keys(typesObj)
        if (types.some(t => t === 'Node')) return 'Node'
        if (types.some(t => t.includes('HEXA') || t.includes('TETRA') || t.includes('PENTA'))) return '3D'
        if (types.some(t => t.includes('QUAD') || t.includes('TRIA'))) return '2D'
        if (types.some(t => t.includes('SEG'))) return '1D'
        return '3D'
    }

    const detectDefaultModel = (category: string): string => {
        if (category === 'Node') return 'Node'
        if (category === '3D') return '3D'
        if (category === '2D') return 'COQUE_3D'
        if (category === '1D') return 'POU_D_T'
        return '3D'
    }

    // REDUNDANT INTERNAL FETCH REMOVED (Centralized in StructuralWorkspace)
    // const loadMeshGroups = async () => { ... }

    const handleCheckboxChange = (index: number) => {
        const newGroups = [...groups]
        newGroups[index].selected = !newGroups[index].selected
        setGroups(newGroups)
    }

    const handleModelChange = (index: number, newModel: string) => {
        const newGroups = [...groups]
        newGroups[index].model = newModel
        setGroups(newGroups)
    }

    const toggleAll = (select: boolean) => {
        const newGroups = groups.map(g => ({ ...g, selected: select }))
        setGroups(newGroups)
    }

    if (!projectPath) {
        return <div className="p-10 text-center text-slate-500">Please select a project.</div>
    }

    if (groups.length === 0) {
        return (
            <div className="flex flex-col items-center justify-center h-full p-10 text-slate-400">
                <div className="text-4xl mb-4 text-slate-600">🕸️</div>
                <h3 className="text-xl font-bold text-slate-300 mb-2">No Mesh Groups Detected</h3>
                <p className="mb-6 text-center max-w-md text-sm">Please scan workspace first from the Toolbar.</p>
                <button
                    onClick={() => window.location.reload()} // Simple fallback as parent should handle refresh
                    className="px-6 py-2 bg-slate-800 hover:bg-slate-700 rounded transition-colors border border-slate-600"
                >
                    Refresh Data
                </button>
            </div>
        )
    }

    return (
        <div className="flex flex-col h-full w-full">
            {/* Header */}
            <div className="flex justify-between items-center mb-4 bg-slate-800/50 p-3 rounded-lg border border-slate-700">
                <div className="flex gap-2">
                    <button
                        onClick={() => toggleAll(true)}
                        className="px-3 py-1.5 text-xs font-semibold bg-slate-700 hover:bg-slate-600 border border-slate-600 rounded transition-colors flex items-center gap-1"
                    >
                        <Check className="w-3 h-3" />
                        Check All
                    </button>
                    <button
                        onClick={() => toggleAll(false)}
                        className="px-3 py-1.5 text-xs font-semibold bg-slate-700 hover:bg-slate-600 border border-slate-600 rounded transition-colors flex items-center gap-1"
                    >
                        <X className="w-3 h-3" />
                        Uncheck All
                    </button>
                </div>
                <div className="text-[10px] text-slate-500 uppercase font-bold tracking-wider">
                    Model Definition
                </div>
            </div>

            {/* Table */}
            <div className="bg-slate-800 border border-slate-700 rounded-lg overflow-hidden shadow-xl flex-1 flex flex-col">
                <div className="overflow-y-auto flex-1">
                    <table className="w-full text-left border-collapse">
                        <thead className="bg-slate-950 text-slate-400 text-[10px] uppercase tracking-wider sticky top-0 z-10">
                            <tr>
                                <th className="p-3 w-12 text-center border-b border-slate-800">Sel.</th>
                                <th className="p-3 w-1/4 border-b border-slate-800">Mesh Source</th>
                                <th className="p-3 w-1/4 border-b border-slate-800">Group Name</th>
                                <th className="p-3 w-1/4 border-b border-slate-800">Composition</th>
                                <th className="p-3 w-1/4 border-b border-slate-800">Model Definition</th>
                            </tr>
                        </thead>

                        <tbody className="divide-y divide-slate-700/50 text-sm">
                            {groups.map((group, idx) => {
                                if (group.category === 'Node' || group.category === 'Point') return null

                                return (
                                    <tr
                                        key={idx}
                                        className={`transition-colors hover:bg-slate-700/30 ${!group.selected ? 'opacity-50 grayscale-[50%]' : ''
                                            }`}
                                    >
                                        <td className="p-3 text-center">
                                            <input
                                                type="checkbox"
                                                checked={group.selected}
                                                onChange={() => handleCheckboxChange(idx)}
                                                className="w-4 h-4 rounded border-slate-500 bg-slate-700 accent-blue-600 cursor-pointer"
                                            />
                                        </td>

                                        <td className="p-3 text-xs font-mono text-slate-500 truncate max-w-[120px]" title={group.meshFile}>
                                            {group.meshFile}
                                        </td>

                                        <td className="p-3 font-mono font-medium text-blue-300">
                                            {group.name}
                                        </td>

                                        <td className="p-3 text-xs text-slate-400 font-mono">
                                            {group.composition}
                                        </td>

                                        <td className="p-3">
                                            <select
                                                value={group.model}
                                                onChange={(e) => handleModelChange(idx, e.target.value)}
                                                disabled={!group.selected}
                                                className="w-full bg-slate-900 border border-slate-600 rounded px-2 py-1.5 text-xs focus:ring-1 focus:ring-blue-500 outline-none disabled:cursor-not-allowed transition-colors hover:border-slate-500"
                                            >
                                                {group.category === '3D' && (
                                                    <optgroup label="Volume (3D)">
                                                        {MODEL_OPTIONS['3D'].map(op => (
                                                            <option key={op.value} value={op.value}>{op.label}</option>
                                                        ))}
                                                    </optgroup>
                                                )}

                                                {group.category === '2D' && (
                                                    <optgroup label="Plate / Shell (2D)">
                                                        {MODEL_OPTIONS['2D'].map(op => (
                                                            <option key={op.value} value={op.value}>{op.label}</option>
                                                        ))}
                                                    </optgroup>
                                                )}

                                                {group.category === '1D' && (
                                                    <optgroup label="Beam / Truss (1D)">
                                                        {MODEL_OPTIONS['1D'].map(op => (
                                                            <option key={op.value} value={op.value}>{op.label}</option>
                                                        ))}
                                                    </optgroup>
                                                )}
                                            </select>
                                        </td>
                                    </tr>
                                )
                            })}
                        </tbody>
                    </table>
                </div>

                {/* Footer */}
                <div className="bg-slate-950 p-2 border-t border-slate-800 text-[10px] text-slate-500 flex justify-between px-4">
                    <span>Detected Groups: {groups.length}</span>
                    <span>Selected: {groups.filter(g => g.selected).length}</span>
                </div>
            </div>
        </div>
    )
}

--- ARQUIVO: frontend/src/components/config/RestrictionConfig.tsx ---
import { useState, useEffect, useRef } from 'react'
import { Plus, Trash2 } from 'lucide-react'

interface Restriction {
    id: string
    name: string
    group: string
    dx: boolean
    dy: boolean
    dz: boolean
    drx: boolean
    dry: boolean
    drz: boolean
}

interface RestrictionConfigProps {
    projectPath: string | null
    availableGroups?: string[]
    initialRestrictions?: any[]
    onUpdate?: (restrictions: any[]) => void
}

export default function RestrictionConfig({
    projectPath,
    availableGroups = [],
    initialRestrictions = [],
    onUpdate
}: RestrictionConfigProps) {
    const [restrictions, setRestrictions] = useState<Restriction[]>([])
    const isFirstRender = useRef(true)
    const lastExportRef = useRef('')

    // DEBUG LOG
    useEffect(() => {
        console.log("RESTRICTION_CHILD: Mounted. Initial Restrictions Prop:", initialRestrictions)
    }, [])

    useEffect(() => {
        console.log("RESTRICTION_CHILD: Internal State Updated:", restrictions)
    }, [restrictions])

    // Load initial restrictions if provided (Persistence)
    useEffect(() => {
        console.log("RESTRICTION_CHILD: Effect - initialRestrictions changed", initialRestrictions)
        if (initialRestrictions.length > 0) { // REMOVED restrictions.length === 0 check to force sync if needed, or check logic
            console.log("RESTRICTION_CHILD: Loading from props...")
            const formatted = initialRestrictions.map((r, index) => {
                // Reconstruct internal state structure from saved config structure
                // Saved: { name, group, dof: { DX: 0, ... } }
                return {
                    id: (index + 1).toString(),
                    name: r.name,
                    group: r.group,
                    dx: r.dof?.DX !== null,
                    dy: r.dof?.DY !== null,
                    dz: r.dof?.DZ !== null,
                    drx: r.dof?.DRX !== null,
                    dry: r.dof?.DRY !== null,
                    drz: r.dof?.DRZ !== null
                }
            })
            setRestrictions(formatted)
        }
    }, [initialRestrictions])

    useEffect(() => {
        // Prevent overwriting parent state on initial mount
        if (isFirstRender.current) {
            isFirstRender.current = false
            return
        }

        if (onUpdate) {
            const exportData = restrictions.map(r => ({
                name: String(r.name || ''),
                group: String(r.group || ''),
                dof: {
                    DX: r.dx ? 0 : null,
                    DY: r.dy ? 0 : null,
                    DZ: r.dz ? 0 : null,
                    DRX: r.drx ? 0 : null,
                    DRY: r.dry ? 0 : null,
                    DRZ: r.drz ? 0 : null
                }
            }))

            // Simple stringification check to avoid redundant parent updates
            const currentString = JSON.stringify(exportData)
            if (lastExportRef.current !== currentString) {
                lastExportRef.current = currentString
                onUpdate(exportData)
            }
        }
    }, [restrictions, onUpdate])

    const addRestriction = () => {
        const newId = (restrictions.length + 1).toString()
        setRestrictions([
            ...restrictions,
            {
                id: newId,
                name: `Restriction_${newId}`,
                group: availableGroups[0] || '',
                dx: true,
                dy: true,
                dz: true,
                drx: false,
                dry: false,
                drz: false
            }
        ])
    }

    const removeRestriction = (id: string) => {
        setRestrictions(restrictions.filter(r => r.id !== id))
    }

    const updateRestriction = (id: string, field: keyof Restriction, value: any) => {
        setRestrictions(
            restrictions.map(r => (r.id === id ? { ...r, [field]: value } : r))
        )
    }

    if (!projectPath) {
        return <div className="p-10 text-center text-slate-500">Please select a project.</div>
    }

    return (
        <div className="flex flex-col h-full w-full p-4">
            {/* Header */}
            <div className="flex justify-between items-center mb-4">
                <h3 className="text-lg font-semibold text-slate-200">Boundary Conditions (Restrictions)</h3>
                <button
                    onClick={addRestriction}
                    disabled={availableGroups.length === 0}
                    className="flex items-center gap-2 px-4 py-2 bg-purple-600 hover:bg-purple-500 rounded-lg transition-colors disabled:opacity-50"
                >
                    <Plus className="w-4 h-4" />
                    Add Restriction
                </button>
            </div>

            {availableGroups.length === 0 && (
                <div className="text-center text-slate-400 p-10">
                    No mesh groups available. Please configure Model first.
                </div>
            )}

            {/* Restrictions List */}
            <div className="space-y-4 flex-1 overflow-y-auto">
                {restrictions.map((restriction) => (
                    <div
                        key={restriction.id}
                        className="bg-slate-800 border border-slate-700 rounded-lg p-4"
                    >
                        <div className="flex justify-between items-start mb-4">
                            <div className="flex-1 grid grid-cols-2 gap-3">
                                <div>
                                    <label className="block text-xs font-medium text-slate-400 mb-1">
                                        Name
                                    </label>
                                    <input
                                        type="text"
                                        value={restriction.name}
                                        onChange={(e) => updateRestriction(restriction.id, 'name', e.target.value)}
                                        className="w-full bg-slate-900 border border-slate-600 rounded px-3 py-2 text-sm focus:outline-none focus:border-purple-500"
                                        placeholder="Restriction Name"
                                    />
                                </div>
                                <div>
                                    <label className="block text-xs font-medium text-slate-400 mb-1">
                                        Mesh Group
                                    </label>
                                    <select
                                        value={restriction.group}
                                        onChange={(e) => updateRestriction(restriction.id, 'group', e.target.value)}
                                        className="w-full bg-slate-900 border border-slate-600 rounded px-3 py-2 text-sm focus:outline-none focus:border-purple-500"
                                    >
                                        {availableGroups.map((group) => (
                                            <option key={group} value={group}>
                                                {group}
                                            </option>
                                        ))}
                                    </select>
                                </div>
                            </div>
                            <button
                                onClick={() => removeRestriction(restriction.id)}
                                className="ml-3 p-2 text-red-400 hover:bg-red-500/10 rounded transition-colors"
                            >
                                <Trash2 className="w-4 h-4" />
                            </button>
                        </div>

                        {/* DOF Checkboxes */}
                        <div className="border-t border-slate-700 pt-3">
                            <p className="text-xs text-slate-400 mb-2">Restricted Degrees of Freedom:</p>
                            <div className="grid grid-cols-6 gap-2">
                                {(['dx', 'dy', 'dz', 'drx', 'dry', 'drz'] as const).map((dof) => (
                                    <label
                                        key={dof}
                                        className="flex items-center gap-2 bg-slate-900/50 p-2 rounded cursor-pointer hover:bg-slate-900 transition-colors"
                                    >
                                        <input
                                            type="checkbox"
                                            checked={restriction[dof]}
                                            onChange={(e) => updateRestriction(restriction.id, dof, e.target.checked)}
                                            className="w-4 h-4 rounded border-slate-500 bg-slate-700 accent-purple-600"
                                        />
                                        <span className="text-xs font-mono uppercase">{dof}</span>
                                    </label>
                                ))}
                            </div>
                        </div>
                    </div>
                ))}
            </div>
        </div>
    )
}

--- ARQUIVO: frontend/src/components/config/ThreeDModel.tsx ---

import { useState, useEffect, useMemo, useRef } from 'react'
import { Canvas } from '@react-three/fiber'
import { OrbitControls, Center } from '@react-three/drei'
import * as THREE from 'three'

interface ThreeDViewProps {
    projectPath: string | null
    geometries: any[]
    meshes?: string[]
}

// --- GEOMETRY FACTORY ---
const createSectionGeometry = (type: string, params: any) => {
    const length = 1.0
    const shape = new THREE.Shape()

    try {
        if (type === 'RECTANGLE') {
            const hy = parseFloat(params.hy) || 100
            const hz = parseFloat(params.hz) || 50
            shape.moveTo(-hy / 2, -hz / 2)
            shape.lineTo(hy / 2, -hz / 2)
            shape.lineTo(hy / 2, hz / 2)
            shape.lineTo(-hy / 2, hz / 2)
            shape.lineTo(-hy / 2, -hz / 2)
        } else if (type === 'CIRCLE' || type === 'TUBE') {
            const r = parseFloat(params.r) || 50
            shape.absarc(0, 0, r, 0, Math.PI * 2, false)
            if (type === 'TUBE') {
                const hole = new THREE.Path()
                const r_inner = r - (parseFloat(params.t) || 5)
                hole.absarc(0, 0, r_inner, 0, Math.PI * 2, true)
                shape.holes.push(hole)
            }
        } else if (type === 'I_SECTION') {
            const h = parseFloat(params.h) || 200
            const tw = parseFloat(params.tw) || 6
            const bf_top = parseFloat(params.bf_top) || 100
            const bf_bot = parseFloat(params.bf_bot) || 100
            const tf_top_val = parseFloat(params.tf_top) || 10
            const tf_bot_val = parseFloat(params.tf_bot) || 10

            shape.moveTo(-bf_top / 2, h / 2)
            shape.lineTo(bf_top / 2, h / 2)
            shape.lineTo(bf_top / 2, h / 2 - tf_top_val)
            shape.lineTo(tw / 2, h / 2 - tf_top_val)
            shape.lineTo(tw / 2, -h / 2 + tf_bot_val)
            shape.lineTo(bf_bot / 2, -h / 2 + tf_bot_val)
            shape.lineTo(bf_bot / 2, -h / 2)
            shape.lineTo(-bf_bot / 2, -h / 2)
            shape.lineTo(-bf_bot / 2, -h / 2 + tf_bot_val)
            shape.lineTo(-tw / 2, -h / 2 + tf_bot_val)
            shape.lineTo(-tw / 2, h / 2 - tf_top_val)
            shape.lineTo(-bf_top / 2, h / 2 - tf_top_val)
            shape.lineTo(-bf_top / 2, h / 2)
        } else {
            const s = 50
            shape.moveTo(-s, -s)
            shape.lineTo(s, -s)
            shape.lineTo(s, s)
            shape.lineTo(-s, s)
        }

        const extrudeSettings = {
            steps: 1,
            depth: length,
            bevelEnabled: false
        };

        return new THREE.ExtrudeGeometry(shape, extrudeSettings)

    } catch (e) {
        console.error("Geo Gen Error", e)
        return new THREE.BoxGeometry(10, 10, 100)
    }
}

// --- BEAM INSTANCER ---
const InstancedBeams = ({ groupName, nodes, connectivity, sectionDef }: any) => {
    const meshRef = useRef<THREE.InstancedMesh>(null)
    const geometry = useMemo(() => createSectionGeometry(sectionDef.section_type || 'RECTANGLE', sectionDef.section_params || {}), [sectionDef])

    const matrices = useMemo(() => {
        const mats: THREE.Matrix4[] = []
        const dummy = new THREE.Object3D()
        const v1 = new THREE.Vector3()
        const v2 = new THREE.Vector3()

        connectivity.forEach((elem: number[]) => {
            if (elem.length < 2) return
            const n1 = nodes[elem[0]]
            const n2 = nodes[elem[1]]

            // Safety check for nodes
            if (!n1 || !n2) return

            v1.set(n1[0], n1[1], n1[2])
            v2.set(n2[0], n2[1], n2[2])

            const dist = v1.distanceTo(v2)
            if (dist < 0.0001) return // Avoid zero length issues

            dummy.position.copy(v1)
            dummy.lookAt(v2)
            dummy.scale.set(1, 1, dist)

            const rot = (parseFloat(sectionDef.section_params?.rotation || 0)) * Math.PI / 180
            dummy.rotateZ(rot)

            dummy.updateMatrix()
            mats.push(dummy.matrix.clone())
        })
        return mats
    }, [nodes, connectivity, sectionDef])

    useEffect(() => {
        if (meshRef.current) {
            matrices.forEach((mat, i) => {
                meshRef.current?.setMatrixAt(i, mat)
            })
            if (meshRef.current.instanceMatrix) {
                meshRef.current.instanceMatrix.needsUpdate = true
            }
        }
    }, [matrices])

    if (!sectionDef || matrices.length === 0) return null

    return (
        <instancedMesh ref={meshRef} args={[geometry, undefined, matrices.length] as any}>
            <meshStandardMaterial color={stringToColor(groupName)} roughness={0.7} metalness={0.1} />
        </instancedMesh>
    )
}

// Simple Line Renderer
const LineGroup = ({ groupName, nodes, connectivity }: any) => {
    const positionBuffer = useMemo(() => {
        const p: number[] = []
        connectivity.forEach((c: number[]) => {
            if (c.length >= 2) {
                const n1 = nodes[c[0]]
                const n2 = nodes[c[1]]
                if (n1 && n2) {
                    p.push(n1[0], n1[1], n1[2])
                    p.push(n2[0], n2[1], n2[2])
                }
            }
        })
        return new Float32Array(p)
    }, [nodes, connectivity])

    if (positionBuffer.length === 0) return null

    return (
        <lineSegments>
            <bufferGeometry>
                <bufferAttribute
                    attach="attributes-position"
                    count={positionBuffer.length / 3}
                    args={[positionBuffer, 3]}
                />
            </bufferGeometry>
            <lineBasicMaterial color={stringToColor(groupName)} linewidth={2} />
        </lineSegments>
    )
}

export default function ThreeDModel({ projectPath, geometries, meshes = [] }: ThreeDViewProps) {
    const [allMeshData, setAllMeshData] = useState<any[]>([])
    const [loading, setLoading] = useState(false)
    const [error, setError] = useState<string | null>(null)

    useEffect(() => {
        if (!projectPath) return

        const fetchAllMeshes = async () => {
            setLoading(true)
            setAllMeshData([])
            setError(null)
            try {
                // Default to empty array if meshes is undefined
                const filesToLoad = (meshes && meshes.length > 0) ? meshes : []

                if (filesToLoad.length === 0) {
                    const res = await fetch('/api/get_mesh_data', {
                        method: 'POST',
                        headers: { 'Content-Type': 'application/json' },
                        body: JSON.stringify({ folder_path: projectPath })
                    })
                    const data = await res.json()
                    if (data.status === 'success') {
                        setAllMeshData([data])
                    }
                } else {
                    const promises = filesToLoad.map(m =>
                        fetch('/api/get_mesh_data', {
                            method: 'POST',
                            headers: { 'Content-Type': 'application/json' },
                            body: JSON.stringify({ folder_path: projectPath, mesh_filename: m })
                        }).then(r => r.json())
                    )

                    const results = await Promise.all(promises)
                    const validData = results.filter(r => r.status === 'success')
                    if (validData.length > 0) {
                        setAllMeshData(validData)
                    } else {
                        // Fallback
                        const res = await fetch('/api/get_mesh_data', {
                            method: 'POST',
                            headers: { 'Content-Type': 'application/json' },
                            body: JSON.stringify({ folder_path: projectPath })
                        })
                        const data = await res.json()
                        if (data.status === 'success') {
                            setAllMeshData([data])
                        }
                    }
                }

            } catch (e: any) {
                console.error(e)
                setError(e.message)
            } finally {
                setLoading(false)
            }
        }
        fetchAllMeshes()
    }, [projectPath, meshes])

    if (loading) return <div className="text-white flex items-center justify-center h-full gap-2">
        <div className="animate-spin rounded-full h-4 w-4 border-b-2 border-white"></div>
        Loading 3D Meshes...
    </div>
    if (error) return <div className="text-red-500 flex items-center justify-center h-full p-10 text-center">
        Error loading 3D Model: {typeof error === 'string' ? error : JSON.stringify(error)}
    </div>

    const allGroupsFlat = useMemo(() => {
        const list: { name: string, hasGeo: boolean, color: string }[] = []
        allMeshData.forEach(mesh => {
            if (mesh && mesh.groups) {
                Object.keys(mesh.groups).forEach(g => {
                    if (!list.find(i => i.name === g)) {
                        const hasGeo = geometries.find((geo: any) => geo.group === g)
                        list.push({
                            name: g,
                            hasGeo: !!hasGeo,
                            color: stringToColor(g)
                        })
                    }
                })
            }
        })
        return list
    }, [allMeshData, geometries])

    return (
        <div className="w-full h-full bg-slate-900 overflow-hidden relative">
            <div className="absolute top-4 right-4 z-10 bg-slate-800/80 p-2 rounded text-xs text-white backdrop-blur max-h-[80%] overflow-y-auto">
                <h4 className="font-bold mb-1">Groups ({allGroupsFlat.length})</h4>
                {allGroupsFlat.map((g, idx) => (
                    <div key={`${g.name}-${idx}`} className="flex items-center gap-2 mb-1">
                        <span className="w-3 h-3 rounded-full" style={{ background: String(g.color || '#ccc') }}></span>
                        <span>{String(g.name || 'Unknown')} {g.hasGeo ? '(Beam)' : ''}</span>
                    </div>
                ))}
            </div>

            <Canvas camera={{ position: [1000, 1000, 1000], fov: 45, far: 100000 }}>
                <color attach="background" args={['#0f172a']} />
                <ambientLight intensity={0.5} />
                <directionalLight position={[1000, 2000, 1000]} intensity={1.5} />
                <Center>
                    <group>
                        {allMeshData.map((mesh, i) => (
                            <group key={i}>
                                {mesh && mesh.groups && Object.entries(mesh.groups).map(([name, data]: [string, any]) => {
                                    const geoDef = geometries.find((g: any) => g.group === name)
                                    const isBeam = geoDef && (geoDef.type?.includes('POU') || geoDef.type?.includes('BARRE'))

                                    // Safety: Check if data.type and data.elements exist
                                    if (!data || !data.elements) return null

                                    if (isBeam && data.type === 'line') {
                                        return (
                                            <InstancedBeams
                                                key={`${i}-${name}-beam`}
                                                groupName={name}
                                                nodes={mesh.nodes}
                                                connectivity={data.elements}
                                                sectionDef={geoDef}
                                            />
                                        )
                                    } else {
                                        if (data.type === 'line') {
                                            return (
                                                <LineGroup
                                                    key={`${i}-${name}-line`}
                                                    groupName={name}
                                                    nodes={mesh.nodes}
                                                    connectivity={data.elements}
                                                />
                                            )
                                        }
                                        return null
                                    }
                                })}
                            </group>
                        ))}
                    </group>
                </Center>
                <OrbitControls makeDefault />
            </Canvas>
        </div >
    )
}

function stringToColor(str: string) {
    let hash = 0;
    for (let i = 0; i < str.length; i++) {
        hash = str.charCodeAt(i) + ((hash << 5) - hash);
    }
    const c = (hash & 0x00FFFFFF).toString(16).toUpperCase();
    return '#' + '00000'.substring(0, 6 - c.length) + c;
}

--- ARQUIVO: frontend/src/components/config/VtkMeshViewer.tsx ---
import React, { useEffect, useRef, useState, useCallback } from 'react'
import {
    Maximize,
    ZoomIn,
    ZoomOut,
    Box,
    ArrowUp,
    ArrowRight,
    LayoutTemplate,
    Eye,
    EyeOff,
    Layers,
    // FileJson, // <--- REMOVIDO PARA CORRIGIR O ERRO
    MousePointer2,
    Grid
} from 'lucide-react'

// Imports do Pipeline Gráfico VTK
import '@kitware/vtk.js/Rendering/Profiles/Geometry'
import vtkRenderWindow from '@kitware/vtk.js/Rendering/Core/RenderWindow'
import vtkRenderer from '@kitware/vtk.js/Rendering/Core/Renderer'
import vtkOpenGLRenderWindow from '@kitware/vtk.js/Rendering/OpenGL/RenderWindow'
import vtkRenderWindowInteractor from '@kitware/vtk.js/Rendering/Core/RenderWindowInteractor'
import vtkInteractorStyleTrackballCamera from '@kitware/vtk.js/Interaction/Style/InteractorStyleTrackballCamera'

// Imports de Dados e Atores
import vtkActor from '@kitware/vtk.js/Rendering/Core/Actor'
import vtkMapper from '@kitware/vtk.js/Rendering/Core/Mapper'
import vtkPolyData from '@kitware/vtk.js/Common/DataModel/PolyData'

interface VtkMeshViewerProps {
    projectPath: string | null
    meshKey: number
    geometries: any[]
}

const VtkMeshViewer: React.FC<VtkMeshViewerProps> = ({ geometries }) => {
    const vtkContainerRef = useRef<HTMLDivElement>(null)

    // Referências vitais do VTK
    const context = useRef<{
        renderWindow: vtkRenderWindow;
        renderer: vtkRenderer;
        openglRenderWindow: vtkOpenGLRenderWindow;
        interactor: vtkRenderWindowInteractor;
    } | null>(null)

    // Mapa para acessar os atores rapidamente sem recriar a cena
    const actorsMap = useRef<Map<string, vtkActor>>(new Map())

    // Estados de UI
    const [isParallel, setIsParallel] = useState(false)
    const [hiddenIds, setHiddenIds] = useState<Set<string>>(new Set())
    const [selectedId, setSelectedId] = useState<string | null>(null)

    // Estado para controlar quais objetos mostram as linhas (Edges/Grid)
    const [edgeVisibleIds, setEdgeVisibleIds] = useState<Set<string>>(new Set())

    // Helper de Cores
    const getMeshColor = (id: string) => {
        const isExtrusion = id.includes('extrusion')
        const isBeam = id.includes('beam')

        if (isExtrusion) {
            if (isBeam) return { r: 0.2, g: 0.8, b: 0.4, hex: '#33cc66', name: '3D Beam' }
            else return { r: 0.2, g: 0.6, b: 1.0, hex: '#3399ff', name: '3D Shell' }
        } else {
            return { r: 1.0, g: 1.0, b: 0.0, hex: '#ffff00', name: 'Mesh Wire' }
        }
    }

    // Ações de Interface
    const toggleVisibility = (id: string, isolate = false) => {
        const newHidden = new Set(hiddenIds)
        if (isolate) {
            geometries.forEach(g => {
                if (g.id !== id) newHidden.add(g.id)
            })
            if (newHidden.has(id)) newHidden.delete(id)
        } else {
            if (newHidden.has(id)) newHidden.delete(id)
            else newHidden.add(id)
        }
        setHiddenIds(newHidden)
    }

    const toggleSelection = (id: string) => {
        setSelectedId(prev => prev === id ? null : id)
    }

    const toggleEdges = (id: string) => {
        const newEdges = new Set(edgeVisibleIds)
        if (newEdges.has(id)) newEdges.delete(id)
        else newEdges.add(id)
        setEdgeVisibleIds(newEdges)
    }

    // ------------------------------------------------------------------------
    // 1. INICIALIZAÇÃO DO AMBIENTE
    // ------------------------------------------------------------------------
    useEffect(() => {
        if (!vtkContainerRef.current) return

        const renderWindow = vtkRenderWindow.newInstance()
        const renderer = vtkRenderer.newInstance()
        renderWindow.addRenderer(renderer)

        const openglRenderWindow = vtkOpenGLRenderWindow.newInstance()
        openglRenderWindow.setContainer(vtkContainerRef.current)
        renderWindow.addView(openglRenderWindow)

        const interactor = vtkRenderWindowInteractor.newInstance()
        interactor.setView(openglRenderWindow)
        interactor.initialize()
        interactor.bindEvents(vtkContainerRef.current)

        const style = vtkInteractorStyleTrackballCamera.newInstance()
        interactor.setInteractorStyle(style)

        renderer.setBackground(0.1, 0.12, 0.15)

        context.current = {
            renderWindow,
            renderer,
            openglRenderWindow,
            interactor
        }

        const resizeObserver = new ResizeObserver((entries) => {
            for (const entry of entries) {
                const { width, height } = entry.contentRect
                openglRenderWindow.setSize(width, height)
                renderWindow.render()
            }
        })
        resizeObserver.observe(vtkContainerRef.current)

        return () => {
            resizeObserver.disconnect()
            if (context.current) {
                context.current.interactor.delete()
                context.current.openglRenderWindow.delete()
                context.current.renderer.delete()
                context.current.renderWindow.delete()
                context.current = null
            }
        }
    }, [])

    // ------------------------------------------------------------------------
    // 2. CONSTRUÇÃO DA CENA
    // ------------------------------------------------------------------------
    useEffect(() => {
        if (!context.current || !geometries) return

        const { renderer, renderWindow } = context.current

        renderer.removeAllActors()
        actorsMap.current.clear()

        // Configuração Padrão de Edges
        const defaultEdges = new Set<string>()
        geometries.forEach(g => {
            if (!g.id.includes('extrusion')) {
                defaultEdges.add(g.id)
            }
        })
        setEdgeVisibleIds(defaultEdges)

        console.log(`[VTK Viewer] Construindo cena para ${geometries.length} objetos...`)

        geometries.forEach((meshItem) => {
            const { data, id } = meshItem
            if (!data || !data.points) return

            const pointsArray = new Float32Array(data.points)
            const cellArray: number[] = []
            if (data.connectivity) {
                data.connectivity.forEach((cell: number[]) => {
                    cellArray.push(cell.length)
                    cellArray.push(...cell)
                })
            }
            const cellsTyped = new Uint32Array(cellArray)

            const polyData = vtkPolyData.newInstance()
            polyData.getPoints().setData(pointsArray, 3)

            if (data.vtk_type === 2 || data.vtk_type === 3) {
                polyData.getLines().setData(cellsTyped)
            } else {
                polyData.getPolys().setData(cellsTyped)
            }

            const mapper = vtkMapper.newInstance()
            mapper.setInputData(polyData)
            const actor = vtkActor.newInstance()
            actor.setMapper(mapper)

            if (!id.includes('extrusion')) {
                mapper.setResolveCoincidentTopologyToPolygonOffset()
                mapper.setResolveCoincidentTopologyPolygonOffsetParameters(-2, -2)
            }

            renderer.addActor(actor)
            actorsMap.current.set(id, actor)
        })

        renderer.resetCamera()
        renderWindow.render()

    }, [geometries])

    // ------------------------------------------------------------------------
    // 3. ATUALIZAÇÃO VISUAL
    // ------------------------------------------------------------------------
    useEffect(() => {
        if (!context.current) return
        const { renderWindow } = context.current

        actorsMap.current.forEach((actor, id) => {
            const prop = actor.getProperty()
            const baseColor = getMeshColor(id)
            const isExtrusion = id.includes('extrusion')

            const isHidden = hiddenIds.has(id)
            const isSelected = selectedId === id
            const showEdges = edgeVisibleIds.has(id)

            actor.setVisibility(!isHidden)
            prop.setEdgeVisibility(showEdges)
            prop.setEdgeColor(0, 0, 0)

            if (isSelected) {
                prop.setColor(1.0, 0.4, 0.0)
                prop.setAmbient(0.6)
                prop.setDiffuse(0.8)
                prop.setOpacity(1.0)
                prop.setLineWidth(showEdges ? 3 : 1)
            } else {
                prop.setColor(baseColor.r, baseColor.g, baseColor.b)
                prop.setAmbient(0.1)
                prop.setDiffuse(0.9)

                if (isExtrusion) {
                    prop.setRepresentationToSurface()
                    prop.setOpacity(0.9)
                    prop.setLineWidth(1)
                } else {
                    prop.setOpacity(1.0)
                    prop.setLineWidth(2)
                }
            }
        })

        renderWindow.render()
    }, [hiddenIds, selectedId, edgeVisibleIds, geometries])

    // ------------------------------------------------------------------------
    // CONTROLADORES
    // ------------------------------------------------------------------------
    const renderScene = () => { if (context.current) context.current.renderWindow.render() }

    const handleResetCamera = useCallback(() => {
        if (!context.current) return
        context.current.renderer.resetCamera()
        renderScene()
    }, [])

    const handleZoom = useCallback((delta: number) => {
        if (!context.current) return
        const camera = context.current.renderer.getActiveCamera()
        if (camera) {
            camera.zoom(delta)
            renderScene()
        }
    }, [])

    const toggleProjection = useCallback(() => {
        if (!context.current) return
        const camera = context.current.renderer.getActiveCamera()
        const newMode = !camera.getParallelProjection()
        camera.setParallelProjection(newMode)
        setIsParallel(newMode)
        context.current.renderer.resetCamera()
        renderScene()
    }, [])

    const setView = useCallback((position: [number, number, number], viewUp: [number, number, number]) => {
        if (!context.current) return
        const camera = context.current.renderer.getActiveCamera()
        camera.setPosition(...position)
        camera.setFocalPoint(0, 0, 0)
        camera.setViewUp(...viewUp)
        context.current.renderer.resetCamera()
        renderScene()
    }, [])

    const handleIso = () => setView([1, -1, 1], [0, 0, 1])
    const handleTop = () => setView([0, 0, 1], [0, 1, 0])
    const handleFront = () => setView([0, -1, 0], [0, 0, 1])
    const handleRight = () => setView([1, 0, 0], [0, 0, 1])


    return (
        <div className="w-full h-full relative overflow-hidden flex flex-col group font-sans">
            <div
                ref={vtkContainerRef}
                className="w-full h-full bg-transparent cursor-crosshair"
                style={{ position: 'absolute', top: 0, left: 0, right: 0, bottom: 0 }}
            />

            {/* OVERLAY ESQUERDA */}
            <div className="absolute top-4 left-4 w-72 flex flex-col gap-2 z-30 max-h-[calc(100%-100px)]">
                <div className="bg-slate-900/95 backdrop-blur-md border border-slate-700 rounded-lg shadow-2xl overflow-hidden flex flex-col">
                    <div className="px-3 py-2 bg-slate-800 border-b border-slate-700 flex items-center justify-between">
                        <div className="flex items-center gap-2 text-slate-200">
                            <Layers size={16} />
                            <span className="text-xs font-bold uppercase tracking-wider">Scene Objects</span>
                        </div>
                        <span className="text-xs bg-slate-700 text-slate-300 px-1.5 py-0.5 rounded-full">
                            {geometries?.length || 0}
                        </span>
                    </div>

                    <div className="p-2 flex flex-col gap-1 overflow-y-auto custom-scrollbar">
                        {geometries && geometries.length > 0 ? (
                            geometries.map((mesh) => {
                                const color = getMeshColor(mesh.id)
                                const isHidden = hiddenIds.has(mesh.id)
                                const isSelected = selectedId === mesh.id
                                const isGridVisible = edgeVisibleIds.has(mesh.id)

                                return (
                                    <div
                                        key={mesh.id}
                                        onClick={() => toggleSelection(mesh.id)}
                                        className={`
                                            group/item flex items-center gap-2 p-2 rounded cursor-pointer transition-all border
                                            ${isSelected
                                                ? 'bg-slate-800 border-orange-500/50 shadow-[0_0_10px_rgba(249,115,22,0.1)]'
                                                : 'bg-transparent border-transparent hover:bg-slate-800 hover:border-slate-700'
                                            }
                                            ${isHidden ? 'opacity-50' : 'opacity-100'}
                                        `}
                                    >
                                        <div className="flex items-center gap-1 shrink-0">
                                            {/* Botão de Visibilidade */}
                                            <button
                                                onClick={(e) => {
                                                    e.stopPropagation()
                                                    toggleVisibility(mesh.id, e.altKey)
                                                }}
                                                className="p-1 text-slate-400 hover:text-white hover:bg-slate-700 rounded transition-colors"
                                                title="Toggle Visibility (Alt+Click to Isolate)"
                                            >
                                                {isHidden ? <EyeOff size={14} /> : <Eye size={14} />}
                                            </button>

                                            {/* Botão de Grid */}
                                            <button
                                                onClick={(e) => {
                                                    e.stopPropagation()
                                                    toggleEdges(mesh.id)
                                                }}
                                                className={`
                                                    p-1 rounded transition-colors
                                                    ${isGridVisible
                                                        ? 'text-cyan-400 hover:text-cyan-300 hover:bg-slate-700'
                                                        : 'text-slate-600 hover:text-slate-400 hover:bg-slate-700'
                                                    }
                                                `}
                                                title="Toggle Mesh Edges (Wireframe)"
                                            >
                                                <Grid size={14} />
                                            </button>
                                        </div>

                                        <div
                                            className="w-2.5 h-2.5 rounded-full shadow-sm ring-1 ring-white/10 shrink-0 mx-1"
                                            style={{
                                                backgroundColor: isSelected ? '#F97316' : color.hex,
                                                boxShadow: isSelected ? '0 0 8px #F97316' : 'none'
                                            }}
                                        />

                                        <div className="flex flex-col min-w-0 flex-1">
                                            <span className={`text-xs truncate ${isSelected ? 'text-orange-400 font-bold' : 'text-slate-200 font-medium'}`}>
                                                {mesh.id.replace('_extrusion.json', '').replace('.json', '')}
                                            </span>
                                        </div>

                                        {isSelected && <MousePointer2 size={12} className="text-orange-500" />}
                                    </div>
                                )
                            })
                        ) : (
                            <div className="p-4 text-center text-slate-500 text-xs italic">
                                Loading geometry...
                            </div>
                        )}
                    </div>
                </div>
            </div>

            {/* TOOLBAR DIREITA */}
            <div className="absolute top-4 right-4 flex flex-col gap-2 z-30 pointer-events-none">
                <div className="bg-slate-800/90 backdrop-blur-sm border border-slate-700 rounded-lg p-1.5 shadow-xl flex flex-col gap-1 pointer-events-auto">
                    <ToolbarButton onClick={handleIso} icon={<Box size={18} />} title="Isometric View" />
                    <ToolbarButton onClick={handleTop} icon={<LayoutTemplate size={18} />} title="Top View" />
                    <ToolbarButton onClick={handleFront} icon={<ArrowUp size={18} />} title="Front View" />
                    <ToolbarButton onClick={handleRight} icon={<ArrowRight size={18} />} title="Right View" />
                </div>
                <div className="bg-slate-800/90 backdrop-blur-sm border border-slate-700 rounded-lg p-1.5 shadow-xl flex flex-col gap-1 pointer-events-auto">
                    <ToolbarButton onClick={handleResetCamera} icon={<Maximize size={18} />} title="Fit All" />
                    <div className="h-px bg-slate-700 my-0.5"></div>
                    <ToolbarButton onClick={() => handleZoom(1.2)} icon={<ZoomIn size={18} />} title="Zoom In" />
                    <ToolbarButton onClick={() => handleZoom(0.8)} icon={<ZoomOut size={18} />} title="Zoom Out" />
                </div>
                <div className="bg-slate-800/90 backdrop-blur-sm border border-slate-700 rounded-lg p-1.5 shadow-xl flex flex-col gap-1 pointer-events-auto">
                    <ToolbarButton
                        onClick={toggleProjection}
                        icon={<Eye size={18} />}
                        active={isParallel}
                        title={isParallel ? "Perspective" : "Orthographic"}
                    />
                </div>
            </div>

            {/* RODAPÉ */}
            <div className="absolute bottom-2 left-2 right-2 flex justify-between items-end pointer-events-none opacity-50 group-hover:opacity-100 transition-opacity">
                <div className="text-[10px] text-slate-400 bg-black/40 px-2 py-1 rounded backdrop-blur-sm">
                    <span className="font-bold text-slate-200">LMB:</span> Rotate &nbsp;|&nbsp;
                    <span className="font-bold text-slate-200">Shift+LMB:</span> Pan &nbsp;|&nbsp;
                    <span className="font-bold text-slate-200">RMB:</span> Zoom
                </div>
                <div className="text-[10px] font-mono text-slate-500">
                    VTK.js Viewport
                </div>
            </div>
        </div>
    )
}

const ToolbarButton = ({ onClick, icon, title, active = false }: { onClick: () => void, icon: React.ReactNode, title: string, active?: boolean }) => (
    <button
        onClick={onClick}
        title={title}
        className={`
            p-1.5 rounded-md transition-all active:scale-95
            flex items-center justify-center
            ${active ? 'bg-blue-600 text-white shadow-sm' : 'text-slate-400 hover:bg-slate-700 hover:text-white'}
        `}
    >
        {icon}
    </button>
)

export default VtkMeshViewer


################################################################################
# PASTA: frontend/src/stories
################################################################################

--- ARQUIVO: frontend/src/stories/Button.tsx ---


import './button.css';

export interface ButtonProps {
  /** Is this the principal call to action on the page? */
  primary?: boolean;
  /** What background color to use */
  backgroundColor?: string;
  /** How large should the button be? */
  size?: 'small' | 'medium' | 'large';
  /** Button contents */
  label: string;
  /** Optional click handler */
  onClick?: () => void;
}

/** Primary UI component for user interaction */
export const Button = ({
  primary = false,
  size = 'medium',
  backgroundColor,
  label,
  ...props
}: ButtonProps) => {
  const mode = primary ? 'storybook-button--primary' : 'storybook-button--secondary';
  return (
    <button
      type="button"
      className={['storybook-button', `storybook-button--${size}`, mode].join(' ')}
      style={{ backgroundColor }}
      {...props}
    >
      {label}
    </button>
  );
};

--- ARQUIVO: frontend/src/stories/Header.tsx ---


import { Button } from './Button';
import './header.css';

type User = {
  name: string;
};

export interface HeaderProps {
  user?: User;
  onLogin?: () => void;
  onLogout?: () => void;
  onCreateAccount?: () => void;
}

export const Header = ({ user, onLogin, onLogout, onCreateAccount }: HeaderProps) => (
  <header>
    <div className="storybook-header">
      <div>
        <svg width="32" height="32" viewBox="0 0 32 32" xmlns="http://www.w3.org/2000/svg">
          <g fill="none" fillRule="evenodd">
            <path
              d="M10 0h12a10 10 0 0110 10v12a10 10 0 01-10 10H10A10 10 0 010 22V10A10 10 0 0110 0z"
              fill="#FFF"
            />
            <path
              d="M5.3 10.6l10.4 6v11.1l-10.4-6v-11zm11.4-6.2l9.7 5.5-9.7 5.6V4.4z"
              fill="#555AB9"
            />
            <path
              d="M27.2 10.6v11.2l-10.5 6V16.5l10.5-6zM15.7 4.4v11L6 10l9.7-5.5z"
              fill="#91BAF8"
            />
          </g>
        </svg>
        <h1>Acme</h1>
      </div>
      <div>
        {user ? (
          <>
            <span className="welcome">
              Welcome, <b>{user.name}</b>!
            </span>
            <Button size="small" onClick={onLogout} label="Log out" />
          </>
        ) : (
          <>
            <Button size="small" onClick={onLogin} label="Log in" />
            <Button primary size="small" onClick={onCreateAccount} label="Sign up" />
          </>
        )}
      </div>
    </div>
  </header>
);

--- ARQUIVO: frontend/src/stories/ModelConfig.stories.tsx ---
import type { Meta, StoryObj } from '@storybook/react';
import ModelConfig from '../components/config/ModelConfig';

// Mock data for mesh groups
const mockMeshGroups = {
    'mesh_v1.med': {
        'Group_1': {
            count: 100,
            types: { 'SEG3': 100 },
            category: '1D'
        },
        'Group_2': {
            count: 50,
            types: { 'TRIA3': 50 },
            category: '2D'
        },
        'Group_3': {
            count: 200,
            types: { 'HEXA8': 200 },
            category: '3D'
        },
        'Node_Group': {
            count: 10,
            types: { 'Node': 10 },
            category: 'Node'
        }
    },
    'mesh_v2.med': {
        'Beam_Main': {
            count: 120,
            types: { 'SEG2': 120 },
            category: '1D'
        }
    }
};

const mockCurrentGeometries = [
    {
        group: 'Group_1',
        _meshFile: 'mesh_v1.med',
        type: 'POU_D_E',
        phenomenon: 'MECANIQUE',
        _category: '1D'
    }
];

const meta = {
    title: 'Config/ModelConfig',
    component: ModelConfig,
    parameters: {
        layout: 'padded', // or 'fullscreen'
        backgrounds: {
            default: 'dark',
        },
    },
    tags: ['autodocs'],
    argTypes: {
        onUpdate: { action: 'updated' },
    },
} satisfies Meta<typeof ModelConfig>;

export default meta;
type Story = StoryObj<typeof meta>;

// 1. Default state: No project path (should show "Please select a project")
export const Default: Story = {
    args: {
        projectPath: null,
        meshGroups: {},
        currentGeometries: [],
    },
};

// 2. Empty State: Project selected but no mesh groups (should show "No Mesh Groups Detected")
export const EmptyGroups: Story = {
    args: {
        projectPath: '/path/to/project',
        meshGroups: {},
        currentGeometries: [],
    },
};

// 3. Populated: Standard use case with mesh groups detected
export const WithData: Story = {
    args: {
        projectPath: '/path/to/project',
        meshGroups: mockMeshGroups,
        currentGeometries: [],
    },
};

// 4. Pre-configured: Some groups already have configurations saved
export const WithSavedConfig: Story = {
    args: {
        projectPath: '/path/to/project',
        meshGroups: mockMeshGroups,
        currentGeometries: mockCurrentGeometries,
    },
};

--- ARQUIVO: frontend/src/stories/Page.tsx ---
import { useState } from 'react';

import { Header } from './Header';
import './page.css';

type User = {
  name: string;
};

export const Page: React.FC = () => {
  const [user, setUser] = useState<User>();

  return (
    <article>
      <Header
        user={user}
        onLogin={() => setUser({ name: 'Jane Doe' })}
        onLogout={() => setUser(undefined)}
        onCreateAccount={() => setUser({ name: 'Jane Doe' })}
      />

      <section className="storybook-page">
        <h2>Pages in Storybook</h2>
        <p>
          We recommend building UIs with a{' '}
          <a href="https://componentdriven.org" target="_blank" rel="noopener noreferrer">
            <strong>component-driven</strong>
          </a>{' '}
          process starting with atomic components and ending with pages.
        </p>
        <p>
          Render pages with mock data. This makes it easy to build and review page states without
          needing to navigate to them in your app. Here are some handy patterns for managing page
          data in Storybook:
        </p>
        <ul>
          <li>
            Use a higher-level connected component. Storybook helps you compose such data from the
            "args" of child component stories
          </li>
          <li>
            Assemble data in the page component from your services. You can mock these services out
            using Storybook.
          </li>
        </ul>
        <p>
          Get a guided tutorial on component-driven development at{' '}
          <a href="https://storybook.js.org/tutorials/" target="_blank" rel="noopener noreferrer">
            Storybook tutorials
          </a>
          . Read more in the{' '}
          <a href="https://storybook.js.org/docs" target="_blank" rel="noopener noreferrer">
            docs
          </a>
          .
        </p>
        <div className="tip-wrapper">
          <span className="tip">Tip</span> Adjust the width of the canvas with the{' '}
          <svg width="10" height="10" viewBox="0 0 12 12" xmlns="http://www.w3.org/2000/svg">
            <g fill="none" fillRule="evenodd">
              <path
                d="M1.5 5.2h4.8c.3 0 .5.2.5.4v5.1c-.1.2-.3.3-.4.3H1.4a.5.5 0 01-.5-.4V5.7c0-.3.2-.5.5-.5zm0-2.1h6.9c.3 0 .5.2.5.4v7a.5.5 0 01-1 0V4H1.5a.5.5 0 010-1zm0-2.1h9c.3 0 .5.2.5.4v9.1a.5.5 0 01-1 0V2H1.5a.5.5 0 010-1zm4.3 5.2H2V10h3.8V6.2z"
                id="a"
                fill="#999"
              />
            </g>
          </svg>
          Viewports addon in the toolbar
        </div>
      </section>
    </article>
  );
};


################################################################################
# PASTA: prosolve
################################################################################

--- ARQUIVO: prosolve/_sectionproperties.py ---


--- ARQUIVO: prosolve/run_aster.py ---
import sys
import os
import subprocess

# --- CONFIGURAÇÃO ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
CONFIG_FILE = os.path.join(BASE_DIR, "config.txt")

def carregar_config():
    """Lê o arquivo config.txt e retorna um dicionário"""
    configs = {}
    if not os.path.exists(CONFIG_FILE):
        print(f"[ERRO] Arquivo de configuração não encontrado: {CONFIG_FILE}")
        return configs
    
    try:
        with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                # Ignora linhas vazias ou comentários
                if not line or line.startswith('#'):
                    continue
                if '=' in line:
                    key, value = line.split('=', 1)
                    configs[key.strip()] = value.strip()
    except Exception as e:
        print(f"[ERRO] Falha ao ler config.txt: {e}")
    
    return configs

def executar_simulacao():
    print("-" * 60)
    print("   WRAPPER DE EXECUÇÃO CODE_ASTER (PYTHON)")
    print("-" * 60)

    # 1. Carregar Configurações
    config = carregar_config()
    aster_bin = config.get("ASTER_BIN")

    if not aster_bin:
        print("[ERRO] 'ASTER_BIN' não definido no config.txt")
        return
    
    if not os.path.exists(aster_bin):
        print(f"[ERRO] Executável do Aster não encontrado no caminho configurado:\n{aster_bin}")
        return

    # 2. Validar Argumentos
    if len(sys.argv) < 2:
        print("[ERRO] Caminho do arquivo export.export não foi fornecido.")
        return

    export_path = sys.argv[1]

    if not os.path.exists(export_path):
        print(f"[ERRO] O arquivo export não existe: {export_path}")
        return

    # 3. Definir Diretório de Trabalho
    working_dir = os.path.dirname(export_path)

    print(f"[INFO] Aster Bin: {aster_bin}")
    print(f"[INFO] Export:    {export_path}")
    print(f"[INFO] Executando Code_Aster...")

    try:
        # 4. Executar o Comando - Use shell=True for Code_Aster on Windows
        # We don't capture output anymore so it goes directly to the console window
        print(f"[INFO] Launching Aster...")
        processo = subprocess.run(
            [aster_bin, export_path],
            cwd=working_dir,
            shell=True
        )

        if processo.returncode == 0:
            print("\n[SUCESSO] O Code_Aster finalizou a execução.")
        else:
            print(f"\n[FALHA] O Code_Aster retornou código de erro: {processo.returncode}")
        
        # Keep window open for inspection
        print("\n" + "="*40)
        print("SIMULATION FINISHED. Window kept open for inspection.")
        print("="*40)
        input("Press Enter to close this window...")

    except Exception as e:
        print(f"\n[CRITICO] Erro inesperado: {e}")
        input("Press Enter to close this window...")

if __name__ == "__main__":
    executar_simulacao()


################################################################################
# PASTA: prosolve/jinja
################################################################################

--- ARQUIVO: prosolve/jinja/generate_comm.py ---
# =========================================================
# generate_comm.py
# Gera arquivo .comm do Code_Aster a partir de JSON + Jinja
# =========================================================

import json
import sys
from pathlib import Path
from jinja2 import Environment, FileSystemLoader

# ---------------------------------------------------------
# 1. Configuração de Diretórios
# ---------------------------------------------------------
BASE_DIR = Path(__file__).resolve().parent

BUILDERS_DIR = BASE_DIR / "builders"
TEMPLATES_DIR = BASE_DIR / "templates"
STUDY_DIR = BASE_DIR / "study"
OUTPUT_DIR = BASE_DIR / "output"

# ---------------------------------------------------------
# 2. Importação de Builders
# ---------------------------------------------------------
sys.path.insert(0, str(BUILDERS_DIR))

try:
    from asse_maillage import build_asse_maillage
    from affe_modele import build_affe_modele
    from defi_materiau import build_defi_materiau
    from affe_materiau import build_affe_materiau
    from affe_cara_elem_shell import build_affe_cara_elem_shell
    # IMPORTAÇÃO ESPECÍFICA PARA DDL
    from affe_char_meca_ddl import build_affe_char_meca_ddl 
    # IMPORTAÇÃO MECA_STATIQUE
    from meca_statique import build_meca_statique
    # IMPORTAÇÃO PESANTEUR
    from pesanteur import build_pesanteur
    # IMPORTAÇÃO LOAD CASES
    from load_cases import build_load_cases
    # IMPORTAÇÃO FORCE_COQUE
    from force_coque import build_force_coque
    # IMPORTAÇÃO POST_ELEM_MASS
    from post_elem_mass import build_post_elem_mass
    # IMPORTAÇÃO POST_RELEVE_T_REACTIONS
    from post_releve_t_reactions import build_post_releve_t_reactions
    from geometry import build_geometry
except ImportError as e:
    raise ImportError(f"Erro ao importar builders: {e}")

# ---------------------------------------------------------
# 3. Leitura dos Arquivos de Configuração (JSON)
# ---------------------------------------------------------

# A. Mesh
mesh_file = STUDY_DIR / "mesh.json"
if not mesh_file.exists(): raise FileNotFoundError("mesh.json")
with open(mesh_file, encoding="utf-8") as f: mesh_config = json.load(f)
mesh_names = [m["name"] for m in mesh_config["meshes"]]

# B. Models
# C. Materials Props
materials_file = STUDY_DIR / "materials.json"
mat_props_list = []
if materials_file.exists():
    with open(materials_file, encoding="utf-8") as f: mat_props_list = json.load(f).get("materials", [])

# D. Material Assignments
assign_file = STUDY_DIR / "material_assignments.json"
assign_list = []
if assign_file.exists():
    with open(assign_file, encoding="utf-8") as f: assign_list = json.load(f).get("assignments", [])

# E. Geometry Properties (Unified Shell, Beam, Volume)
geom_file = STUDY_DIR / "geometry.json"
geometry_list = []
if geom_file.exists():
    with open(geom_file, encoding="utf-8") as f: 
        geometry_list = json.load(f).get("geometries", [])
else:
    # Fallback to old models.json if geometry.json doesn't exist (compatibility)
    models_file = STUDY_DIR / "models.json"
    if models_file.exists():
        with open(models_file, encoding="utf-8") as f:
            old_models = json.load(f).get("models", [])
            for m in old_models:
                geometry_list.append({"group": m["group"], "type": "Shell" if m["type"]=="DKT" else "Solid"})
    
    # Fallback to old shell_properties.json for thickness
    shells_file = STUDY_DIR / "shell_properties.json"
    if shells_file.exists():
        with open(shells_file, encoding="utf-8") as f:
            old_shells = json.load(f).get("shells", [])
            for s in old_shells:
                for target in geometry_list:
                    if target["group"] == s["group"]:
                        target["thickness"] = s["thickness"]

# F. DDL IMPO (JSON Específico)
ddl_file = STUDY_DIR / "ddl_impo.json"
ddl_list = []
if ddl_file.exists():
    with open(ddl_file, encoding="utf-8") as f: 
        ddl_list = json.load(f).get("ddl_impo", [])

# G. MECA_STATIQUE (JSON)
meca_file = STUDY_DIR / "meca_statique.json"
meca_config = {}
if meca_file.exists():
    with open(meca_file, encoding="utf-8") as f:
        meca_config = json.load(f)

# H. PESANTEUR (JSON)
pes_file = STUDY_DIR / "pesanteur.json"
pes_config = {}
if pes_file.exists():
    with open(pes_file, encoding="utf-8") as f:
        pes_config = json.load(f)

# I. LOAD CASES (JSON)
lc_file = STUDY_DIR / "load_cases.json"
lc_list = []
if lc_file.exists():
    with open(lc_file, encoding="utf-8") as f:
        lc_list = json.load(f).get("load_cases", [])

# J. FORCE_COQUE (JSON)
foc_file = STUDY_DIR / "force_coque.json"
foc_config = {}
if foc_file.exists():
    with open(foc_file, encoding="utf-8") as f:
        foc_config = json.load(f)

# K. POST_ELEM_MASS (JSON)
mass_file = STUDY_DIR / "post_elem_mass.json"
mass_config = {}
if mass_file.exists():
    with open(mass_file, encoding="utf-8") as f:
        mass_config = json.load(f)

# L. POST_RELEVE_T_REACTIONS (JSON)
reac_file = STUDY_DIR / "post_releve_t_reactions.json"
reac_config = {}
if reac_file.exists():
    with open(reac_file, encoding="utf-8") as f:
        reac_config = json.load(f)

# ---------------------------------------------------------
# 4. Preparação dos Dados (Builders)
# ---------------------------------------------------------

FINAL_MESH = "MAIL"
FINAL_MODEL = "MODELE"
FINAL_CHMAT = "CHAM_MATER"
FINAL_CARA = "CARA_ELEM"
FINAL_DDL = "CHARGE_DDL" # Nome específico da carga de DDL
FINAL_RESU = "RESU_MECA"

# A. Assembly
asse_data = build_asse_maillage(mesh_names, result_name=FINAL_MESH)

# B & E. Geometry (Model + Properties)
geom_data = build_geometry(geometry_list, model_name=FINAL_MODEL, result_name=FINAL_CARA)
model_data = { "result_name": FINAL_MODEL, "mesh_name": FINAL_MESH, "items": geom_data["model_items"] }
# Add default phenomene
for item in model_data["items"]: item["phenomene"] = "MECANIQUE"

# C. Material Definição
defi_mat_data = build_defi_materiau(mat_props_list)

# D. Material Atribuição
affe_mat_data = build_affe_materiau(assign_list, model_name=FINAL_MODEL, result_name=FINAL_CHMAT)

# F. DDL IMPO (Builder Específico)
ddl_data = build_affe_char_meca_ddl(
    ddl_list, 
    model_name=FINAL_MODEL, 
    result_name=FINAL_DDL
)

# G. PESANTEUR (Builder) - Agora antes de MECA_STATIQUE
# Define nome para a carga de gravidade
FINAL_PES = "CHARGE_PES"
pes_data = build_pesanteur(pes_config, model_name=FINAL_MODEL, result_name=FINAL_PES)

# I. FORCE_COQUE (Builder)
# Retorna dados de calculo (Python) e dados de carga (Aster)
foc_calc_data, foc_load_data = build_force_coque(foc_config, model_name=FINAL_MODEL)
FINAL_FOC = foc_load_data.get("result_name", "CHARGE_FOC")

# L. POST_RELEVE_T_REACTIONS (Builder)
reac_data = build_post_releve_t_reactions(reac_config, ddl_list)

# H. LOAD CASES (Builder)
# Passamos config de meca, dados de pesanteur e ddl para validação
lc_data = build_load_cases(
    lc_list, 
    meca_config, 
    pes_data=pes_data, 
    ddl_data=ddl_data, 
    model_name=FINAL_MODEL,
    reaction_extraction_data=reac_data,
    foc_data=foc_load_data
)

# K. POST_ELEM_MASS (Builder)
mass_data = build_post_elem_mass(
    mass_config, 
    model_name=FINAL_MODEL, 
    field_mat_name=FINAL_CHMAT, 
    cara_elem_name=FINAL_CARA
)

# ---------------------------------------------------------
# 5. Configuração Jinja
# ---------------------------------------------------------
env = Environment(loader=FileSystemLoader(str(TEMPLATES_DIR)), trim_blocks=True, lstrip_blocks=True)

try:
    tpl_lire = env.get_template("lire_maillage.j2")
    tpl_asse = env.get_template("asse_maillage.j2")
    tpl_inspect = env.get_template("inspect_mesh.j2") 
    tpl_model = env.get_template("affe_modele.j2")
    tpl_defi = env.get_template("defi_materiau.j2")
    tpl_affe = env.get_template("affe_materiau.j2")
    tpl_cara = env.get_template("affe_cara_elem.j2")
    # TEMPLATE ESPECÍFICO
    tpl_ddl = env.get_template("affe_char_meca_ddl.j2") 
    tpl_meca = env.get_template("meca_statique.j2") # Ainda usado se precisar fallback ou single, mas load_cases usa o dele
    tpl_pes = env.get_template("pesanteur.j2")
    tpl_lc = env.get_template("load_cases.j2")
    tpl_foc_calc = env.get_template("force_coque_calc.j2")
    tpl_foc_load = env.get_template("force_coque_load.j2")
    tpl_mass = env.get_template("post_elem_mass.j2")
except Exception as e:
    raise RuntimeError(f"Erro template: {e}")

# ---------------------------------------------------------
# 6. Escrita do Arquivo .comm
# ---------------------------------------------------------
OUTPUT_DIR.mkdir(exist_ok=True)
comm_path = OUTPUT_DIR / "calcul.comm"

print(f"Gerando script...")

with open(comm_path, "w", encoding="utf-8") as f:
    f.write("DEBUT(LANG='FR')\n\n")

    # 1. LEITURA
    f.write("# --- 1. Leitura ---\n")
    for i, mesh in enumerate(mesh_config["meshes"]):
        f.write(tpl_lire.render(
            mesh_name=mesh["name"], 
            unit=mesh.get("unit", 20+i), 
            filename=mesh.get("filename", f"{mesh['name']}.med")
        ))
        f.write("\n")

    # 2. ASSEMBLY
    f.write("# --- 2. Assembly ---\n")
    if asse_data["mode"] == "ASSE":
        f.write(tpl_asse.render(**asse_data))
    elif asse_data["mode"] == "SINGLE":
        if asse_data['final_mesh'] != FINAL_MESH:
            f.write(f"{FINAL_MESH} = {asse_data['final_mesh']}\n")
    f.write("\n")


    # --- NOVO BLOCO: INSPEÇÃO E GERAÇÃO DE JSON ---
    f.write("# --- 2.1 Inspeção da Malha e Geração de JSON ---\n")
    # Passamos 'MAIL' (ou o valor de FINAL_MESH) para o template saber qual objeto inspecionar
    f.write(tpl_inspect.render(mesh_variable=FINAL_MESH))
    f.write("\n")
    # -----------------------------------------------

    # 3. MODELO
    f.write("# --- 3. Modelo ---\n")
    if model_data["items"]:
        f.write(tpl_model.render(**model_data))
        f.write("\n")

    # 4. MATERIAL (DEFI)
    f.write("# --- 4. Definição de Materiais ---\n")
    if defi_mat_data:
        f.write(tpl_defi.render(definitions=defi_mat_data))
        f.write("\n")

    # 5. MATERIAL (AFFE)
    f.write("# --- 5. Atribuição de Materiais ---\n")
    if affe_mat_data["items"]:
        f.write(tpl_affe.render(**affe_mat_data))
        f.write("\n")

    # 6. GEOMETRY PROPERTIES (Shells, Beams)
    f.write("# --- 6. Propriedades Geométricas (Cascas, Vigas) ---\n")
    if geom_data["cara_items"]:
        f.write(tpl_cara.render(**geom_data))
        f.write("\n")

    # 6.b BLOCO DE CÁLCULO DE ÁREA (FORCE_COQUE PRE-PROCESSAMENTO)
    if foc_calc_data:
        f.write("# --- 6.b Cálculo de Pression Equivalente (Force/Area) ---\n")
        f.write(tpl_foc_calc.render(**foc_calc_data))
        f.write("\n")

    # 7. DDL IMPO
    f.write("# --- 7. Condições de Contorno (DDL_IMPO) ---\n")
    if ddl_data:
        f.write(tpl_ddl.render(commands=ddl_data))
        f.write("\n")
    else:
        f.write("# AVISO: Nenhuma condição DDL_IMPO definida.\n")

    # 9. PESANTEUR (Agora antes de MECA)
    f.write("# --- 9. Carga de Pesanteur ---\n")
    if pes_data:
        f.write(tpl_pes.render(commands=pes_data))
        f.write("\n")

    # 10. FORCE_COQUE
    f.write("# --- 10. Carga de Force Coque ---\n")
    if foc_load_data and foc_load_data.get("load_items"):
        f.write(tpl_foc_load.render(**foc_load_data))
        f.write("\n")

    # 9. MECA_STATIQUE (Iteração por Load Case) via Template Load Cases
    if lc_data.get("runs"):
        f.write(tpl_lc.render(**lc_data))
        f.write("\n")

    # 11. POST-PROCESSING (MASS)
    if mass_data:
        f.write("# --- 11. Post-Processing (Mass) ---\n")
        f.write(tpl_mass.render(commands=mass_data))
        f.write("\n")

    f.write("FIN()\n")

print(f"Arquivo gerado: {comm_path}")

--- ARQUIVO: prosolve/jinja/inspect_mesh.py ---
import sys
import json
import os
from pathlib import Path
from jinja2 import Environment, FileSystemLoader

# --- CONFIGURAÇÃO DE CAMINHOS ---
# Localização deste script: root/jinja/inspect_mesh.py
BASE_DIR = Path(__file__).resolve().parent
BUILDERS_DIR = BASE_DIR / "builders"
TEMPLATES_DIR = BASE_DIR / "templates"

# Validação de diretórios essenciais
if not BUILDERS_DIR.exists():
    print(f"[ERROR] Diretorio de builders nao encontrado: {BUILDERS_DIR}")
    sys.exit(1)

if not TEMPLATES_DIR.exists():
    print(f"[ERROR] Diretorio de templates nao encontrado: {TEMPLATES_DIR}")
    sys.exit(1)

# Adiciona builders ao path do sistema para importação
sys.path.insert(0, str(BUILDERS_DIR))

try:
    from asse_maillage import build_asse_maillage
    # lire_maillage é opcional importar se fizermos o loop manual, mas asse é vital
except ImportError as e:
    print(f"[ERROR] Falha ao importar builders do Code_Aster: {e}")
    sys.exit(1)

def generate_inspection_comm(project_folder):
    """
    Orquestrador que lê o mesh.json e gera o arquivo de comando med.comm
    dentro da pasta simulation_files.
    """
    project_path = Path(project_folder)
    sim_files_path = project_path / "simulation_files"
    
    # ARQUIVO DE ENTRADA (Gerado pelo Python Main)
    mesh_json_path = sim_files_path / "mesh.json"
    
    # ARQUIVO DE SAÍDA (Onde o .comm será salvo)
    output_comm = sim_files_path / "med.comm"

    print(f"[INSPECT] Gerando .comm para: {project_path}")
    print(f"[INSPECT] Arquivo de destino: {output_comm}")

    # 1. Ler o JSON de Input (mesh.json)
    if not mesh_json_path.exists():
        print(f"[ERROR] mesh.json nao encontrado em: {mesh_json_path}")
        return False

    try:
        with open(mesh_json_path, 'r', encoding='utf-8') as f:
            mesh_config = json.load(f)
    except Exception as e:
        print(f"[ERROR] Falha ao ler ou decodificar mesh.json: {e}")
        return False

    # 2. Configurar Jinja2
    env = Environment(loader=FileSystemLoader(str(TEMPLATES_DIR)), trim_blocks=True, lstrip_blocks=True)
    
    try:
        tpl_lire = env.get_template("lire_maillage.j2")
        tpl_asse = env.get_template("asse_maillage.j2")
        tpl_inspect = env.get_template("inspect_mesh.j2")
    except Exception as e:
        print(f"[ERROR] Erro ao carregar templates Jinja: {e}")
        return False

    # 3. Processar Dados
    unit_start = mesh_config.get("unit_start", 80)
    meshes_list = mesh_config.get("meshes", [])
    
    if not meshes_list:
        print("[ERROR] Nenhuma malha definida dentro de mesh.json")
        return False
    
    try:
        # Inicio do arquivo .comm
        comm_content = "DEBUT(LANG='FR')\n\n"
        comm_content += "# --- 1. Leitura das Malhas ---\n"

        mesh_names = []
        
        # Loop para gerar comandos LIRE_MAILLAGE
        for i, mesh in enumerate(meshes_list):
            unit = unit_start + i
            mesh_name = mesh["name"]
            mesh_names.append(mesh_name)
            
            # O nome do arquivo no .comm é menos relevante se usarmos UNITE no export,
            # mas mantemos por consistência.
            filename = mesh.get("filename", f"{mesh_name}.med")
            
            # Renderiza template LIRE
            comm_content += tpl_lire.render(
                mesh_name=mesh_name,
                unit=unit,
                filename=filename
            ) + "\n"

        # Builder ASSE_MAILLAGE (Assembly)
        comm_content += "\n# --- 2. Assembly ---\n"
        asse_data = build_asse_maillage(mesh_names, result_name="MAIL")
        
        final_mesh_variable = "MAIL"
        
        # Lógica de renderização do Assembly
        if asse_data["mode"] == "ASSE":
            # Se houver múltiplas malhas, usa o comando ASSE_MAILLAGE
            comm_content += tpl_asse.render(**asse_data)
            final_mesh_variable = "MAIL"
            
        elif asse_data["mode"] == "SINGLE":
            # Se for única, o nome final é o nome da malha lida
            original_name = asse_data['final_mesh']
            
            # Cria alias MAIL = Mesh_Nome para padronizar o script de inspeção
            if original_name != "MAIL":
                 comm_content += f"MAIL = {original_name}\n"
                 final_mesh_variable = "MAIL"
        
        comm_content += "\n"

        # 4. Injeção do Script de Inspeção (Gera mesh_groups.json)
        comm_content += "# --- 3. Inspecao e Geracao de JSON ---\n"
        
        # --- PREPARAÇÃO DO CAMINHO ABSOLUTO ---
        # Converte para string e substitui backslash por forward slash para evitar 
        # problemas de escape string no Python gerado (ex: \t, \n, \r)
        abs_output_folder = str(sim_files_path.absolute()).replace("\\", "/")
        
        # Passamos a variável output_folder para o template
        comm_content += tpl_inspect.render(
            mesh_variable=final_mesh_variable,
            output_folder=abs_output_folder
        )
        comm_content += "\n"

        comm_content += "FIN()\n"

        # 5. Salvar Arquivo med.comm final na pasta simulation_files
        # --- Encoding="latin-1" para compatibilidade com Code_Aster Windows ---
        with open(output_comm, "w", encoding="latin-1") as f:
            f.write(comm_content)
            
        print(f"[INSPECT] med.comm gerado com sucesso em: {output_comm}")
        return True

    except Exception as e:
        print(f"[ERROR] Falha na construcao do conteudo .comm: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Uso: python inspect_mesh.py <caminho_do_projeto>")
        sys.exit(1)
    
    folder_arg = sys.argv[1]
    success_status = generate_inspection_comm(folder_arg)
    
    # Retorna código de saída para o subprocesso pai (main.pyw) capturar
    sys.exit(0 if success_status else 1)

--- ARQUIVO: prosolve/jinja/teste.py ---
from medcoupling import MEDFileMesh

print("MEDCoupling OK")


################################################################################
# PASTA: prosolve/jinja/builders
################################################################################

--- ARQUIVO: prosolve/jinja/builders/affe_cara_elem_shell.py ---
def build_affe_cara_elem_shell(shell_list, model_name="MODELE", result_name="CARA_ELEM"):
    """
    Constrói a estrutura para AFFE_CARA_ELEM focada apenas em COQUE (Cascas).
    """
    items = []

    for item in shell_list:
        group = item["group"]
        thickness = item["thickness"]
        excentricity = item.get("excentricity", 0.0)
        # Vetor padrão para cascas
        vector = tuple(item.get("vector", [1.0, 0.0, 0.0]))

        items.append({
            "group": group,
            "epais": thickness,
            "vecteur": vector,
            "excentrement": excentricity,
            
            # Constantes fixas para este tipo de análise
            "a_cis": 0.8333333,
            "coef_rigi_drz": 1e-05,
            "coque_ncou": 1,
            "iner_rota": "OUI",
            "modi_metrique": "NON"
        })

    return {
        "result_name": result_name,
        "model_name": model_name,
        "items": items
    }

--- ARQUIVO: prosolve/jinja/builders/affe_char_meca_ddl.py ---
def build_affe_char_meca_ddl(ddl_list, model_name="MODELE", result_name="CHARGE_DDL"):
    """
    Constrói a estrutura para AFFE_CHAR_MECA focada EXCLUSIVAMENTE em DDL_IMPO.
    Retorna uma lista de dicionários, um para cada comando individual.
    """
    commands = []
    order_keys = ["DRX", "DRY", "DRZ", "DX", "DY", "DZ"]

    for item in ddl_list:
        name = item.get("name", result_name)
        input_params = item.get("params", {})
        
        active_params = []
        for key in order_keys:
            if key in input_params:
                active_params.append((key, input_params[key]))

        commands.append({
            "name": name,
            "group": item["group"],
            "params": active_params,
            "model_name": model_name,
            "double_lagrange": "OUI",
            "info": 1,
            "veri_affe": "OUI",
            "veri_norm": "OUI"
        })

    return commands

--- ARQUIVO: prosolve/jinja/builders/affe_materiau.py ---
def build_affe_materiau(assignments_list, model_name="MODELE", result_name="CHMAT"):
    """
    Constrói a estrutura para AFFE_MATERIAU baseada em um JSON de atribuições.
    assignments_list: Lista de dicts { "material": "NOME", "groups": [...] }
    """
    items = []

    for item in assignments_list:
        raw_mat_name = item["material"]
        groups = item.get("groups", [])
        
        if not groups:
            continue
            
        # Reconstrói o nome da variável igual ao DEFI (ex: ACIER -> M_ACIER)
        # Importante: A regra de nomeação deve ser idêntica ao builder do DEFI
        safe_name = raw_mat_name.upper().replace(" ", "_")
        var_name = f"M_{safe_name}"

        items.append({
            "mater": var_name,
            "groups": groups
        })

    return {
        "result_name": result_name,
        "model_name": model_name,
        "items": items
    }

--- ARQUIVO: prosolve/jinja/builders/affe_modele.py ---
def build_affe_modele(models_list, mesh_name="MAIL", result_name="MODELE"):
    """
    Prepara os dados para o comando AFFE_MODELE.
    """
    items = []
    
    for model in models_list:
        # Aqui você pode adicionar lógica extra, validações, etc.
        item = {
            "group": model["group"],
            "modelisation": model["type"],
            "phenomene": "MECANIQUE" # Padrão, mas poderia vir do JSON
        }
        items.append(item)

    return {
        "result_name": result_name,
        "mesh_name": mesh_name,
        "items": items
    }

--- ARQUIVO: prosolve/jinja/builders/asse_maillage.py ---
# =========================================================
# Builder: ASSE_MAILLAGE
# Junta N malhas em UMA
# =========================================================

def build_asse_maillage(mesh_names, result_name="MAIL"):
    if not isinstance(mesh_names, list):
        raise ValueError("mesh_names deve ser lista")

    if len(mesh_names) == 0:
        raise RuntimeError("FATAL ERROR: No mesh units defined.")

    # Se só uma malha, nem precisa ASSE_MAILLAGE
    if len(mesh_names) == 1:
        return {
            "mode": "SINGLE",
            "final_mesh": mesh_names[0],
            "result_name": mesh_names[0],
            "items": []
        }

    items = [{"mesh": m} for m in mesh_names]

    return {
        "mode": "ASSE",
        "result_name": result_name,
        "final_mesh": result_name,
        "items": items
    }

--- ARQUIVO: prosolve/jinja/builders/defi_materiau.py ---
def build_defi_materiau(materials_list):
    """
    Prepara os dados para o comando DEFI_MATERIAU.
    Retorna uma lista de dicionários contendo var_name, E, NU e RHO.
    """
    definitions = []

    for mat in materials_list:
        # Sanitização do nome para criar variável no Code_Aster
        # Ex: "Concrete C30" -> "M_CONCRETE_C30"
        clean_name = mat["name"].upper().replace(" ", "_")
        var_name = f"M_{clean_name}"

        props = mat.get("props", {})
        
        definitions.append({
            "var_name": var_name,
            "E": props.get("E"),
            "NU": props.get("NU"),
            "RHO": props.get("RHO")
        })

    return definitions

--- ARQUIVO: prosolve/jinja/builders/force_coque.py ---
def build_force_coque(config, model_name="MODELE"):
    """
    Constrói dados para FORCE_COQUE com normalização de área.
    """
    foc_list = config.get("force_coque", [])
    if not foc_list:
        return {}, {} # Sem dados

    # 1. Configuração para o loop de cálculo de área (Python puro dentro do .comm)
    # Lista de tuplas: (nome, group_ma, total_force)
    press_config = []
    
    # 2. Configuração para o comando AFFE_CHAR_MECA
    # Lista de items de carga
    load_items = []

    for item in foc_list:
        name = item["name"]
        group = item["group"]
        force = item["total_force"]
        direction = item.get("direction", "PRES") # Default PRES, or FX, FY, FZ...

        # Adiciona à lista de cálculo
        press_config.append( (name, group, force) )

        # Constrói o item de carga que vai usar o valor calculado
        # O valor calculado estará em press_lookup[name][1]
        # Sintaxe JINJA vai inserir isso
        load_items.append({
            "name": name,
            "group": group,
            "direction": direction,
            # Placeholder para o template saber que deve pegar do dict
            "lookup_key": name 
        })

    calc_data = {
        "model_name": model_name,
        "press_config": press_config,
        # Precisamos de uma lista de TODOS os grupos envolvidos para criar o campo dummy corretamente?
        # O script original usa 'all_groups_ma'. Vamos passar a lista de grupos usados aqui.
        "groups": list(set([item["group"] for item in foc_list]))
    }

    load_data = {
        "model_name": model_name,
        "result_name": "CHARGE_FOC",
        "load_items": load_items,
        "double_lagrange": "OUI",
        "info": 1,
        "veri_affe": "OUI"
    }

    return calc_data, load_data

--- ARQUIVO: prosolve/jinja/builders/geometry.py ---
def build_geometry(geometry_list, model_name="MODELE", result_name="CARA_ELEM"):
    """
    Unified builder for GEOMETRY.
    Processes Shells, Beams, and Volumes.
    Returns data for model assignment and element characteristics.
    """
    model_items = []
    cara_items = []

    for item in geometry_list:
        group = item["group"]
        element_type = item.get("type", "Solid").upper()

        if element_type == "SHELL":
            # Model assignment
            model_items.append({
                "group": group,
                "modelisation": item.get("formulation", "DKT").upper()
            })
            # Cara assignment
            cara_items.append({
                "type": "COQUE",
                "group": group,
                "epais": item.get("thickness", 1.0),
                "excentrement": item.get("offset", 0.0),
                "vecteur": f"({item.get('vx', 1.0)}, {item.get('vy', 0.0)}, {item.get('vz', 0.0)})"
            })

        elif element_type == "BEAM":
            # Model assignment
            model_items.append({
                "group": group,
                "modelisation": "POU_D_T"
            })
            # Cara assignment
            section = item.get("section", "RECTANGLE").upper()
            if section == "RECTANGLE":
                hy = item.get("hy", 1.0)
                hz = item.get("hz", 1.0)
                cara = "('HY', 'HZ')"
                vale = f"({hy}, {hz})"
            else: # CIRCLE
                r = item.get("r", 1.0)
                cara = "('R')"
                vale = f"({r})"
                
            cara_items.append({
                "type": "POUTRE",
                "group": group,
                "section": section,
                "cara": cara,
                "vale": vale
            })

        else: # SOLID / VOLUME
            model_items.append({
                "group": group,
                "modelisation": "3D"
            })

    return {
        "model_items": model_items,
        "cara_items": cara_items,
        "model_result_name": model_name,
        "cara_result_name": result_name
    }

--- ARQUIVO: prosolve/jinja/builders/lire_maillage.py ---
# =========================================================
# Builder: LIRE_MAILLAGE (Code_Aster)
# Responsabilidade:
# - Validar dados
# - Gerar corpo do comando
# - Calcular UNITE automaticamente
# =========================================================

def build_lire_maillages(meshes, unit_start):
    """
    meshes: lista de dicts com:
        - name   : nome do concept (obrigatório)
        - format : MED | ASTER (opcional, default MED)

    unit_start: inteiro (ex: 80)
    """

    if not isinstance(meshes, list) or not meshes:
        raise ValueError("meshes deve ser uma lista não vazia")

    if not isinstance(unit_start, int):
        raise ValueError("unit_start deve ser inteiro")

    seen_names = set()
    maillages = []

    for idx, mesh in enumerate(meshes):
        if not isinstance(mesh, dict):
            raise ValueError("Cada mesh deve ser um dicionário")

        name = mesh.get("name")
        format = mesh.get("format", "MED")

        if not name or not isinstance(name, str):
            raise ValueError("Cada mesh precisa de um 'name' válido")

        if name in seen_names:
            raise ValueError(f"Nome de malha duplicado: {name}")
        seen_names.add(name)

        if format not in ("MED", "ASTER"):
            raise ValueError(
                f"Formato inválido para {name}: {format}"
            )

        unite = unit_start + idx

        body_lines = [
            f"    FORMAT='{format}',",
            f"    UNITE={unite},",
        ]

        maillages.append({
            "name": name,
            "body": "\n".join(body_lines)
        })

    return maillages

--- ARQUIVO: prosolve/jinja/builders/load_cases.py ---
def build_load_cases(lc_list, meca_config, pes_data, ddl_data, model_name="MODELE", reaction_extraction_data=None, **kwargs):
    """
    Constrói a lista de configurações de Load Cases (MECA_STATIQUE).
    """
    # Lista de NOMES válidos gerados pelos builds anteriores
    # ddl_data e pes_data são agora listas de configurações (cada uma tem 'name' ou 'result_name')
    valid_names = [d["name"] for d in ddl_data]
    
    if isinstance(pes_data, list):
        for p in pes_data:
            if "result_name" in p:
                valid_names.append(p["result_name"])
    elif pes_data and "result_name" in pes_data:
        # Fallback para objeto único
        valid_names.append(pes_data["result_name"])
    
    foc_data = kwargs.get("foc_data")
    if foc_data and "result_name" in foc_data:
        valid_names.append(foc_data["result_name"])

    meca_runs = []
    
    if not lc_list:
        # Fallback se não houver LC, tenta usar o primeiro DDL disponível ou avisa
        default_load = valid_names[0] if valid_names else "CHARGE_DDL"
        lc_list = [{"name": "MECA", "loads": [default_load]}]

    base_config = meca_config.get("meca_statique", {})

    for lc in lc_list:
        case_name = lc.get("name", "CASE")
        result_name = f"RESU_{case_name}"
        
        excit_list_lc = []
        for load_name in lc.get("loads", []):
            # Validação: se o nome existe nos loads gerados
            if load_name in valid_names:
                excit_list_lc.append({
                    "charge": load_name,
                    "type_charge": "FIXE_CSTE"
                })

        # Prepara dados para o template load_cases.j2
        meca_runs.append({
            "case_name": case_name,
            "result_name": result_name,
            "cara_elem": base_config.get("cara_elem", "CARA_ELEM"),
            "cham_mater": base_config.get("cham_mater", "CHAM_MATER"),
            "modele": model_name,
            "excit_list": excit_list_lc,
            "option": base_config.get("option", "SIEF_ELGA"),
            "solveur": base_config.get("solveur", {}),
            "info": base_config.get("info", 1),
            "reaction_extraction": reaction_extraction_data
        })

    return {
        "runs": meca_runs
    }

--- ARQUIVO: prosolve/jinja/builders/meca_statique.py ---
def build_meca_statique(config, result_name="RESU"):
    """
    Constrói a estrutura para MECA_STATIQUE.
    """
    data = config.get("meca_statique", {})
    
    # Extrai listas e valores simples
    excit_list = data.get("excit", [])
    
    return {
        "result_name": result_name,
        "cara_elem": data.get("cara_elem"),
        "cham_mater": data.get("cham_mater"),
        "excit_list": excit_list,
        "modele": data.get("modele"),
        # Valores fixos/default conforme solicitação
        "info": 1,
        "inst": 0.0,
        "option": "SIEF_ELGA",
        "solveur": {
            "acceleration": "AUTO",
            "elim_lagr": "LAGR2",
            "gestion_memoire": "AUTO",
            "low_rank_seuil": 0.0,
            "matr_distribuee": "NON",
            "methode": "MUMPS",
            "nb_rhs": 1,
            "nprec": 8,
            "pcent_pivot": 35,
            "posttraitements": "AUTO",
            "pretraitements": "AUTO",
            "reduction_mpi": 0,
            "renum": "AUTO",
            "resi_rela": 1e-06,
            "stop_singulier": "OUI",
            "type_resol": "AUTO"
        }
    }

--- ARQUIVO: prosolve/jinja/builders/pesanteur.py ---
def build_pesanteur(config, model_name="MODELE", result_name="CHARGE_PES"):
    """
    Constrói a estrutura para uma lista de cargas de PESANTEUR em AFFE_CHAR_MECA.
    Retorna uma lista de configurações, uma para cada comando.
    """
    pes_list = config.get("pesanteur", [])
    if not isinstance(pes_list, list):
        # Fallback para o formato antigo se necessário (embora o novo JSON envie lista)
        if isinstance(pes_list, dict) and pes_list:
            pes_list = [pes_list]
        else:
            return []

    commands = []
    for item in pes_list:
        final_name = item.get("name", result_name)
        commands.append({
            "result_name": final_name,
            "model_name": model_name,
            "gravite": item.get("gravite", 9.81),
            "direction": tuple(item.get("direction", [0.0, 0.0, -1.0])),
            "group_ma": item.get("group_ma", None),
            "double_lagrange": "OUI",
            "info": 1,
            "veri_affe": "OUI",
            "veri_norm": "OUI"
        })

    return commands

--- ARQUIVO: prosolve/jinja/builders/post_elem_mass.py ---
def build_post_elem_mass(config, model_name="MODELE", field_mat_name="CHAM_MATER", cara_elem_name="CARA_ELEM"):
    """
    Builds data for POST_ELEM (MASS_INER) and IMPR_TABLE.
    """
    calculations = config.get("mass_calculations", [])
    
    commands = []
    for item in calculations:
        commands.append({
            "result_name": item.get("result_name", "tab_mass"),
            "model_name": model_name,
            "field_mat_name": field_mat_name,
            "cara_elem_name": cara_elem_name,
            "title": item.get("title", "Physical_Mass_Structure"),
            "unit": item.get("unit", 26),
            "format": item.get("format", "TABLEAU"),
            "separator": item.get("separator", ","),
            "export_title": item.get("export_title", "MASS")
        })
        
    return commands

--- ARQUIVO: prosolve/jinja/builders/post_releve_t_reactions.py ---
def build_post_releve_t_reactions(config, ddl_list):
    """
    Builds data for reaction extraction.
    Identifies which groups have constraints (from ddl_list).
    """
    reac_config = config.get("reaction_extraction", {})
    if not reac_config.get("enabled", False):
        return None

    # Get unique groups from ddl_list
    constrained_groups = list(set([item["group"] for item in ddl_list if "group" in item]))

    if not constrained_groups:
        return None

    return {
        "groups": constrained_groups,
        "unit": reac_config.get("unit", 26),
        "format": reac_config.get("format", "TABLEAU"),
        "separator": reac_config.get("separator", ","),
        "resultante": tuple(reac_config.get("extract_components", ["DX", "DY", "DZ"])),
        "moment": tuple(reac_config.get("moment_components", ["DRX", "DRY", "DRZ"]))
    }


################################################################################
# PASTA: prosolve/jinja/templates
################################################################################

--- ARQUIVO: prosolve/jinja/templates/affe_cara_elem.j2 ---
{{ cara_result_name }} = AFFE_CARA_ELEM(
    MODELE={{ model_result_name }},
    {% for item in cara_items %}
    {{ item.type }} = _F(
        GROUP_MA = '{{ item.group }}',
        {% if item.type == 'COQUE' %}
        EPAIS = {{ item.epais }},
        EXCENTREMENT = {{ item.excentrement }},
        VECTEUR = {{ item.vecteur }},
        INER_ROTA = 'OUI',
        {% elif item.type == 'POUTRE' %}
        SECTION = '{{ item.section }}',
        CARA = {{ item.cara }},
        VALE = {{ item.vale }},
        {% endif %}
    ),
    {% endfor %}
)

--- ARQUIVO: prosolve/jinja/templates/affe_cara_elem_shell.j2 ---
{{ result_name }} = AFFE_CARA_ELEM(
    MODELE={{ model_name }},
    INFO=1,
    COQUE=(
    {%- for item in items %}
        _F(
            GROUP_MA='{{ item.group }}',
            EPAIS={{ item.epais }},
            VECTEUR={{ item.vecteur }},
            EXCENTREMENT={{ item.excentrement }},
            A_CIS={{ item.a_cis }},
            COEF_RIGI_DRZ={{ item.coef_rigi_drz }},
            COQUE_NCOU={{ item.coque_ncou }},
            INER_ROTA='{{ item.iner_rota }}',
            MODI_METRIQUE='{{ item.modi_metrique }}'
        ),
    {%- endfor %}
    ),
);

--- ARQUIVO: prosolve/jinja/templates/affe_char_meca_ddl.j2 ---
{% for item in commands %}
{{ item.name }} = AFFE_CHAR_MECA(
    DDL_IMPO=_F(
    {%- for key, val in item.params %}
        {{ key }}={{ val }},
    {%- endfor %}
        GROUP_NO='{{ item.group }}'),
    DOUBLE_LAGRANGE='{{ item.double_lagrange }}',
    INFO={{ item.info }},
    MODELE={{ item.model_name }},
    VERI_AFFE='{{ item.veri_affe }}',
    VERI_NORM='{{ item.veri_norm }}',
);
{% endfor %}

--- ARQUIVO: prosolve/jinja/templates/affe_materiau.j2 ---
{{ result_name }} = AFFE_MATERIAU(
    MODELE={{ model_name }},
    AFFE=(
    {%- for item in items %}
        _F(
            GROUP_MA=({{ "'" + item.groups|join("', '") + "'" }}),
            MATER={{ item.mater }}
        ),
    {%- endfor %}
    ),
);

--- ARQUIVO: prosolve/jinja/templates/affe_modele.j2 ---
{{ result_name }} = AFFE_MODELE(
    MAILLAGE={{ mesh_name }},
    AFFE=(
    {%- for item in items %}
        _F(GROUP_MA='{{ item.group }}', PHENOMENE='{{ item.phenomene }}', MODELISATION='{{ item.modelisation }}'),
    {%- endfor %}
    ),
);

--- ARQUIVO: prosolve/jinja/templates/asse_maillage.j2 ---
{# 
    Lógica para N malhas:
    O Code_Aster ASSE_MAILLAGE funciona melhor de forma binária (1+1).
    Este loop pega a primeira malha física e vai 'superpondo' as demais
    sequencialmente, acumulando tudo na variável definida em {{ result_name }}.
#}

{% set first_mesh = items[0].mesh %}

{% for item in items[1:] %}

{{ result_name }} = ASSE_MAILLAGE(
    {# Se for a primeira iteração, une a Malha 0 com a Malha 1 #}
    {# Nas próximas, une o Resultado Acumulado (MAIL) com a Malha Atual #}
    MAILLAGE_1 = {% if loop.first %}{{ first_mesh }}{% else %}{{ result_name }}{% endif %},
    
    MAILLAGE_2 = {{ item.mesh }},
    
    OPERATION = 'SUPERPOSE',
);

{% endfor %}

--- ARQUIVO: prosolve/jinja/templates/defi_materiau.j2 ---
{% for mat in definitions %}
{{ mat.var_name }} = DEFI_MATERIAU(
    ELAS=_F(
        E={{ mat.E }},
        NU={{ mat.NU }},
        RHO={{ mat.RHO }}
    )
);
{% endfor %}

--- ARQUIVO: prosolve/jinja/templates/export.j2 ---
P actions make_etude
P rep_trav {{ temp_path }}
P memory_limit 2048
P time_limit 900.0
P version stable
P ncpus 1
P mpi_nbcpu 1
P mode interactif
F comm {{ comm_path }} D 1
F mess {{ message_path }} R 6
R base {{ base_path }} R 0
{% for mesh in meshes %}
F mmed {{ mesh.path }} D {{ mesh.unit }}
{% endfor %}
{% if csv_path %}
F dat {{ csv_path }} R 26
{% endif %}

--- ARQUIVO: prosolve/jinja/templates/force_coque_calc.j2 ---
# -----------------------------------------------------------------------
# FORCE TO PRESSURE CONVERSION LOGIC
# -----------------------------------------------------------------------
# Dados de entrada populados pelo Jinja
PRESS_CONFIG = {{ press_config }}
GROUPS_CALC = {{ groups }}

# Cria materiais dummy para cálculo
M_CALC = DEFI_MATERIAU(ELAS=_F(E=1.0, NU=0.3, RHO=1.0))
FIELD_CALC = AFFE_MATERIAU(
    MODELE={{ model_name }}, 
    AFFE=(
        _F(GROUP_MA=GROUPS_CALC, MATER=M_CALC),
    )
)
CARA_CALC = AFFE_CARA_ELEM(
    MODELE={{ model_name }}, 
    COQUE=(
        _F(GROUP_MA=GROUPS_CALC, EPAIS=1.0),
    )
)

press_lookup = {}
print("INFO: --- CONVERTING PRESSURE LOADS ---")
for p_name, p_group, p_force in PRESS_CONFIG:
    # Calcula geometria (mass_iner retorna massa = area * rho(1) * epais(1) = area)
    tbl_geom = POST_ELEM(
        MODELE={{ model_name }}, 
        CHAM_MATER=FIELD_CALC, 
        CARA_ELEM=CARA_CALC, 
        MASS_INER=_F(GROUP_MA=(p_group,))
    )
    val_dict = tbl_geom.EXTR_TABLE().values()
    # 'MASSE' é a chave usual para massa em POST_ELEM com MASS_INER
    # Verifica-se se é MASSE ou outra coisa, mas MASS_INER gera MASSE.
    area_val = sum(val_dict['MASSE'])
    
    if area_val > 0.0:
        pressure_pa = p_force / area_val
        print(f"      Load '{p_name}' on '{p_group}': {pressure_pa:.2f} Pa (Force: {p_force}, Area: {area_val:.2f})")
        press_lookup[p_name] = (p_group, pressure_pa)
    else:
        print(f"WARNING: Area is zero for group {p_group}. Force {p_name} set to 0.")
        press_lookup[p_name] = (p_group, 0.0)

--- ARQUIVO: prosolve/jinja/templates/force_coque_load.j2 ---
{{ result_name }} = AFFE_CHAR_MECA(
    MODELE={{ model_name }},
    FORCE_COQUE=(
    {%- for item in load_items %}
        _F(
            GROUP_MA='{{ item.group }}',
            # Usa o valor calculado no Python (press_lookup[nome][1])
            {{ item.direction }}=press_lookup['{{ item.name }}'][1]
        ),
    {%- endfor %}
    ),
    DOUBLE_LAGRANGE='{{ double_lagrange }}',
    INFO={{ info }},
    VERI_AFFE='{{ veri_affe }}',
);

--- ARQUIVO: prosolve/jinja/templates/inspect_mesh.j2 ---
# -----------------------------------------------------------------------------
# 3. DIAGNOSTICO DETALHADO + EXPORTACAO JSON (INJETADO VIA JINJA)
# -----------------------------------------------------------------------------
import sys
import json
import os

def imprimir_na_tela(texto):
    """Forca a impressao no console"""
    sys.stdout.write(texto + "\n")
    sys.stdout.flush()

# Cabecalho da tabela no terminal
print("\n" + "="*90)
print(f"{'NOME DO GRUPO':<30} | {'TOTAL':<8} | {'COMPOSICAO (TIPO:QTD)'}")
print("="*90)

# Dicionario que vai guardar os dados para o JSON
dados_output = {
    "source_mesh": "Code_Aster_Inspection",
    "groups": {}
}

# 1. Pega lista de grupos da malha '{{ mesh_variable }}'
lista_grupos = {{ mesh_variable }}.getGroupsOfCells()
lista_grupos.sort()

for g_nome in lista_grupos:
    # 2. Pega IDs e quantidade
    elementos_ids = {{ mesh_variable }}.getCells(g_nome)
    total_elementos = len(elementos_ids)
    
    # 3. Conta tipos
    contagem_tipos = {}
    for elem_id in elementos_ids:
        tipo = {{ mesh_variable }}.getCellTypeName(elem_id)
        if tipo in contagem_tipos:
            contagem_tipos[tipo] += 1
        else:
            contagem_tipos[tipo] = 1
            
    # 4. Formata para o terminal
    lista_composicao = []
    for tipo, qtd in contagem_tipos.items():
        lista_composicao.append(f"{tipo}:{qtd}")
    str_composicao = ", ".join(lista_composicao)
    
    print(f"{g_nome:<30} | {total_elementos:<8} | {str_composicao}")
    
    # 5. Guarda no dicionario
    dados_output["groups"][g_nome] = {
        "count": total_elementos,
        "types": contagem_tipos
    }

# 6. Processamento de Grupos de NOS (Nodes)
lista_grupos_nos = {{ mesh_variable }}.getGroupsOfNodes()
lista_grupos_nos.sort()

for g_nome in lista_grupos_nos:
    # A API getNodes pede uma lista de nomes [g_nome]
    nos_ids = {{ mesh_variable }}.getNodes([g_nome])
    total_nos = len(nos_ids)
    
    # Formata para o terminal
    str_composicao = f"Node:{total_nos}"
    print(f"{g_nome:<30} | {total_nos:<8} | {str_composicao}")
    
    # Guarda no dicionario
    # O tipo 'Node' eh usado para o JS detectar corretamente na UI
    dados_output["groups"][g_nome] = {
        "count": total_nos,
        "types": {"Node": total_nos}
    }

print("="*90 + "\n")

# -----------------------------------------------------------------------------
# SALVANDO O MESH_GROUPS.JSON (CAMINHO ABSOLUTO INJETADO)
# -----------------------------------------------------------------------------
try:
    # A variavel output_folder e injetada pelo script Python inspect_mesh.py
    # Usamos string raw (r) para evitar problemas com barras
    pasta_destino = r"{{ output_folder }}"
    
    # Garante que a pasta existe (embora o Python main ja tenha criado)
    if not os.path.exists(pasta_destino):
        os.makedirs(pasta_destino)
    
    # Define o caminho completo do arquivo JSON
    caminho_json = os.path.join(pasta_destino, "mesh_groups.json")
    
    with open(caminho_json, 'w') as f:
        json.dump(dados_output, f, indent=4)
        
    imprimir_na_tela(f"[ASTER] Resultado da inspecao salvo em:\n{caminho_json}")

except Exception as e:
    imprimir_na_tela(f"[ERRO] Falha ao salvar mesh_groups.json: {e}")

--- ARQUIVO: prosolve/jinja/templates/lire_maillage.j2 ---
{{ mesh_name }} = LIRE_MAILLAGE(
    UNITE={{ unit }},
    FORMAT='MED',
    NOM_MED='{{ mesh_name }}'
);

--- ARQUIVO: prosolve/jinja/templates/load_cases.j2 ---
{% for run in runs %}
# --- Análise Mecânica: {{ run.case_name }} ---
{{ run.result_name }} = MECA_STATIQUE(
    CARA_ELEM={{ run.cara_elem }},
    CHAM_MATER={{ run.cham_mater }},
    EXCIT=(
    {%- for excit in run.excit_list %}
        _F(CHARGE={{ excit.charge }}, TYPE_CHARGE='{{ excit.type_charge }}'),
    {%- endfor %}
    ),
    INFO={{ run.info }},
    MODELE={{ run.modele }},
    OPTION='{{ run.option }}',
    SOLVEUR=_F(
        {%- for key, val in run.solveur.items() %}
        {{ key }}={{ val|tojson if val is string else val }}, # Simplificação
        {%- endfor %}
    ),
);

{% if run.reaction_extraction %}
# --- Extração de Reações: {{ run.case_name }} ---
{{ run.result_name }} = CALC_CHAMP(reuse={{ run.result_name }},
                         RESULTAT={{ run.result_name }},
                         FORCE=('REAC_NODA',))

REAC_{{ run.case_name }} = POST_RELEVE_T(ACTION=_F(OPERATION='EXTRACTION',
                                          INTITULE='Reac_{{ run.case_name }}',
                                          RESULTAT={{ run.result_name }},
                                          NOM_CHAM='REAC_NODA',
                                          GROUP_NO=({% for g in run.reaction_extraction.groups %}'{{ g }}'{{ ', ' if not loop.last }}{% endfor %}),
                                          RESULTANTE={{ run.reaction_extraction.resultante }},
                                          MOMENT={{ run.reaction_extraction.moment }},
                                          POINT=(0,0,0)))

IMPR_TABLE(TABLE=REAC_{{ run.case_name }},
           UNITE={{ run.reaction_extraction.unit }},
           FORMAT='{{ run.reaction_extraction.format }}',
           SEPARATEUR='{{ run.reaction_extraction.separator }}',
           TITRE='REAC_{{ run.case_name }}')
{% endif %}
{% endfor %}

--- ARQUIVO: prosolve/jinja/templates/meca_statique.j2 ---
{{ result_name }} = MECA_STATIQUE(CARA_ELEM={{ cara_elem }},
                    CHAM_MATER={{ cham_mater }},
                    {% for item in excit_list %}
                    EXCIT=_F(CHARGE={{ item.charge }},
                             TYPE_CHARGE='{{ item.type_charge }}'),
                    {% endfor %}
                    INFO={{ info }},
                    INST={{ inst }},
                    MODELE={{ modele }},
                    OPTION='{{ option }}',
                    SOLVEUR=_F(ACCELERATION='{{ solveur.acceleration }}',
                               ELIM_LAGR='{{ solveur.elim_lagr }}',
                               GESTION_MEMOIRE='{{ solveur.gestion_memoire }}',
                               LOW_RANK_SEUIL={{ solveur.low_rank_seuil }},
                               MATR_DISTRIBUEE='{{ solveur.matr_distribuee }}',
                               METHODE='{{ solveur.methode }}',
                               NB_RHS={{ solveur.nb_rhs }},
                               NPREC={{ solveur.nprec }},
                               PCENT_PIVOT={{ solveur.pcent_pivot }},
                               POSTTRAITEMENTS='{{ solveur.posttraitements }}',
                               PRETRAITEMENTS='{{ solveur.pretraitements }}',
                               REDUCTION_MPI={{ solveur.reduction_mpi }},
                               RENUM='{{ solveur.renum }}',
                               RESI_RELA={{ solveur.resi_rela }},
                               STOP_SINGULIER='{{ solveur.stop_singulier }}',
                               TYPE_RESOL='{{ solveur.type_resol }}'))

--- ARQUIVO: prosolve/jinja/templates/pesanteur.j2 ---
{% for item in commands %}
{{ item.result_name }} = AFFE_CHAR_MECA(
    MODELE={{ item.model_name }},
    PESANTEUR=_F(
        GRAVITE={{ item.gravite }},
        DIRECTION={{ item.direction }},
        {%- if item.group_ma %}
        GROUP_MA='{{ item.group_ma }}',
        {%- endif %}
    ),
    DOUBLE_LAGRANGE='{{ item.double_lagrange }}',
    INFO={{ item.info }},
    VERI_AFFE='{{ item.veri_affe }}',
    VERI_NORM='{{ item.veri_norm }}',
);
{% endfor %}

--- ARQUIVO: prosolve/jinja/templates/post_elem_mass.j2 ---
{% for cmd in commands %}
{{ cmd.result_name }} = POST_ELEM(MODELE={{ cmd.model_name }}, 
                       CHAM_MATER={{ cmd.field_mat_name }}, 
                       CARA_ELEM={{ cmd.cara_elem_name }},
                       MASS_INER=_F(TOUT='OUI'), 
                       TITRE='{{ cmd.title }}')

IMPR_TABLE(TABLE={{ cmd.result_name }}, 
           UNITE={{ cmd.unit }}, 
           FORMAT='{{ cmd.format }}', 
           SEPARATEUR='{{ cmd.separator }}', 
           TITRE='{{ cmd.export_title }}')
{% endfor %}


################################################################################
# PASTA: temp
################################################################################

--- ARQUIVO: temp/research_shell_methods.py ---
import sys
try:
    import medcoupling as mc
    
    print("=" * 80)
    print("INSPECTING extrudeConnectivity")
    print("=" * 80)
    
    # Check if it's a static method on MEDCouplingUMesh or something else
    try:
        print(mc.MEDCouplingUMesh.extrudeConnectivity.__doc__)
    except:
        print("extrudeConnectivity not in MEDCouplingUMesh")
        
    print("\n" + "=" * 80)
    print("INSPECTING computeSkin")
    print("=" * 80)
    try:
        print(mc.MEDCouplingUMesh.computeSkin.__doc__)
    except:
        print("computeSkin not in MEDCouplingUMesh")

except Exception as e:
    import traceback
    traceback.print_exc()

--- ARQUIVO: temp/simulate_frontend_handshake.py ---
import subprocess
import json
import os

# ==============================================================================
# SIMULATE_FRONTEND_HANDSHAKE.PY - ETAPA 3: VALIDAÇÃO EM /TEMP
# ==============================================================================

base_dir = r"c:\Users\jorge\OneDrive\ProSolveSimulation"
med_dir = os.path.join(base_dir, "MEDCOUPLING-9.15.0", "MEDCOUPLING-9.15.0")
extractor = os.path.join(base_dir, "backend", "services", "med", "med_extractor.py")
output_dump = os.path.join(base_dir, "temp", "global_state_dump.json")

test_files = {
    "shell": os.path.join(base_dir, "testcases", "shell", "shell.med"),
    "beam": os.path.join(base_dir, "testcases", "beam", "beam.med")
}

def handshake():
    print("--- SIMULATING FRONTEND HANDSHAKE ---")
    
    for key, path in test_files.items():
        if not os.path.exists(path):
            print(f"Skipping {key}: File not found.")
            continue
            
        print(f"\nProcessing: {os.path.basename(path)}...")
        
        # Chamar via subprocess (como a API faz)
        cmd = f'cmd /c "cd /d {med_dir} && call env_launch.bat && python \"{extractor}\" \"{path}\""'
        res = subprocess.run(cmd, capture_output=True, text=True, shell=True)
        
        if res.returncode == 0:
            # Pegar última linha (JSON minificado)
            output = res.stdout.strip().split('\n')[-1]
            try:
                data = json.loads(output)
                if data['status'] == 'success':
                    print(f"✓ Extracted: {data['filename']}")
                    
                    # SIMULAÇÃO JS (window.projectState consumption)
                    groups = data['data']['groups']
                    found_target = False
                    for name, info in groups.items():
                        # Critério de Sucesso do Protocolo
                        print(f"  > Group: {name[:20]:<20} | Cat: {info['category']:<4} | Normals: {'YES' if info['normals'] else 'NO'}")
                        
                        if info['category'] == "2D" and info['normals']: found_target = True
                        if info['category'] == "1D" and info['normals']: 
                            print("    [FAIL] 1D elements should NOT have normals!")
                            
                    # Persistir dump do último arquivo (shell costuma ser o mais rico)
                    if key == "shell":
                        with open(output_dump, 'w', encoding='utf-8') as f:
                            json.dump(data, f, indent=2)
                        print(f"\n[DUMP] Global state simulation saved to: {output_dump}")
                else:
                    print(f"  [ERROR] Extractor returned status error: {data.get('message')}")
            except Exception as e:
                print(f"  [ERROR] Failed to parse JSON or simulate JS logic: {e}")
        else:
            print(f"  [CRITICAL] Subprocess failed: {res.stderr}")

if __name__ == "__main__":
    handshake()

--- ARQUIVO: temp/test_flask_pipeline.py ---
import requests
import os
import json
import time

# ==============================================================================
# TEST_FLASK_PIPELINE.PY
# Testa a integração entre main.py e o Frontend
# ==============================================================================

def run_test():
    print("--- INICIANDO TESTE DE INTEGRAÇÃO FLASK ---")
    
    # 1. Check Health
    try:
        health = requests.get("http://localhost:5000/health")
        if health.status_code == 200:
            print(f"[PASS] Server is Online: {health.json()}")
        else:
            print(f"[FAIL] Health check failed: {health.status_code}")
            return
    except Exception as e:
        print(f"[CRITICAL] Servidor não está rodando na porta 5000: {e}")
        print("DICA: Rode 'python backend/main.py' em outro terminal primeiro!")
        return

    # 2. Test Mesh DNA Extraction
    base_dir = r"c:\Users\jorge\OneDrive\ProSolveSimulation"
    shell_med = os.path.join(base_dir, "testcases", "shell", "shell.med")
    
    print(f"\nSolicitando DNA para: {os.path.basename(shell_med)}...")
    start_time = time.time()
    
    try:
        response = requests.post(
            "http://localhost:5000/api/mesh_dna",
            json={"file_path": shell_med}
        )
        
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            data = response.json()
            if data['status'] == 'success':
                groups = data['data']['groups']
                print(f"[PASS] Extração bem sucedida em {elapsed:.2f}s")
                print(f"[INFO] Grupos Recebidos: {list(groups.keys())}")
                
                # Verifica integridade mínima
                if "group_shell" in groups:
                    info = groups["group_shell"]
                    print(f"  > Validando 'group_shell': {info['count']} elementos")
                    if info['normals'] and len(info['normals']) > 0:
                        print("  > [OK] Normais presentes para 2D.")
                    else:
                        print("  > [FAIL] Normais ausentes para 2D.")
            else:
                print(f"[FAIL] Extrator retornou erro: {data.get('message')}")
        else:
            print(f"[FAIL] Erro HTTP {response.status_code}: {response.text}")
            
    except Exception as e:
        print(f"[ERROR] Falha na requisição: {e}")

if __name__ == "__main__":
    run_test()

--- ARQUIVO: temp/test_native_merge.py ---
import sys
try:
    import medcoupling as mc
    
    print("=" * 80)
    print("TESTING NATIVE MESH MERGING")
    print("=" * 80)
    
    # Create two disjoint meshes
    c1 = mc.DataArrayDouble([0,0,0, 1,0,0, 0,1,0], 3, 3)
    m1 = mc.MEDCouplingUMesh("M1", 2)
    m1.setCoords(c1)
    m1.allocateCells(1)
    m1.insertNextCell(mc.NORM_TRI3, [0, 1, 2])
    
    c2 = mc.DataArrayDouble([10,0,0, 11,0,0, 10,1,0], 3, 3)
    m2 = mc.MEDCouplingUMesh("M2", 2)
    m2.setCoords(c2)
    m2.allocateCells(1)
    m2.insertNextCell(mc.NORM_TRI3, [0, 1, 2])
    
    print("Merging M1 and M2 using native static method MergeUMeshes...")
    try:
        # MergeUMeshes is a static method that takes a list of meshes
        merged = mc.MEDCouplingUMesh.MergeUMeshes([m1, m2])
        print(f"Merged nodes: {merged.getNumberOfNodes()} (Expected: 6)")
        print(f"Merged cells: {merged.getNumberOfCells()} (Expected: 2)")
        print(f"Sample coord of merged: {merged.getCoords().getTuple(3)}")
    except Exception as e:
        print(f"MergeUMeshes failed: {e}")

except Exception as e:
    import traceback
    traceback.print_exc()

--- ARQUIVO: temp/test_pipeline_handshake.py ---
import sys
import os
import json
import subprocess
import threading
import time
import requests
from flask import Flask, request, jsonify

# ==============================================================================
# TEST_PIPELINE_HANDSHAKE.PY - ETAPA 3: SIMULAÇÃO DE FLUXO COMPLETO
# ==============================================================================

app = Flask(__name__)

# Mock do Endpoint Real
@app.route('/api/mesh_dna', methods=['POST'])
def api_mesh_dna():
    try:
        data = request.get_json()
        file_path = data.get('file_path')
        
        # Telemetry: Port
        port = 5001
        print(f"\n[SERVER] [MED_API] Serving mesh data on Port: {port}")

        # Execute med_extractor.py
        base_dir = r"c:\Users\jorge\OneDrive\ProSolveSimulation"
        med_dir = os.path.join(base_dir, "MEDCOUPLING-9.15.0", "MEDCOUPLING-9.15.0")
        extractor = os.path.join(base_dir, "backend", "services", "med", "med_extractor.py")
        
        cmd = f'cmd /c "cd /d {med_dir} && call env_launch.bat && python \"{extractor}\" \"{file_path}\""'
        result = subprocess.run(cmd, capture_output=True, text=True, shell=True)

        if result.returncode == 0:
            output = result.stdout.strip().split('\n')[-1]
            extracted_data = json.loads(output)
            
            # Telemetry: Groups (Protocol)
            groups = list(extracted_data['data']['groups'].keys())
            print(f"[SERVER] [MED_API] Extracted Groups: {groups}")
            return jsonify(extracted_data)
        else:
             return jsonify({"status": "error", "message": result.stderr}), 500
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500

def run_server():
    app.run(port=5001, debug=False, use_reloader=False)

def run_client():
    time.sleep(2) # Espera o servidor subir
    
    print("--- [CLIENT] STARTING PIPELINE HANDSHAKE ---")
    base_dir = r"c:\Users\jorge\OneDrive\ProSolveSimulation"
    test_mesh = os.path.join(base_dir, "testcases", "shell", "shell.med")
    
    start_time = time.time()
    try:
        # Mocking the fetch call
        print(f"[CLIENT] Fetching Mesh DNA from port 5001...")
        response = requests.post("http://localhost:5001/api/mesh_dna", 
                               json={"file_path": test_mesh})
        
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            data = response.json()
            groups = data['data']['groups']
            
            print(f"[CLIENT] [MeshDNA] Groups Received: {list(groups.keys())}")
            print(f"[CLIENT] Performance Check: Request took {elapsed:.2f} seconds.")
            
            # Validation Logic
            found_shell = False
            for name, info in groups.items():
                if info['category'] == "2D":
                    if info['normals'] and len(info['normals']) > 0:
                        print(f"  > [PASS] Group '{name}' has normals.")
                        found_shell = True
                    else:
                         print(f"  > [FAIL] Group '{name}' is 2D but has NO normals.")
            
            if found_shell:
                print("\n[SUCCESS] Pipeline Handshake Verified!")
            else:
                 print("\n[WARNING] No 2D groups found to verify normals.")
                 
        else:
            print(f"[CLIENT] FAILED: {response.text}")
            
    except Exception as e:
        print(f"[CLIENT] CRITICAL ERROR: {e}")

if __name__ == "__main__":
    # Inicia servidor em thread separada
    server_thread = threading.Thread(target=run_server, daemon=True)
    server_thread.start()
    
    # Inicia o cliente
    run_client()
    
    print("\n--- SHUTTING DOWN MOCK SERVER ---")

--- ARQUIVO: temp/test_simplified_shell.py ---
import sys
try:
    import medcoupling as mc
    
    print("=" * 80)
    print("TESTING SIMPLIFIED SHELL GENERATION")
    print("=" * 80)
    
    # Create surface mesh
    coords = mc.DataArrayDouble([0,0,0, 1,0,0, 0,1,0], 3, 3)
    surf = mc.MEDCouplingUMesh("Surf", 2)
    surf.setCoords(coords)
    surf.allocateCells(1)
    surf.insertNextCell(mc.NORM_TRI3, [0, 1, 2])
    
    # Method 1: The "2 command" way if there's a direct extrude
    print("\nTrying direct extrude if it exists...")
    # Checking for any 'extrude' that takes just distance
    if hasattr(surf, 'extrude'):
        print("extrude() exists!")
        # help(surf.extrude)
    
    # Method 2: What I was doing but cleaner
    print("\nSimulating path + extruded + boundary...")
    path_coords = mc.DataArrayDouble([0,0,-0.5, 0,0,0.5], 2, 3)
    path = mc.MEDCouplingUMesh("Path", 1)
    path.setCoords(path_coords)
    path.allocateCells(1)
    path.insertNextCell(mc.NORM_SEG2, [0, 1])
    
    vol = surf.buildExtrudedMesh(path, 0)
    skin = vol.buildBoundaryMesh(True)
    
    print(f"Skin cells: {skin.getNumberOfCells()}")
    print(f"Skin nodes: {skin.getNumberOfNodes()}")

    # Comparison: Is there a way to get the skin WITHOUT building the volume?
    # Maybe buildSkin?
    if hasattr(surf, 'computeSkin'):
        # computeSkin on 2D returns the boundary edges (1D)
        edge_skin = surf.computeSkin()
        print(f"computeSkin on 2D mesh returns dimension: {edge_skin.getMeshDimension()}")

except Exception as e:
    import traceback
    traceback.print_exc()

--- ARQUIVO: temp/validate_global_state_data.py ---
import subprocess
import json
import os
import sys

# ==============================================================================
# VALIDATE_GLOBAL_STATE_DATA.PY - ETAPA 3: VALIDAÇÃO EM /TEMP
# ==============================================================================

base_dir = r"c:\Users\jorge\OneDrive\ProSolveSimulation"
med_dir = os.path.join(base_dir, "MEDCOUPLING-9.15.0", "MEDCOUPLING-9.15.0")
extractor_path = os.path.join(base_dir, "backend", "services", "med", "med_extractor.py")
test_mesh = os.path.join(base_dir, "testcases", "shell", "shell.med")
output_debug = os.path.join(base_dir, "temp", "debug_extraction.json")

def validate():
    print("--- INICIANDO VALIDAÇÃO DE PROTOCOLO ---")
    
    if not os.path.exists(test_mesh):
        print(f"Erro: Arquivo de teste não encontrado em {test_mesh}")
        return

    # Comando para rodar no ambiente MEDCoupling
    cmd = f'cmd /c "cd /d {med_dir} && call env_launch.bat && python \"{extractor_path}\" \"{test_mesh}\""'
    
    print(f"Executando extração em {os.path.basename(test_mesh)}...")
    res = subprocess.run(cmd, capture_output=True, text=True, shell=True)
    
    if res.returncode != 0:
        print("FALHA CRÍTICA NO SUBPROCESSO:")
        print(res.stderr)
        return

    # Tenta encontrar e processar o JSON
    try:
        out_lines = res.stdout.strip().split('\n')
        data = None
        for line in reversed(out_lines):
            line = line.strip()
            if line.startswith('{') and line.endswith('}'):
                data = json.loads(line)
                break
        
        if not data or data.get("status") != "success":
            print("FALHA: JSON retornado é inválido ou status != success")
            print(f"Saída bruta: {res.stdout[:500]}")
            return

        print("\n--- CHECKLIST DE SUCESSO ---")
        groups = data.get("data", {}).get("groups", {})
        
        # 1. Verificar chave groups
        if groups:
            print("[OK] Chave 'groups' presente.")
        else:
            print("[FAIL] Chave 'groups' ausente ou vazia.")

        # 2. Verificar grupo de casca (dimension=2) e normais
        for g_name, g_info in groups.items():
            cat = g_info.get("category")
            has_norms = g_info.get("normals") is not None and len(g_info.get("normals")) > 0
            
            print(f"   -> Verificando Grupo '{g_name}' ({cat}):")
            
            if cat == "2D":
                if has_norms:
                    print(f"      [OK] Grupo 2D tem normais.")
                else:
                    print(f"      [FAIL] Grupo 2D deveria ter normais.")
            else:
                if not has_norms:
                    print(f"      [OK] Grupo {cat} não tem normais.")
                else:
                    print(f"      [FAIL] Grupo {cat} NÃO deveria ter normais.")

            # 3. Confirmar points e connectivity
            if len(g_info.get("points")) > 0 and len(g_info.get("connectivity")) > 0:
                print(f"      [OK] Malha válida (Pts: {len(g_info['points'])//3}, Conn: {len(g_info['connectivity'])})")
            else:
                print(f"      [FAIL] Dados de malha vazios.")

        # PERSISTÊNCIA
        with open(output_debug, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2)
        print(f"\n[SUCESSO] JSON de debug salvo em: {output_debug}")

    except Exception as e:
        print(f"ERRO DURANTE VALIDAÇÃO: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    validate()

--- ARQUIVO: temp/verify_modular_vtk.py ---
import sys
import os
import json

# Add backend directory to path
backend_dir = os.path.join(os.getcwd(), "backend")
if backend_dir not in sys.path:
    sys.path.append(backend_dir)

from services.vtk_converter import med_to_vtk_json

def verify_modular_system():
    base_dir = r"c:\Users\jorge\OneDrive\ProSolveSimulation"
    shell_med = os.path.join(base_dir, "testcases", "shell", "shell.med")
    
    # Mock Geometries
    mock_geos = [
        {"group": "group_shell", "type": "COQUE_3D", "section_params": {"thickness": 10.0, "offset": 5.0}, "_category": "2D"},
        {"group": "group_beam", "type": "POU_D_T", "section_params": {"radius": 0.5}, "_category": "1D"}
    ]
    
    print("\n--- STARTING MODULAR VTK VERIFICATION ---")
    print(f"File: {shell_med}")
    
    result = med_to_vtk_json(shell_med, mock_geos)
    
    if result["status"] == "success":
        print("\nSUCCESS: Backend returned data!")
        print(f"Points: {result.get('num_points')}")
        cells = result.get("cells", {})
        print(f"Groups processed: {list(cells.keys())}")
        
        for g_name, g_data in cells.items():
            is_ext = g_data.get("is_extruded", False)
            c_type = g_data.get("type")
            count = len(g_data.get("connectivity", []))
            print(f"  -> Group '{g_name}': {count} cells, type '{c_type}', Extruded: {is_ext}")
            
            if is_ext:
                print(f"     PASSED: Extrusion verified for '{g_name}'")
    else:
        print("\nFAILED: Backend returned error.")
        print(f"Message: {result.get('message')}")
        if "traceback" in result: print(result["traceback"])

if __name__ == "__main__":
    verify_modular_system()

--- ARQUIVO: temp/verify_vtk_ext.py ---
import sys
import os
import json
import subprocess

def verify_vtk_extruder_systematic():
    base_dir = r"c:\Users\jorge\OneDrive\ProSolveSimulation"
    med_dir = os.path.join(base_dir, "MEDCOUPLING-9.15.0", "MEDCOUPLING-9.15.0")
    extruder_path = os.path.join(base_dir, "backend", "services", "med", "vtk_extruder.py")
    shell_med = os.path.join(base_dir, "testcases", "shell", "shell.med")
    
    # Mock Geometries
    mock_geos = [{"group": "group_shell", "type": "COQUE_3D", "section_params": {"thickness": 5.0, "offset": 2.5}}]
    geom_json = os.path.join(base_dir, "temp", "mock_geos_vtk_extruder.json")
    os.makedirs(os.path.dirname(geom_json), exist_ok=True)
    with open(geom_json, 'w') as f:
        json.dump(mock_geos, f)
        
    cmd = f'cmd /c "cd /d {med_dir} && call env_launch.bat && python \"{extruder_path}\" \"{shell_med}\" \"{geom_json}\""'
    print(f"Running VTK Extruder: {cmd}")
    
    result = subprocess.run(cmd, capture_output=True, text=True, shell=True)
    
    output = result.stdout.strip()
    data = None
    lines = output.split('\n')
    for line in reversed(lines):
        line = line.strip()
        if line.startswith('{') and line.endswith('}'):
            try:
                data = json.loads(line)
                break
            except: continue
            
    if data and data.get("status") == "success":
        print("\nSUCCESS: VTK Extruder returned data!")
        cells = data.get("cells", {})
        print(f"Groups: {list(cells.keys())}")
        if "group_shell_EXTRUDED" in cells:
            ext = cells["group_shell_EXTRUDED"]
            print(f"Extruded cells: {ext.get('count')}")
            # A linear extrusion of 1250 quads should create 1250 volumes (hexas/wedges)
            # vtkLinearExtrusionFilter creates quads for side faces + original surface + cap surface.
            # Actually it's often more than just a skin.
            if ext.get('count') > 0:
                print("PASSED: Extrusion generated cells.")
        else:
            print("FAILED: Missing extruded group.")
    else:
        print("FAILED: Extruder failed.")
        print(f"Output: {output}")
        if result.stderr: print(f"Error: {result.stderr}")

if __name__ == "__main__":
    verify_vtk_extruder_systematic()

