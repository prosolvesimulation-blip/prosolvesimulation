import os
import sys
import subprocess
import json
from flask import Blueprint, jsonify, request, current_app
import webview
from jinja2 import Environment, FileSystemLoader
import threading

# from services.vtk_converter import call_med_extractor  # DELETED
# from services.med.vtk_extruder import extrude_beam_memory, extrude_shell_memory  # Imported inside routes now

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

# For med_mesher call
ROOT_DIR = os.path.abspath(os.path.join(BASE_DIR, ".."))
MED_ENV_DIR = os.path.join(ROOT_DIR, "MEDCOUPLING-9.15.0", "MEDCOUPLING-9.15.0")
MESHER_SCRIPT = os.path.join(ROOT_DIR, "backend", "services", "med", "med_mesher.py")
RESULTS_SCRIPT = os.path.join(ROOT_DIR, "backend", "services", "med", "med_results_service.py")
ANALYSIS_SCRIPT = os.path.join(ROOT_DIR, "backend", "services", "med", "med_analysis_service.py")

api_blueprint = Blueprint('api', __name__)

@api_blueprint.route('/health', methods=['GET'])
def health():
    return jsonify({"status": "ok", "message": "ProSolve Professional API"})

@api_blueprint.route('/verification', methods=['GET'])
def get_verification_data():
    """
    Reads mass properties and reaction forces from CSV files generated by Code_Aster.
    """
    try:
        project_path = request.args.get('project_path')
        if not project_path:
            return jsonify({"status": "error", "message": "No project path provided"}), 400

        sim_dir = os.path.join(project_path, "simulation_files")
        mass_file = os.path.join(sim_dir, "mass_properties.csv")
        reac_file = os.path.join(sim_dir, "reactions.csv")

        # 1. Parse Mass Properties
        mass_data = None
        if os.path.exists(mass_file):
            try:
                # Code_Aster CSV format is specific: Skip header comments (#)
                # find the header line starting with LIEU,ENTITE,MASSE...
                with open(mass_file, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                
                header_idx = -1
                for i, line in enumerate(lines):
                    if line.strip().startswith("LIEU"):
                        header_idx = i
                        break
                
                if header_idx != -1 and header_idx + 1 < len(lines):
                    # Data is usually on the next line
                    data_line = lines[header_idx + 1]
                    parts = [p.strip() for p in data_line.split(',')]
                    # Filter out empty strings result of trailing commas
                    vals = [x for x in parts if x]
                    
                    if len(vals) >= 15: # Ensure we have enough columns
                        # Mapping based on standard Code_Aster header
                        # LIEU, ENTITE, MASSE, CDG_X, CDG_Y, CDG_Z, IX_G, IY_G, IZ_G...
                        mass_data = {
                            "mass": float(vals[2]),
                            "cdg_x": float(vals[3]),
                            "cdg_y": float(vals[4]),
                            "cdg_z": float(vals[5]),
                            "ix_g": float(vals[6]),
                            "iy_g": float(vals[7]),
                            "iz_g": float(vals[8])
                        }
            except Exception as e:
                print(f"Error parsing mass CSV: {e}")

        # 2. Parse Reactions
        reactions_data = []
        if os.path.exists(reac_file):
            try:
                with open(reac_file, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                
                # REAC_NODA format often has multiple tables in one file or multiple files
                # Simplified parser: look for data lines after headers
                # We expect headers like: NOEUD, DX, DY, DZ, DRX, DRY, DRZ...
                # BUT IMPR_TABLE output for RESULTANTE is simpler: Parameter names and values
                
                # Strategy: Detect case name from TITRE comments or Intitule column
                current_case = "Unknown"
                for line in lines:
                    line = line.strip()
                    
                    # Code_Aster IMPR_TABLE with TITRE prints usually # TITRE : REACTIONS_Case 1
                    # But can appear as ##REACTIONS_Case 1 in some versions
                    if "REACTIONS_" in line and line.startswith('#'):
                        # Extract everything after REACTIONS_ until a space or end of string
                        parts = line.split("REACTIONS_")
                        if len(parts) > 1:
                            # Take the first word and clean it
                            current_case = parts[1].split()[0].split(',')[0].strip()
                            continue
                        
                    if not line or line.startswith('#') or "DX" in line or "NOEUD" in line: # Skip header/comments
                        continue
                        
                    parts = [p.strip() for p in line.split(',')]
                    vals = [v for v in parts if v]
                    
                    # Heuristic: line with numbers corresponding to forces
                    if len(vals) >= 6: 
                        try:
                            # Fallback: If case is unknown, look at the first column (Intitule)
                            # which often contains 'Reac_Case_1'
                            row_case = current_case
                            if row_case == "Unknown" and len(vals) > 6:
                                first_col = vals[0]
                                if "Reac_" in first_col:
                                    row_case = first_col.split("Reac_")[-1].strip()

                            # Try to parse at least 6 floats from the end
                            floats = [float(v) for v in vals[-6:]]
                            reactions_data.append({
                                "case_name": row_case,
                                "fx": floats[0],
                                "fy": floats[1],
                                "fz": floats[2],
                                "mx": floats[3],
                                "my": floats[4],
                                "mz": floats[5]
                            })
                        except:
                            continue

            except Exception as e:
                print(f"Error parsing reaction CSV: {e}")

        return jsonify({
            "status": "success",
            "mass_properties": mass_data,
            "reactions": reactions_data
        })

    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500

@api_blueprint.route('/open_folder_dialog', methods=['GET'])
def open_folder_dialog():
    """Opens native Windows Folder Picker using PyWebView."""
    try:
        if len(webview.windows) > 0:
            window = webview.windows[0]
            folder_path = window.create_file_dialog(webview.FOLDER_DIALOG)
            
            if folder_path and len(folder_path) > 0:
                return jsonify({"status": "success", "path": folder_path[0]})
        return jsonify({"status": "cancelled"})
    except Exception as e:
        print(f"Dialog Error: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500

@api_blueprint.route('/scan_workspace', methods=['POST'])
def scan_workspace():
    """Scans a folder for geometry and mesh files."""
    data = request.get_json()
    folder_path = data.get('folder_path')
    
    if not folder_path or not os.path.exists(folder_path):
        return jsonify({"status": "error", "message": "Invalid Path"}), 400

    try:
        files = os.listdir(folder_path)
        geo_files = [f for f in files if f.lower().endswith(('.step', '.stp'))]
        mesh_files = [f for f in files if f.lower().endswith('.med') and f.lower() != 'resu.med']
        config_files = [f for f in files if f.lower().endswith('.comm')]
        
        result = {
            "status": "success",
            "geometry": len(geo_files) > 0,
            "mesh": len(mesh_files) > 0,
            "config": len(config_files) > 0,
            "files": {
                "geometry": geo_files,
                "mesh": mesh_files,
                "config": config_files,
            }
        }
        
        # Professional Refactor: Don't trigger Aster init automatically (too slow)
        # The groups will be read on-the-fly via MEDCOUPLING when the component mounts.
        # if mesh_files:
        #     init_result = init_aster_files(folder_path, mesh_files)
        #     result["aster_init"] = init_result
        
        return jsonify(result)
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500

def init_aster_files(folder_path, mesh_files):
    """
    Initialize Code_Aster files:
    1. Create simulation_files and temp directories
    2. Generate mesh.json
    3. Generate export.export
    4. Call inspect_mesh.py
    """
    try:
        sim_files_dir = os.path.join(folder_path, "simulation_files")
        temp_working_dir = os.path.join(sim_files_dir, "temp")
        
        # Create directories
        os.makedirs(sim_files_dir, exist_ok=True)
        os.makedirs(temp_working_dir, exist_ok=True)
        
        # Generate mesh.json
        mesh_data_list = []
        for i, med_file in enumerate(mesh_files):
            name = os.path.splitext(med_file)[0].replace("-", "_").replace(" ", "_")
            mesh_data_list.append({
                "name": name,
                "filename": med_file,
                "format": "MED"
            })
        
        mesh_json_path = os.path.join(sim_files_dir, "mesh.json")
        with open(mesh_json_path, 'w', encoding='utf-8') as f:
            json.dump({"unit_start": 80, "meshes": mesh_data_list}, f, indent=4)
        
        # Generate export.export using Jinja2
        jinja_dir = os.path.join(BASE_DIR, "services", "jinja", "templates")
        env = Environment(loader=FileSystemLoader(jinja_dir), trim_blocks=True, lstrip_blocks=True)
        tpl_export = env.get_template("export.j2")
        
        export_meshes = []
        for i, m in enumerate(mesh_data_list):
            export_meshes.append({
                "path": os.path.abspath(os.path.join(folder_path, m["filename"])),
                "unit": 80 + i
            })
        
        export_content = tpl_export.render(
            temp_path=os.path.abspath(temp_working_dir),
            comm_path=os.path.abspath(os.path.join(sim_files_dir, "med.comm")),
            meshes=export_meshes,
            message_path=os.path.abspath(os.path.join(sim_files_dir, "message")),
            base_path=os.path.abspath(os.path.join(sim_files_dir, "base")),
            csv_path=None
        )
        
        export_path = os.path.join(sim_files_dir, "export.export")
        with open(export_path, "w", encoding="utf-8") as f:
            f.write(export_content)
        
        # Call inspect_mesh.py (Generates med.comm)
        script_path = os.path.join(BASE_DIR, "services", "jinja", "inspect_mesh.py")
        if os.path.exists(script_path):
            result = subprocess.run(
                [sys.executable, script_path, folder_path],
                capture_output=True,
                text=True,
                encoding='utf-8',
                errors='replace'
            )
            if result.returncode != 0:
                print(f"[ASTER] inspect_mesh.py failed: {result.stderr}")
                return False
                
        # EXECUTE CODE_ASTER (as_run)
        # 1. READ CONFIG
        config = get_prosolve_config()
        run_aster_bin = config.get("ASTER_BIN")
        
        # Fallbacks
        if not run_aster_bin or not os.path.exists(run_aster_bin):
            # Fallback 1: Local Jinja bat
            local_bat = os.path.join(BASE_DIR, "services", "jinja", "run_aster.bat")
            if os.path.exists(local_bat):
                run_aster_bin = local_bat
                print(f"[ASTER] Configured path not found. Using local fallback: {local_bat}")
            else:
                print(f"[ASTER] CRITICAL: Code_Aster executable not found in config or fallback.")
                return True # We return true to not block UI load, but scan incomplete
        
        print(f"[ASTER] Executing Inspection via: {run_aster_bin}")
        print(f"[ASTER] Target Export: {export_path}")

        exec_result = subprocess.run(
            [run_aster_bin, export_path],
            capture_output=True,
            text=True,
            encoding='utf-8',
            errors='ignore'
        )
        
        # DUMP OUTPUT TO TERMINAL
        print(f"--- ASTER STDOUT ---\n{exec_result.stdout}\n--------------------")
        if exec_result.stderr:
            print(f"--- ASTER STDERR ---\n{exec_result.stderr}\n--------------------")
            
        if exec_result.returncode != 0:
            print(f"[ASTER] Execution failed (Code {exec_result.returncode})")
        else:
             print(f"[ASTER] Inspection Execution Complete.")
        
        return True
    except Exception as e:
        print(f"[ASTER] Init error: {e}")
        return False

@api_blueprint.route('/read_mesh_groups', methods=['POST'])
def read_mesh_groups():
    """Reads mesh groups directly from MED files using MEDCOUPLING extractor."""
    try:
        # from services.vtk_converter import call_med_extractor # DELETED
        from services.med.med_extractor import extract_med_data as call_med_extractor # Use direct if safe or env wrapper
        # Actually, let's use the env wrapper to be safe since it's an API call
        from api.routes import call_med_extractor_env as call_med_extractor_safe
        
        data = request.get_json()
        folder_path = data.get('folder_path')
        
        if not folder_path or not os.path.exists(folder_path):
            return jsonify({"status": "error", "message": "Path not provided or invalid"}), 400
        
        # Find all .med files
        files = [f for f in os.listdir(folder_path) if f.lower().endswith('.med') and 'resu' not in f.lower()]
        
        if not files:
            return jsonify({"status": "error", "message": "No .med files found"}), 404
            
        combined_groups = {}
        
        for mesh_file in files:
            target_path = os.path.join(folder_path, mesh_file)
            print(f"[API] Extracting groups from: {mesh_file}")
            
            # Use our professional extractor
            result = call_med_extractor_safe(target_path)
            
            if result and result.get("status") == "success":
                # Merge into a format matches mesh_groups.json structure
                # { "groups": { "NAME": { "count": X, "types": { "T1": Y } } } }
                for g_name, g_data in result.get("cells", {}).items():
                    # If multiple files have the same group, they "merge" or we prefix?
                    # The frontend usually expects groups to be unique mapped to geometry.
                    # We'll use the group name directly to support multi-mesh assembly.
                    combined_groups[g_name] = {
                        "count": g_data.get("count", 0),
                        "types": g_data.get("types", {}),
                        "type": g_data.get("type", "unknown"), # Pass the general VTK type (line, quad, ... or node)
                        "source": mesh_file # Metadata
                    }
            else:
                print(f"[API] Failed to extract from {mesh_file}")

        return jsonify({
            "status": "success", 
            "data": {
                "source_mesh": "MEDCOUPLING_Professional_Extractor",
                "groups": combined_groups
            }
        })
    except Exception as e:
        import traceback
        traceback.print_exc()
        print(f"[API] Error reading groups: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500

def get_prosolve_config():
    """Helper to read config.txt"""
    config_file = os.path.join(BASE_DIR, "..", "prosolve", "config.txt")
    config = {}
    if os.path.exists(config_file):
        with open(config_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if "=" in line and not line.startswith("#"):
                    key, val = line.split("=", 1)
                    config[key.strip()] = val.strip()
    return config

@api_blueprint.route('/get_settings', methods=['GET'])
def get_settings():
    """Returns content of config.txt"""
    try:
        config = get_prosolve_config()
        return jsonify({
            "status": "success",
            "settings": {
                "aster_path": config.get("ASTER_BIN", ""),
                "freecad_path": config.get("FREECAD_BIN", ""),
                "salome_path": config.get("SALOME_BIN", "")
            }
        })
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500

@api_blueprint.route('/save_settings', methods=['POST'])
def save_settings():
    """Updates config.txt"""
    try:
        data = request.get_json()
        aster = data.get('aster_path', '')
        freecad = data.get('freecad_path', '')
        salome = data.get('salome_path', '')
        
        config_file = os.path.join(BASE_DIR, "..", "prosolve", "config.txt")
        
        # We rewrite the file preserving comments? Hard with simple parsing.
        # Simple approach: Write new file with standard header
        with open(config_file, 'w', encoding='utf-8') as f:
            f.write("# Configurações de Caminhos do ProSolve\n")
            f.write("# Gerado Automaticamente\n\n")
            f.write(f"ASTER_BIN={aster}\n")
            f.write(f"FREECAD_BIN={freecad}\n")
            f.write(f"SALOME_BIN={salome}\n")
            
        return jsonify({"status": "success", "message": "Settings saved"})
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500

@api_blueprint.route('/launch_tool', methods=['POST'])
def launch_tool():
    """Launch external tools (FreeCAD, Salome) from config."""
    data = request.get_json()
    tool_name = data.get('tool_name')
    
    if not tool_name:
        return jsonify({"status": "error", "message": "Tool name required"}), 400
    
    config = get_prosolve_config()
    bin_path = None
    
    if tool_name.lower() == 'salome':
        bin_path = config.get("SALOME_BIN")
    elif tool_name.lower() == 'freecad':
        bin_path = config.get("FREECAD_BIN")
    elif tool_name.lower() == 'aster':
        bin_path = config.get("ASTER_BIN")
        
    if not bin_path or not os.path.exists(bin_path):
        return jsonify({"status": "error", "message": f"{tool_name} path not configured or not found: {bin_path}"}), 404
    
    try:
        subprocess.Popen([bin_path], shell=True if tool_name.lower() == 'salome' else False)
        return jsonify({"status": "success", "message": f"{tool_name} launched"})
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500

@api_blueprint.route('/calculate_section', methods=['POST'])
def calculate_section():
    """Calculate section properties using an external extractor and cache."""
    try:
        import hashlib
        import subprocess
        
        data = request.get_json()
        section_type = data.get('type')
        params = data.get('params', {})
        
        if not section_type:
            return jsonify({"status": "error", "message": "Section type required"}), 400
            
        # 1. CACHE MANAGEMENT
        param_str = json.dumps({"type": section_type, "params": params}, sort_keys=True)
        param_hash = hashlib.sha256(param_str.encode()).hexdigest()
        
        cache_dir = os.path.join(BASE_DIR, ".cache", "sections")
        os.makedirs(cache_dir, exist_ok=True)
        cache_file = os.path.join(cache_dir, f"{param_hash}.json")
        
        if os.path.exists(cache_file):
            with open(cache_file, "r", encoding="utf-8") as f:
                return jsonify(json.load(f))
        
        # 2. DYNAMIC EXTRACTION
        extractor_path = os.path.join(BASE_DIR, "services", "section_extractor.py")
        
        result = subprocess.run(
            [sys.executable, extractor_path],
            input=json.dumps(data),
            capture_output=True,
            text=True,
            encoding='utf-8'
        )
        
        if result.returncode != 0:
            return jsonify({"status": "error", "message": f"Extractor failed: {result.stderr}"}), 500
            
        try:
            output_json = json.loads(result.stdout)
        except:
            return jsonify({"status": "error", "message": f"Invalid extractor output: {result.stdout}"}), 500
            
        # 3. SAVE TO CACHE
        if output_json.get("status") == "success":
            with open(cache_file, "w", encoding="utf-8") as f:
                json.dump(output_json, f)
                
        return jsonify(output_json)
        
    except Exception as e:
        print(f"[API] Section calc error: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500

@api_blueprint.route('/save_project', methods=['POST'])
def save_project():
    """
    Saves the full project configuration to project.json.
    Also separates data into specific JSONs for the Jinja generation pipeline
    and runs generate_comm.py.
    """
    try:
        data = request.get_json()
        folder_path = data.get('folder_path')
        project_config = data.get('config')
        
        if not folder_path or not project_config:
            return jsonify({"status": "error", "message": "Missing path or config"}), 400
            
        study_dir = os.path.join(os.path.dirname(BASE_DIR), "backend", "services", "jinja", "study")
        project_file = os.path.join(folder_path, "project.json")
        
        # 1. Save master project.json
        with open(project_file, 'w', encoding='utf-8') as f:
            json.dump(project_config, f, indent=4)
            
        # 2. Simplification: No longer need to decompose into multiple JSONs
        # unless builders are updated to read main json (already done)
        # We only need project.json to exist in the folder_path root
        
        # 3. Run generate_comm.py
        script_path = os.path.join(os.path.dirname(BASE_DIR), "backend", "services", "jinja", "generate_comm.py")
        print(f"[SAVE] Project Path: {folder_path}")
        print(f"[SAVE] Script Path: {script_path}")
        
        result = subprocess.run(
            [sys.executable, script_path, "--project_path", folder_path],
            capture_output=True,
            text=True, 
            encoding='utf-8',
            errors='replace'
        )
        
        if result.stdout: print(f"--- GENERATOR STDOUT ---\n{result.stdout}")
        if result.stderr: print(f"--- GENERATOR STDERR ---\n{result.stderr}")
        
        if result.returncode != 0:
            return jsonify({"status": "warning", "message": f"Saved, but generation failed: {result.stderr}"})
            
        sim_dir = os.path.join(folder_path, "simulation_files")
        os.makedirs(sim_dir, exist_ok=True)
        dst_comm = os.path.abspath(os.path.join(sim_dir, "calcul.comm"))
            
        # 4. GENERATE EXPORT.EXPORT for SIMULATION
        jinja_dir = os.path.join(BASE_DIR, "services", "jinja", "templates")
        env = Environment(loader=FileSystemLoader(jinja_dir), trim_blocks=True, lstrip_blocks=True)
        tpl_export = env.get_template("export.j2")
        
        # Prepare mesh list for export from unified config
        mesh_data_list = project_config.get("meshes", [])
        
        export_mesh_objs = []
        for i, m in enumerate(mesh_data_list):
            # med file is strictly the filename in the same folder usually
            full_path = os.path.abspath(os.path.join(folder_path, m["filename"]))
            export_mesh_objs.append({
                "path": full_path,
                "unit": 80 + i 
            })
            
        temp_working_dir = os.path.join(sim_dir, "temp")
        os.makedirs(temp_working_dir, exist_ok=True)
        
        export_content = tpl_export.render(
            temp_path=os.path.abspath(temp_working_dir),
            comm_path=dst_comm, # Points to calcul.comm
            meshes=export_mesh_objs,
            message_path=os.path.abspath(os.path.join(sim_dir, "message")),
            base_path=os.path.abspath(os.path.join(sim_dir, "base")),
            resu_med_path=os.path.abspath(os.path.join(sim_dir, "resu.med")),
            mass_csv_path=os.path.abspath(os.path.join(sim_dir, "mass_properties.csv")),
            reactions_csv_path=os.path.abspath(os.path.join(sim_dir, "reactions.csv"))
        )
        export_file = os.path.join(sim_dir, "export.export")
        with open(export_file, "w", encoding="utf-8") as f:
            f.write(export_content)
            
        return jsonify({"status": "success", "message": "Project saved, generated, and ready for simulation"})
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        return jsonify({"status": "error", "message": str(e)}), 500

@api_blueprint.route('/run_simulation', methods=['POST'])
def run_simulation():
    """Executes the simulation using export.export."""
    try:
        data = request.get_json()
        folder_path = data.get('folder_path')
        
        if not folder_path:
             return jsonify({"status": "error", "message": "Path required"}), 400
             
        sim_dir = os.path.join(folder_path, "simulation_files")
        export_path = os.path.join(sim_dir, "export.export")
        
        if not os.path.exists(export_path):
            return jsonify({"status": "error", "message": "Export file not found. Save project first."}), 404
            
        # Get Aster Bin
        config = get_prosolve_config()
        aster_bin = config.get("ASTER_BIN")
        if not aster_bin:
             return jsonify({"status": "error", "message": "Code_Aster path not configured in Settings."}), 400
             
        cmd = [aster_bin, export_path]
        print(f"[SIMULATION] Starting: {cmd}")
        
        # Determine cwd (project folder or simulation_files?)
        # Usually where export is or where we want output
        
        # Launch in a NEW CONSOLE and use cmd /k to keep window open after finish
        cmd_open = ["cmd", "/k"] + cmd
        print(f"[SIM] Launching Aster: {cmd_open}")
        
        subprocess.Popen(
            cmd_open,
            cwd=sim_dir,
            creationflags=subprocess.CREATE_NEW_CONSOLE
        )
        
        return jsonify({
            "status": "success", 
            "message": "Simulation started. Console window will remain open for inspection."
        })
            
    except Exception as e:
        print(f"[SIMULATION] Error: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500




@api_blueprint.route('/open_project', methods=['POST'])
def open_project_config():
    """Reads project.json and returns it."""
    try:
        data = request.get_json()
        folder_path = data.get('folder_path')
        
        if not folder_path:
             return jsonify({"status": "error", "message": "Path required"}), 400
             
        project_file = os.path.join(folder_path, "project.json")
        if not os.path.exists(project_file):
            return jsonify({"status": "not_found", "message": "Project config not found"})
            
        with open(project_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
            
        return jsonify({"status": "success", "config": config})
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500


@api_blueprint.route('/get_mesh_data', methods=['POST'])
def get_mesh_data():
    """Reads mesh data for visualization."""
    try:
        from services.mesh_reader import read_mesh_file
        
        data = request.get_json()
        folder_path = data.get('folder_path')
        mesh_filename = data.get('mesh_filename') # Optional, else pick first
        
        if not folder_path:
             return jsonify({"status": "error", "message": "Path required"}), 400
             
        # Find mesh file
        target_path = ""
        if mesh_filename:
            target_path = os.path.join(folder_path, mesh_filename)
        else:
            # Auto find
            files = [f for f in os.listdir(folder_path) if f.lower().endswith('.med') and 'resu' not in f.lower()]
            if not files:
                return jsonify({"status": "error", "message": "No .med file found"}), 404
            target_path = os.path.join(folder_path, files[0])
            
        result = read_mesh_file(target_path)
        return jsonify(result)
        
    except Exception as e:
        print(f"[API] Mesh data error: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500




@api_blueprint.route('/get_mesh_vtk', methods=['POST'])
def get_mesh_vtk():
    """Reads ALL mesh files in VTK format for visualization."""
    try:
        from services.med.vtk_extruder import med_to_vtk_pipeline as med_to_vtk_json
        
        data = request.get_json()
        folder_path = data.get('folder_path')
        geometries = data.get('geometries', [])
        
        if not folder_path:
            return jsonify({"status": "error", "message": "Path required"}), 400
            
        # Find ALL mesh files
        files = [f for f in os.listdir(folder_path) if f.lower().endswith('.med') and 'resu' not in f.lower()]
        if not files:
            return jsonify({"status": "error", "message": "No .med file found"}), 404
        
        print(f"[API] Found {len(files)} mesh files: {files}")
        
        # Combine all meshes
        combined_points = []
        combined_cells = {}
        point_offset = 0
        
        for mesh_file in files:
            target_path = os.path.join(folder_path, mesh_file)
            print(f"[API] Processing: {mesh_file}")
            
            result = med_to_vtk_json(target_path, geometries=geometries)
            
            if result["status"] != "success":
                print(f"[API] Error loading {mesh_file}: {result.get('message')}")
                continue
            
            # Add points with offset
            combined_points.extend(result["points"])
            
            # Add cells with adjusted indices
            for group_name, group_data in result["cells"].items():
                # Prefix group name with file name to avoid conflicts
                prefixed_name = f"{mesh_file.replace('.med', '')}_{group_name}"
                
                # Adjust cell connectivity indices
                adjusted_connectivity = []
                for cell in group_data["connectivity"]:
                    adjusted_cell = [idx + point_offset for idx in cell]
                    adjusted_connectivity.append(adjusted_cell)
                
                combined_cells[prefixed_name] = {
                    "type": group_data["type"],
                    "connectivity": adjusted_connectivity
                }
            
            point_offset += len(result["points"])
        
        print(f"[API] Combined mesh: {len(combined_points)} points, {len(combined_cells)} groups")
        
        return jsonify({
            "status": "success",
            "points": combined_points,
            "cells": combined_cells,
            "num_points": len(combined_points),
            "num_groups": len(combined_cells)
        })
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        print(f"[API] VTK mesh error: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500





@api_blueprint.route('/3d/generate', methods=['POST'])
def generate_3d_view():
    """
    RESTORED: In-Memory 3D Generation Endpoint.
    1. Receives geometry state (params for extrusion).
    2. Reads MED files.
    3. Applies native VTK extrusion (Beam/Shell) in memory.
    4. Returns a list of separate components for the viewer.
    """
    try:
        from services.med.vtk_extruder import med_to_vtk_pipeline as med_to_vtk_json
        
        data = request.get_json()
        project_path = data.get('project_path')
        geometry_state = data.get('geometry_state', [])
        
        print(f"[3D-VIEW] START: project_path={project_path}")
        print(f"[3D-VIEW] RECEIVED {len(geometry_state)} geometry configs.")
        if len(geometry_state) > 0:
            print(f"[3D-VIEW] Sample Config 0: group={geometry_state[0].get('group')}, category={geometry_state[0].get('_category')}, has_section_mesh={bool(geometry_state[0].get('section_mesh'))}")

        
        if not project_path or not os.path.exists(project_path):
            return jsonify({"status": "error", "message": "Invalid Project Path"}), 400

        # Find MED files
        med_files = [f for f in os.listdir(project_path) if f.lower().endswith('.med') and 'resu' not in f.lower()]
        
        if not med_files:
            return jsonify({"status": "warning", "message": "No mesh files found", "data": []})

        scene_components = []

        for med_file in med_files:
            full_path = os.path.join(project_path, med_file)
            print(f"[3D-GEN] Processing {med_file} with {len(geometry_state)} geometry configs...")
            
            # This returns a merged structure: { points: [...], cells: { "GroupA": {conn...}, "GroupB": ... } }
            vtk_result = med_to_vtk_json(full_path, geometries=geometry_state)
            
            if vtk_result.get("status") != "success":
                print(f"[3D-GEN] Failed to process {med_file}: {vtk_result.get('message')}")
                continue
                
            # Flatten groups into separate scene components for the inventory
            for group_name, group_data in vtk_result["cells"].items():
                component = {
                    "id": f"{med_file}_{group_name}",
                    "data": {
                        "points": vtk_result["points"],
                        "connectivity": group_data["connectivity"],
                        "vtk_type": group_data.get("vtk_type", 5),
                        "is_extruded": group_data.get("is_extruded", False),
                        "is_base": group_data.get("is_base", False)
                    }
                }
                scene_components.append(component)

        return jsonify({
            "status": "success",
            "data": scene_components
        })

    except Exception as e:
        import traceback
        traceback.print_exc()
        return jsonify({"status": "error", "message": str(e), "traceback": traceback.format_exc()}), 500


@api_blueprint.route('/get_hq_assembly', methods=['POST'])
def get_hq_assembly():
    """
    TRIGGER: Direct transcription of standalone success logic.
    Calls med_mesher.py via SALOME env for ALL project meshes.
    """
    try:
        data = request.get_json()
        folder_path = data.get('folder_path')
        
        if not folder_path or not os.path.exists(folder_path):
            return jsonify({"status": "error", "message": "Invalid project path"}), 400
            
        # 1. FIND ALL MESHES
        med_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.med') and 'resu' not in f.lower()]
        
        # 2. RUN MED_MESHER FOR EACH (HQ Extraction)
        # Paths for environment setup
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        med_env_dir = os.path.join(os.path.dirname(project_root), "MEDCOUPLING-9.15.0", "MEDCOUPLING-9.15.0")
        mesher_script = os.path.join(project_root, "services", "med", "med_mesher.py")
        
        for med_file in med_files:
            source_path = os.path.join(folder_path, med_file)
            # cmd /c "cd /d ... && call ... && python ... ..."
            cmd = f'cmd /c "cd /d {med_env_dir} && call env_launch.bat && python \"{mesher_script}\" \"{source_path}\""'
            print(f"[HQ-TRIGGER] Extracting: {med_file}...")
            subprocess.run(cmd, shell=True, capture_output=True)
            
        # 3. AGGREGATE RESULTS
        json_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.json') and f != 'project.json']
        
        combined_points = []
        combined_cells = {}
        point_offset = 0
        
        for j_file in json_files:
            j_path = os.path.join(folder_path, j_file)
            try:
                with open(j_path, 'r', encoding='utf-8') as f:
                    mesh_data = json.load(f)
            except: continue
                
            if mesh_data.get("status") != "success": continue
            
            pts = mesh_data.get("points", [])
            conn = mesh_data.get("connectivity", [])
            vtk_type = mesh_data.get("vtk_type", 9)
            
            combined_points.extend(pts)
            
            mesh_name = os.path.splitext(j_file)[0]
            num_pts_in_mesh = len(pts) // 3
            
            # Index offset correction (Standalone success logic)
            adjusted_conn = []
            for cell in conn:
                adjusted_conn.append([idx + point_offset for idx in cell])
                
            combined_cells[mesh_name] = {
                "type": {3: "line", 5: "triangle", 9: "quad", 10: "tetra", 12: "hexa"}.get(vtk_type, "poly"),
                "connectivity": adjusted_conn
            }
            
            point_offset += num_pts_in_mesh

        return jsonify({
            "status": "success",
            "points": combined_points,
            "cells": combined_cells,
            "num_points": len(combined_points) // 3
        })
    except Exception as e:
        import traceback
        traceback.print_exc()
        return jsonify({"status": "error", "message": str(e)}), 500






REPORT_SCRIPT = os.path.join(ROOT_DIR, "backend", "services", "med", "med_analysis_report.py")
PROCESSOR_SCRIPT = os.path.join(ROOT_DIR, "backend", "services", "med", "vtk_analysis_processor.py")

@api_blueprint.route('/mesh_dna', methods=['POST'])
def api_mesh_dna():
    """Proxy for legacy mesh_dna call, handles routing to dedicated analysis reporter."""
    try:
        data = request.get_json()
        file_path = data.get('file_path')

        if not file_path or not os.path.exists(file_path):
            return jsonify({"status": "error", "message": "Invalid file path"}), 400

        # ANALYSIS ROUTING: Use dedicated REPORT for results
        if "resu" in os.path.basename(file_path).lower():
            print(f"[API] Routing {os.path.basename(file_path)} to REPORT SERVICE")
            output = run_extraction_command(file_path, "--mesh")
            
            # For DNA, we just need the report format or wrap it to match legacy
            if output.get("status") == "success":
                # Match legacy structure for window.projectState
                return jsonify({
                    "status": "success",
                    "points": output["data"]["points"],
                    "cells": {
                        "_FULL_MESH_": {
                            "connectivity": output["data"]["connectivity"],
                            "num_elements": output["data"]["num_elements"],
                            "type": "poly"
                        }
                    }
                })
            return jsonify(output)
        
        # Standard workflow for input meshes - MUST RUN IN SALOME ENVIRONMENT
        return jsonify(call_med_extractor_env(file_path))

    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500

def call_med_extractor_env(file_path):
    """Executes med_extractor.py in the SALOME environment for library access."""
    extractor_script = os.path.join(ROOT_DIR, "backend", "services", "med", "med_extractor.py")
    try:
        cmd = (
            f'cmd /c "cd /d \"{MED_ENV_DIR}\" && '
            f'call env_launch.bat && '
            f'python \"{extractor_script}\" \"{file_path}\""'
        )
        print(f"[API] Executing med_extractor in env...")
        result = subprocess.run(cmd, capture_output=True, text=True, shell=True)
        
        if result.returncode == 0:
            output = result.stdout
            if "__JSON_START__" in output and "__JSON_END__" in output:
                json_str = output.split("__JSON_START__")[1].split("__JSON_END__")[0]
                return json.loads(json_str)
            return {"status": "error", "message": f"Malformed output: {output}"}
        return {"status": "error", "message": f"Process failed: {result.stderr}"}
    except Exception as e:
        return {"status": "error", "message": str(e)}


@api_blueprint.route('/report/generate', methods=['POST'])
def generate_report():
    """Aggregates project data and generates a DOCX report."""
    try:
        from services.report.report_service import create_report_file
        
        data = request.get_json()
        project_path = data.get('project_path')
        images = data.get('images', []) 
        selection = data.get('selection')
        
        if not project_path or not os.path.exists(project_path):
            return jsonify({"status": "error", "message": "Invalid Project Path"}), 400
            
        # 1. Load Project Configuration
        project_config = {}
        project_file = os.path.join(project_path, "project.json")
        if os.path.exists(project_file):
            try:
                with open(project_file, 'r', encoding='utf-8') as f:
                    project_config = json.load(f)
            except Exception as e:
                print(f"[Report] Failed to load project.json: {e}")

        # 2. Gather Simulation Data
        context = {
            "project_name": os.path.basename(project_path),
            "project_config": project_config,
            "images": images,
            "mass_props": {},
            "reactions": [],
            "max_stress": 0.0
        }
        
        sim_dir = os.path.join(project_path, "simulation_files")
        mass_file = os.path.join(sim_dir, "mass_properties.csv")
        reac_file = os.path.join(sim_dir, "reactions.csv")
        
        # Parse Mass
        if os.path.exists(mass_file):
            try:
                with open(mass_file, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    header_idx = -1
                    for i, line in enumerate(lines):
                        if line.strip().startswith("LIEU"):
                            header_idx = i
                            break
                    if header_idx != -1 and header_idx + 1 < len(lines):
                        parts = [p.strip() for p in lines[header_idx+1].split(',') if p.strip()]
                        if len(parts) >= 6:
                            context["mass_props"] = {
                                "mass": float(parts[2]),
                                "cdg_x": float(parts[3]),
                                "cdg_y": float(parts[4]),
                                "cdg_z": float(parts[5])
                            }
            except: pass

        # Parse Reactions
        if os.path.exists(reac_file):
            try:
                with open(reac_file, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    for line in lines:
                        parts = [p.strip() for p in line.split(',') if p.strip()]
                        if len(parts) >= 6 and parts[0].replace('.','').replace('-','').isdigit(): 
                            context["reactions"].append({
                                "fx": float(parts[-6]),
                                "fy": float(parts[-5]),
                                "fz": float(parts[-4]),
                                "mx": float(parts[-3]),
                                "my": float(parts[-2]),
                                "mz": float(parts[-1]),
                                "case_name": "Load Case"
                            })
            except: pass

        # Parse Max Stress (Field VMIS)
        try:
            resu_path = os.path.join(sim_dir, "resu.med")
            if os.path.exists(resu_path):
                meta_res = run_extraction_command(resu_path, "--meta")
                if meta_res["status"] == "success":
                    fields = meta_res["fields"].keys()
                    vm_field = next((f for f in fields if "VM" in f or "SIGM" in f), None)
                    if vm_field:
                        data_res = run_extraction_command(resu_path, vm_field)
                        if data_res["status"] == "success":
                            vals = data_res["data"]["values"]
                            if vals:
                                context["max_stress"] = max(vals)
        except Exception as e:
            print(f"[Report] Stress extraction failed: {e}")

        # 3. Generate DOCX
        result = create_report_file(project_path, context, selection=selection)
        return jsonify(result)

    except Exception as e:
        import traceback
        traceback.print_exc()
        return jsonify({"status": "error", "message": str(e)}), 500

@api_blueprint.route('/report/open', methods=['POST'])
def open_report():
    """Opens the generated DOCX report using the system's default handler (Windows)."""
    try:
        data = request.get_json()
        file_path = data.get('file_path')
        
        if not file_path or not os.path.exists(file_path):
            return jsonify({"status": "error", "message": "File not found"}), 404
            
        print(f"[API] Opening report: {file_path}")
        os.startfile(file_path)
        
        return jsonify({"status": "success", "message": "Report opened successfully"})
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500

    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500

@api_blueprint.route('/post/results', methods=['POST'])
def get_simulation_results():
    """Lists fields from resu.med via REPORT SERVICE."""
    try:
        data = request.get_json()
        project_path = data.get('project_path')
        resu_path = os.path.join(project_path, "simulation_files", "resu.med")
        if not os.path.exists(resu_path):
            return jsonify({"status": "error", "message": "Result file not found"}), 404
            
        output = run_extraction_command(resu_path, "--meta")
        return jsonify(output)
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500

@api_blueprint.route('/post/field', methods=['POST'])
def get_result_field():
    """
    ULTIMATE PIPELINE: 
    1. Extract raw data via REPORT SERVICE
    2. Process via VTK PROCESSOR (Native Active Scalars)
    3. Return Unified Scene to Screen
    """
    try:
        data = request.get_json()
        project_path = data.get('project_path')
        mode = data.get('mode') 
        
        resu_path = os.path.join(project_path, "simulation_files", "resu.med")
        
        # STAGE 1: THE REPORT
        mesh_rpt = run_extraction_command(resu_path, "--mesh")
        field_rpt = run_extraction_command(resu_path, mode)
        
        if mesh_rpt.get("status") != "success" or field_rpt.get("status") != "success":
            return jsonify({"status": "error", "message": "Extraction Stage Failed"})

        # STAGE 2: THE PROCESSOR (Dual Process Bridge)
        bundle = {"mesh_report": mesh_rpt, "field_report": field_rpt}
        scene = run_processor_command(bundle, mode)
        
        return jsonify(scene)
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500

def run_result_command(file_path, mode, step=0):
    """Old bridge (keeping for compatibility if other modules use it)."""
    return run_extraction_command(file_path, mode)

def run_extraction_command(file_path, cmd):
    """Bridge to the high-fidelity Report service."""
    command = (
        f'cmd /c "cd /d "{MED_ENV_DIR}" && '
        f'call env_launch.bat && '
        f'cd /d "{ROOT_DIR}" && '
        f'python "{REPORT_SCRIPT}" "{cmd}" "{file_path}""'
    )
    return _execute_pipe_command(command)

def run_processor_command(data_bundle, field_name):
    """Bridge to the VTK-Native processor."""
    # Data is passed asescaped JSON string
    json_data = json.dumps(data_bundle).replace('"', '\"')
    command = (
        f'cmd /c "cd /d "{MED_ENV_DIR}" && '
        f'call env_launch.bat && '
        f'cd /d "{ROOT_DIR}" && '
        f'python "{PROCESSOR_SCRIPT}" "{json_data}" "{field_name}""'
    )
    # We use a custom execution for processor because of large JSON input
    proc = subprocess.run(command, shell=True, capture_output=True, text=True)
    if proc.returncode == 0:
        raw_out = proc.stdout
        marker = "__JSON_START__"
        if marker in raw_out:
            json_str = raw_out.split(marker)[1].split("__JSON_END__")[0]
            return json.loads(json_str)
    return {"status": "error", "message": "Processor failed", "stderr": proc.stderr}

def _execute_pipe_command(command):
    """General pipe bridge."""
    proc = subprocess.run(command, shell=True, capture_output=True, text=True)
    if proc.returncode == 0:
        raw_out = proc.stdout
        start_marker = "__JSON_START__"
        end_marker = "__JSON_END__"
        if start_marker in raw_out and end_marker in raw_out:
            json_str = raw_out.split(start_marker)[1].split(end_marker)[0]
            return json.loads(json_str)
    return {"status": "error", "message": f"Pipe failure: {proc.stderr}"}
